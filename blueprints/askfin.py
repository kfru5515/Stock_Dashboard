import dart_fss as dart
import time

import os
import json
import traceback
import requests
import google.generativeai as genai
import FinanceDataReader as fdr
import pandas as pd
import numpy as np
import statistics
from flask import Blueprint, render_template, request, jsonify, session
from dotenv import load_dotenv
from datetime import datetime, timedelta
import concurrent.futures

from pykrx import stock
import re
from bs4 import BeautifulSoup

from fuzzywuzzy import fuzz

# --- Global Caches for Initial Loading ---
GLOBAL_KRX_LISTING = None
GLOBAL_TICKER_NAME_MAP = None 
GLOBAL_NAME_TICKER_MAP = None
ANALYSIS_CACHE = {} 
GLOBAL_SECTOR_MASTER_DF = None 
GLOBAL_STOCK_SECTOR_MAP = None 
STOCK_DETAIL_CACHE = {} 
# --- Environment Variable Loading and API Key Setup ---
load_dotenv()

askfin_bp = Blueprint('askfin', __name__, url_prefix='/askfin')

try:
    DART_API_KEY = os.getenv("DART_API_KEY")
    if not DART_API_KEY:
        raise ValueError("DART API í‚¤ê°€ .env íŒŒì¼ì— ì—†ìŠµë‹ˆë‹¤.")
    dart.set_api_key(api_key=DART_API_KEY)
    print("DART API í‚¤ê°€ ì„±ê³µì ìœ¼ë¡œ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")
except Exception as e:
    print(f"[ê²½ê³ ] DART API í‚¤ ì„¤ì • ì‹¤íŒ¨: {e}")

try:
    API_KEY = os.getenv("GOOGLE_AI_API_KEY")
    if not API_KEY: raise ValueError("API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
    genai.configure(api_key=API_KEY)
    model = genai.GenerativeModel('gemini-1.5-flash')

    PROMPT_TEMPLATE = """
You are a financial analyst. Your primary task is to analyze a user's query and convert it into a structured JSON object.

- You MUST respond with a JSON object that follows the schema.
- **EXCEPTION**: If the user's query is a general question, a greeting, or something that CANNOT be structured into the JSON schema, you MUST respond with a conversational, friendly answer in plain text INSTEAD of JSON.
- For "comparison_analysis", the "target" MUST be an array of strings.
- Be specific with the "period" value. If the user says "ì´ë²ˆì£¼", use "ì´ë²ˆì£¼". If they say "ì§€ë‚œ 1ë¶„ê¸°", use "ì§€ë‚œ 1ë¶„ê¸°".

## JSON Schema:
{{"query_type": "string", "period": "string|null", "condition": "string|object|null", "target": "string|array|null", "action": "string|null"}}

## Examples (JSON Output):

# --- ê¸°ë³¸ ì˜ˆì‹œ ---
1.  User Query: "ì§€ë‚œ 3ë…„ ë™ì•ˆ ê²¨ìš¸ì— ì˜¤ë¥¸ ì½˜í…ì¸  ê´€ë ¨ ì£¼ì‹"
    JSON Output:
    ```json
    {{"query_type": "stock_analysis", "period": "ì§€ë‚œ 3ë…„", "condition": "ê²¨ìš¸", "target": "ì½˜í…ì¸  ê´€ë ¨ì£¼", "action": "ì˜¤ë¥¸ ì£¼ì‹"}}
    ```
2.  User Query: "ìµœê·¼ CPI ì§€ìˆ˜ ì•Œë ¤ì¤˜"
    JSON Output:
    ```json
    {{"query_type": "indicator_lookup", "period": "ìµœê·¼", "condition": null, "target": "CPI ì§€ìˆ˜", "action": "ì¡°íšŒ"}}
    ```
3.  User Query: "ì¸ê³µì§€ëŠ¥, 2ì°¨ì „ì§€ ì¤‘ ì§€ë‚œ 1ë…„ê°„ ê°€ì¥ ë§ì´ ì˜¤ë¥¸ í…Œë§ˆëŠ”?"
    JSON Output:
    ```json
    {{"query_type": "comparison_analysis", "period": "ì§€ë‚œ 1ë…„ê°„", "condition": null, "target": ["ì¸ê³µì§€ëŠ¥", "2ì°¨ì „ì§€"], "action": "ê°€ì¥ ë§ì´ ì˜¤ë¥¸ í…Œë§ˆ"}}
    ```

# --- êµ¬ì²´ì ì¸/ë‹¨ê¸° ê¸°ê°„ ì˜ˆì‹œ ---
4.  User Query: "ì´ë²ˆì£¼ ê°€ì¥ ë§ì´ ì˜¤ë¥¸ ì£¼ì‹ì€ ë­ì•¼?"
    JSON Output:
    ```json
    {{"query_type": "stock_analysis", "period": "ì´ë²ˆì£¼", "condition": null, "target": "ì£¼ì‹", "action": "ê°€ì¥ ë§ì´ ì˜¤ë¥¸ ì£¼ì‹"}}
    ```
5.  User Query: "ì˜¤ëŠ˜ ì œì¼ ë§ì´ ë‚´ë¦° ë°˜ë„ì²´ì£¼ëŠ”?"
    JSON Output:
    ```json
    {{"query_type": "stock_analysis", "period": "ì˜¤ëŠ˜", "condition": null, "target": "ë°˜ë„ì²´ì£¼", "action": "ì œì¼ ë§ì´ ë‚´ë¦° ì£¼ì‹"}}
    ```

# --- ë¶„ê¸°/ì‹¤ì  ë° ì¬ë¬´ì§€í‘œ ì¡°ê±´ ì˜ˆì‹œ ---
6.  User Query: "ì˜¬í•´ 1ë¶„ê¸° ì‹¤ì ì´ ì¢‹ì•˜ë˜ IT ì£¼ì‹ ì°¾ì•„ì¤˜"
    JSON Output:
    ```json
    {{"query_type": "stock_analysis", "period": "ì˜¬í•´ 1ë¶„ê¸°", "condition": {{"type": "earnings", "performance": "good"}}, "target": "IT ì£¼ì‹", "action": "ì°¾ì•„ì¤˜"}}
    ```
7.  User Query: "PBRì´ 1ë³´ë‹¤ ë‚®ì€ ìš°ëŸ‰ì£¼ ì•Œë ¤ì¤˜"
    JSON Output:
    ```json
    {{"query_type": "stock_analysis", "period": null, "condition": {{"type": "fundamental", "indicator": "PBR", "operator": "<", "value": 1}}, "target": "ìš°ëŸ‰ì£¼", "action": "ì•Œë ¤ì¤˜"}}
    ```

# --- ë‹¨ì¼ ì¢…ëª© í˜„ì¬ê°€ ì¡°íšŒ ì˜ˆì‹œ ---
8.  User Query: "ì‚¼ì„±ì „ì ì§€ê¸ˆ ì–¼ë§ˆì•¼?"
    JSON Output:
    ```json
    {{"query_type": "single_stock_price", "period": null, "condition": null, "target": "ì‚¼ì„±ì „ì", "action": "í˜„ì¬ê°€ ì¡°íšŒ"}}
    ```
9.  User Query: "í•œí™”ì˜¤ì…˜ ì£¼ê°€ ì•Œë ¤ì¤„ë˜"
    JSON Output:
    ```json
    {{"query_type": "single_stock_price", "period": null, "condition": null, "target": "í•œí™”ì˜¤ì…˜", "action": "í˜„ì¬ê°€ ì¡°íšŒ"}}
    ```
# --- â–¼â–¼â–¼ [ì¶”ê°€] ì½”ìŠ¤ë‹¥ ì¢…ëª© ì˜ˆì‹œ â–¼â–¼â–¼ ---
10. User Query: "ì—ì½”í”„ë¡œë¹„ì—  í˜„ì¬ ì£¼ê°€"
    JSON Output:
    ```json
    {{"query_type": "single_stock_price", "period": null, "condition": null, "target": "ì—ì½”í”„ë¡œë¹„ì— ", "action": "í˜„ì¬ê°€ ì¡°íšŒ"}}
    ```
11. User Query: "ì…€íŠ¸ë¦¬ì˜¨ì œì•½ ì£¼ê°€"
    JSON Output:
    ```json
    {{"query_type": "single_stock_price", "period": null, "condition": null, "target": "ì…€íŠ¸ë¦¬ì˜¨ì œì•½", "action": "í˜„ì¬ê°€ ì¡°íšŒ"}}
    ```
12. User Query: "ì¹´ì¹´ì˜¤ê²Œì„ì¦ˆ ì–¼ë§ˆì—ìš”?"
    JSON Output:
    ```json
    {{"query_type": "single_stock_price", "period": null, "condition": null, "target": "ì¹´ì¹´ì˜¤ê²Œì„ì¦ˆ", "action": "í˜„ì¬ê°€ ì¡°íšŒ"}}
    ```

# --- â–¼â–¼â–¼ [ì¶”ê°€] ë³µí•© ì¡°ê±´ ë° ë°°ë‹¹/ê±°ë˜ëŸ‰ ì˜ˆì‹œ â–¼â–¼â–¼ ---
13. User Query: "ì˜¤ëŠ˜ ê±°ë˜ëŸ‰ì´ ê°€ì¥ ë§ì´ í„°ì§„ ì£¼ì‹ì€?"
    JSON Output:
    ```json
    {{"query_type": "stock_analysis", "period": "ì˜¤ëŠ˜", "condition": {{"type": "volume", "level": "highest"}}, "target": "ì£¼ì‹", "action": "ê±°ë˜ëŸ‰ì´ ê°€ì¥ ë§ì´ í„°ì§„ ì£¼ì‹"}}
    ```
14. User Query: "ê¸ˆë¦¬ ì¸ìƒê¸°ì— ê°€ì¥ ë§ì´ ì˜¬ëë˜ ì€í–‰ì£¼ëŠ”?"
    JSON Output:
    ```json
    {{"query_type": "stock_analysis", "period": "ê¸ˆë¦¬ ì¸ìƒê¸°", "condition": null, "target": "ì€í–‰ì£¼", "action": "ê°€ì¥ ë§ì´ ì˜¬ëë˜ ì£¼ì‹"}}
    ```
15. User Query: "ë°°ë‹¹ìˆ˜ìµë¥  ë†’ì€ í†µì‹ ì£¼ ì•Œë ¤ì¤˜"
    JSON Output:
    ```json
    {{"query_type": "stock_analysis", "period": null, "condition": {{"type": "fundamental", "indicator": "dividend_yield", "operator": ">", "value": "high"}}, "target": "í†µì‹ ì£¼", "action": "ë°°ë‹¹ìˆ˜ìµë¥  ë†’ì€ ì£¼ì‹"}}
    ```

## Example (Plain Text Output):

1.  User Query: "ì£¼ì‹ ì´ˆë³´ì¸ë° ì–´ë–¤ ì¢…ëª©ì´ ì¢‹ì•„?"
    Plain Text Output: ì£¼ì‹ íˆ¬ìë¥¼ ì²˜ìŒ ì‹œì‘í•˜ì‹œëŠ”êµ°ìš”! íŠ¹ì • ì¢…ëª©ì„ ì¶”ì²œí•´ë“œë¦¬ê¸°ë³´ë‹¤ëŠ”, ë¨¼ì € ì‹œê°€ì´ì•¡ì´ í¬ê³  ëª¨ë‘ê°€ ì˜ ì•„ëŠ” ìš°ëŸ‰ì£¼ë¶€í„° ì†Œì•¡ìœ¼ë¡œ ì‹œì‘í•´ë³´ì‹œëŠ” ê²ƒì„ ê¶Œí•´ë“œë ¤ìš”. ì‚¼ì„±ì „ìë‚˜ SKí•˜ì´ë‹‰ìŠ¤ ê°™ì€ ì¢…ëª©ë“¤ì˜ ìµœê·¼ ë‰´ìŠ¤ì™€ ë¦¬í¬íŠ¸ë¥¼ ê¾¸ì¤€íˆ ì‚´í´ë³´ì‹œëŠ” ê²ƒë„ ì¢‹ì€ ê³µë¶€ê°€ ë  ê±°ì˜ˆìš”.
2.  User Query: "3ë…„ ë’¤ì— ì‚¼ì„±ì „ì ì£¼ê°€ ì–¼ë§ˆì¼ê¹Œ?"
    Plain Text Output: ë¯¸ë˜ì˜ ì£¼ê°€ë¥¼ ì •í™•íˆ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì€ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤. ì£¼ê°€ëŠ” ìˆ˜ë§ì€ ê²½ì œ ì§€í‘œ, ì‹œì¥ ìƒí™©, ê¸°ì—…ì˜ ì‹¤ì  ë“±ì— ë”°ë¼ ë³€ë™í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ë‹¤ë§Œ, ê¸°ì—…ì˜ ì¬ë¬´ ìƒíƒœë‚˜ ì„±ì¥ ê°€ëŠ¥ì„±ì„ ë¶„ì„í•˜ë©° ì¥ê¸°ì ì¸ ê´€ì ì—ì„œ íˆ¬ì ê²°ì •ì„ ë‚´ë¦¬ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.
3.  User Query: "ë‹¨íƒ€ë‘ ì¥ê¸°íˆ¬ì ì¤‘ì— ë­ê°€ ë” ë‚˜ì•„?"
    Plain Text Output: ë‹¨íƒ€ì™€ ì¥ê¸°íˆ¬ìëŠ” ê°ìì˜ ì¥ë‹¨ì ì´ ìˆì–´ ì–´ëŠ í•œìª½ì´ ì ˆëŒ€ì ìœ¼ë¡œ ë‚«ë‹¤ê³  ë§í•˜ê¸°ëŠ” ì–´ë µìŠµë‹ˆë‹¤. ë‹¨íƒ€ëŠ” ë¹ ë¥¸ ìˆ˜ìµì„ ê¸°ëŒ€í•  ìˆ˜ ìˆì§€ë§Œ ë†’ì€ ìœ„í—˜ê³¼ ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ ë™ë°˜í•˜ë©°, ì¥ê¸°íˆ¬ìëŠ” ì•ˆì •ì ì´ì§€ë§Œ ìˆ˜ìµì„ ë³´ê¸°ê¹Œì§€ ì˜¤ëœ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë³¸ì¸ì˜ íˆ¬ì ì„±í–¥ê³¼ ëª©í‘œì— ë§ëŠ” ë°©ë²•ì„ ì„ íƒí•˜ëŠ” ê²ƒì´ ê°€ì¥ ì¤‘ìš”í•©ë‹ˆë‹¤.

4.  User Query: "PERì´ ë­ì•¼?"
    Plain Text Output: PER(ì£¼ê°€ìˆ˜ìµë¹„ìœ¨)ì€ ì£¼ê°€ë¥¼ ì£¼ë‹¹ìˆœì´ìµ(EPS)ìœ¼ë¡œ ë‚˜ëˆˆ ê°’ìœ¼ë¡œ, ê¸°ì—…ì´ ë²Œì–´ë“¤ì´ëŠ” ì´ìµì— ë¹„í•´ ì£¼ê°€ê°€ ë†’ê²Œ í˜¹ì€ ë‚®ê²Œ í‰ê°€ë˜ì—ˆëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ëŒ€í‘œì ì¸ íˆ¬ì ì§€í‘œì…ë‹ˆë‹¤. PERì´ ë‚®ì„ìˆ˜ë¡ ì£¼ê°€ê°€ ì €í‰ê°€ë˜ì—ˆë‹¤ê³  í•´ì„í•˜ëŠ” ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤.
5.  User Query: "ì•ˆë…•í•˜ì„¸ìš”"
    Plain Text Output: ì•ˆë…•í•˜ì„¸ìš”! ê¸ˆìœµ ë¶„ì„ AIì…ë‹ˆë‹¤. ì£¼ì‹ì´ë‚˜ ê²½ì œì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ì‹œë©´ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”.

## Task:
User Query: "{user_query}"
Your Output:
"""

except Exception as e:
    print(f"AskFin Blueprint: ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨ - {e}")
    model = None

def initialize_global_data():
    """
    ì„œë²„ ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ í˜¸ì¶œë˜ì–´ ì „ì—­ìœ¼ë¡œ ì‚¬ìš©ë  ì£¼ì‹ ê¸°ë³¸ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ìºì‹œí•©ë‹ˆë‹¤.
    """
    global GLOBAL_KRX_LISTING, GLOBAL_TICKER_NAME_MAP, GLOBAL_NAME_TICKER_MAP

    print("[ì• í”Œë¦¬ì¼€ì´ì…˜ ì´ˆê¸°í™”] í•„ìˆ˜ ì£¼ì‹ ë°ì´í„° ë¡œë”© ì‹œì‘...")
    try:
        print("  - KOSPI ë° KOSDAQ ì¢…ëª© ëª©ë¡ (FDR) ë¡œë”© ì¤‘...")
        GLOBAL_KRX_LISTING = fdr.StockListing('KRX')
        print(f"  - ì¢…ëª© ëª©ë¡ ë¡œë”© ì™„ë£Œ. ì´ {len(GLOBAL_KRX_LISTING)}ê°œ ì¢…ëª©.")
        print(f"  - GLOBAL_KRX_LISTING ì»¬ëŸ¼: {GLOBAL_KRX_LISTING.columns.tolist()}")

        GLOBAL_KRX_LISTING['FullCode'] = GLOBAL_KRX_LISTING.apply(
            lambda row: f"{row['Code']}.KQ" if row['Market'] == 'KOSDAQ' else f"{row['Code']}.KS", axis=1
        )
        print(f"  - GLOBAL_KRX_LISTINGì— 'FullCode' ì»¬ëŸ¼ ì¶”ê°€ ì™„ë£Œ.")

        # 2. ì¢…ëª© ì½”ë“œ <-> ì´ë¦„ ë§¤í•‘ ë¡œë”© (pykrx)
        print("  - ì¢…ëª© ì½”ë“œ/ì´ë¦„ ë§¤í•‘ (pykrx) ë¡œë”© ì¤‘...")
        all_tickers = stock.get_market_ticker_list(market="ALL")
        GLOBAL_TICKER_NAME_MAP = {ticker: stock.get_market_ticker_name(ticker) for ticker in all_tickers}
        GLOBAL_NAME_TICKER_MAP = {name: ticker for ticker, name in GLOBAL_TICKER_NAME_MAP.items()}
        print(f"  - ì¢…ëª© ì½”ë“œ/ì´ë¦„ ë§¤í•‘ ìƒì„± ì™„ë£Œ. ì´ {len(GLOBAL_NAME_TICKER_MAP)}ê°œ ë§¤í•‘.")


        print("[ì• í”Œë¦¬ì¼€ì´ì…˜ ì´ˆê¸°í™”] ëª¨ë“  í•„ìˆ˜ ì£¼ì‹ ë°ì´í„° ë¡œë”© ì™„ë£Œ.")

    except Exception as e:
        print(f"[ì´ˆê¸°í™” ì˜¤ë¥˜] í•„ìˆ˜ ì£¼ì‹ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {e}")
        traceback.print_exc()


def _load_ticker_maps():
    """
    ì¢…ëª© ì •ë³´ ë§µì„ ì „ì—­ ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ë„ë¡ ë³€ê²½.
    ì´ í•¨ìˆ˜ëŠ” initialize_global_data()ê°€ í˜¸ì¶œëœ í›„ì—ë§Œ ìœ íš¨í•©ë‹ˆë‹¤.
    """
    global GLOBAL_TICKER_NAME_MAP, GLOBAL_NAME_TICKER_MAP
    if GLOBAL_NAME_TICKER_MAP is None:
        print("ê²½ê³ : _load_ticker_maps() í˜¸ì¶œ ì‹œ ê¸€ë¡œë²Œ ì¢…ëª© ë§µì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê°•ì œë¡œ ì´ˆê¸°í™” ì‹œë„.")
        initialize_global_data()


def analyze_institutional_buying(start_date, end_date):
    """
    ì£¼ì–´ì§„ ê¸°ê°„ ë™ì•ˆ ê¸°ê´€ì˜ ìˆœë§¤ìˆ˜ ëŒ€ê¸ˆì„ ê¸°ì¤€ìœ¼ë¡œ ìƒìœ„ ì¢…ëª©ì„ ë¶„ì„í•©ë‹ˆë‹¤.
    """
    print(f"DEBUG: {start_date} ~ {end_date} ê¸°ê°„ì˜ ê¸°ê´€ ìˆœë§¤ìˆ˜ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    try:
        df_kospi = stock.get_market_trading_value_by_date(start_date, end_date, "KOSPI")
        df_kosdaq = stock.get_market_trading_value_by_date(start_date, end_date, "KOSDAQ")
        
        df_all = pd.concat([df_kospi, df_kosdaq]).reset_index()

        if 'ê¸°ê´€ê³„' in df_all.columns:
            df_all.rename(columns={'ê¸°ê´€ê³„': 'ê¸°ê´€'}, inplace=True)

        if 'ê¸°ê´€' not in df_all.columns:
            print("DEBUG: íˆ¬ììë³„ ê±°ë˜ëŒ€ê¸ˆ ë°ì´í„°ì— 'ê¸°ê´€' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return []

        institutional_net_buy = df_all.groupby('í‹°ì»¤')['ê¸°ê´€'].sum().sort_values(ascending=False)
        
        top_stocks = institutional_net_buy.head(50).reset_index()
        
        if GLOBAL_TICKER_NAME_MAP is None:
            initialize_global_data()
            
        analysis_results = []
        for index, row in top_stocks.iterrows():
            ticker = row['í‹°ì»¤']
            net_buy_value = row['ê¸°ê´€']
            
            analysis_results.append({
                "code": ticker,
                "name": GLOBAL_TICKER_NAME_MAP.get(ticker, "N/A"),
                "value": round(net_buy_value / 1_0000_0000, 2), # ì–µ ë‹¨ìœ„ë¡œ ë³€í™˜
                "label": "ê¸°ê´€ ìˆœë§¤ìˆ˜(ì–µ ì›)",
            })
        
        print(f"DEBUG: ê¸°ê´€ ìˆœë§¤ìˆ˜ ìƒìœ„ {len(analysis_results)}ê°œ ì¢…ëª© ë¶„ì„ ì™„ë£Œ.")
        return analysis_results

    except Exception as e:
        print(f"ê¸°ê´€ ìˆœë§¤ìˆ˜ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        traceback.print_exc()
        return []
    

def _get_fdr_indicator(indicator_info, intent_json):
    """FinanceDataReaderë¥¼ í†µí•´ ì¼ë³„ ì§€í‘œë¥¼ ì¡°íšŒí•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ëŠ” í—¬í¼ í•¨ìˆ˜"""
    try:
        name = indicator_info['name']
        code = indicator_info['code']


        period_str = intent_json.get("period")
        req_start_date, req_end_date = parse_period(period_str)
        query_start_date = req_end_date - timedelta(days=90) 
        query_end_date = req_end_date 
        
        data = fdr.DataReader(code, query_start_date, query_end_date) 
        
        if data.empty:
            return {"error": f"{name} ë°ì´í„° ì¡°íšŒì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."}

        target_data_in_period = data.loc[data.index <= req_end_date].sort_index(ascending=True)

        if target_data_in_period.empty:
             return {"error": f"{name} ì§€í‘œì— ëŒ€í•´ ìš”ì²­í•˜ì‹  ê¸°ê°„({req_end_date.strftime('%Yë…„ %mì›” %dì¼')})ì˜ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}

        latest_for_period = target_data_in_period['Close'].iloc[-1]
        latest_date_for_period = target_data_in_period.index[-1]

        previous_data_in_period = data.loc[data.index < latest_date_for_period].sort_index(ascending=True)
        previous_for_period = None
        if not previous_data_in_period.empty:
            previous_for_period = previous_data_in_period['Close'].iloc[-1]

        if previous_for_period is not None:
            change = latest_for_period - previous_for_period
            change_str = f"{abs(change):.2f} ìƒìŠ¹" if change > 0 else f"{abs(change):.2ff} í•˜ë½" if change < 0 else "ë³€ë™ ì—†ìŒ"
            result_sentence = f"ìš”ì²­í•˜ì‹  ê¸°ê°„ì˜ ë§ˆì§€ë§‰({latest_date_for_period.strftime('%Yë…„ %mì›” %dì¼')}) {name}ëŠ”(ì€) {latest_for_period:,.2f}ì´ë©°, ì§ì „ ì˜ì—…ì¼ ëŒ€ë¹„ {change_str}í–ˆìŠµë‹ˆë‹¤."
        else:
            result_sentence = f"ìš”ì²­í•˜ì‹  ê¸°ê°„ì˜ ë§ˆì§€ë§‰({latest_date_for_period.strftime('%Yë…„ %mì›” %dì¼')}) {name}ëŠ”(ì€) {latest_for_period:,.2f}ì…ë‹ˆë‹¤. ì§ì „ ì˜ì—…ì¼ ë°ì´í„°ê°€ ì—†ì–´ ë³€ë™ ì •ë³´ë¥¼ ì œê³µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        return {
            "query_intent": intent_json,
            "analysis_subject": name,
            "result": [result_sentence]
        }
    except Exception as e:
        return {"error": f"{indicator_info.get('name', 'ì•Œìˆ˜ì—†ëŠ”')} ì§€í‘œ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"}

def _get_bok_indicator(indicator_info, intent_json):
    """í•œêµ­ì€í–‰(BOK) APIë¥¼ í†µí•´ ì›”ë³„ ì§€í‘œë¥¼ ì¡°íšŒí•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ëŠ” í—¬í¼ í•¨ìˆ˜"""
    try:
        name = indicator_info['name']
        bok_api_key = os.getenv("ECOS_API_KEY")
        if not bok_api_key: return {"error": "í•œêµ­ì€í–‰ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}

        end_date = datetime.now().strftime('%Y%m')
        start_date = (datetime.now() - timedelta(days=120)).strftime('%Y%m')
        url = (f"https://ecos.bok.or.kr/api/StatisticSearch/{bok_api_key}/json/kr/1/10/"
               f"{indicator_info['stats_code']}/MM/{start_date}/{end_date}/{indicator_info['item_code']}")

        response = requests.get(url, timeout=10).json()
        rows = response.get("StatisticSearch", {}).get("row", [])
        
        if len(rows) < 2:
            return {"error": f"ìµœê·¼ {name} ë°ì´í„°ë¥¼ ë¹„êµí•  ë§Œí¼ ì¶©ë¶„íˆ ì¡°íšŒí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
            
        latest = rows[-1]
        previous = rows[-2]
        latest_date = f"{latest['TIME'][:4]}ë…„ {latest['TIME'][4:]}ì›”"
        change = float(latest['DATA_VALUE']) - float(previous['DATA_VALUE'])
        change_str = f"{abs(change):.2f} ìƒìŠ¹" if change > 0 else f"{abs(change):.2f} í•˜ë½" if change < 0 else "ë³€ë™ ì—†ìŒ"

        result_sentence = (f"ê°€ì¥ ìµœê·¼({latest_date}) {name}ëŠ”(ì€) {latest['DATA_VALUE']}ì´ë©°, ì „ì›” ëŒ€ë¹„ {change_str}í–ˆìŠµë‹ˆë‹¤.")
        
        return {
            "query_intent": intent_json,
            "analysis_subject": name,
            "result": [result_sentence]
        }
    except Exception as e:
        return {"error": f"í•œêµ­ì€í–‰(BOK) ì§€í‘œ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"}

def execute_comparison_analysis(intent_json):
    """
    ì—¬ëŸ¬ í…Œë§ˆë¥¼ ë¹„êµ ë¶„ì„í•˜ì—¬ ê°€ì¥ ì„±ê³¼ê°€ ì¢‹ì€/ë‚˜ìœ í…Œë§ˆë¥¼ ì°¾ì•„ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜.
    """
    try:
        targets = intent_json.get("target", [])
        period_str = intent_json.get("period")
        action_str = intent_json.get("action", "")

        if not isinstance(targets, list) or len(targets) < 2:
            return {"error": "ë¹„êµ ë¶„ì„ì„ ìœ„í•´ì„œëŠ” ë‘ ê°œ ì´ìƒì˜ ëŒ€ìƒì´ í•„ìš”í•©ë‹ˆë‹¤."}

        start_date, end_date = parse_period(period_str)
        analysis_period_info = f"{start_date.strftime('%Y-%m-%d')} ~ {end_date.strftime('%Y-%m-%d')}"
        
        comparison_results = []

        print(f"ë¹„êµ ë¶„ì„ ì‹œì‘: {targets}")
        for theme in targets:
            print(f"  - '{theme}' í…Œë§ˆ ë¶„ì„ ì¤‘...")
            target_stocks, _ = get_target_stocks(theme)
            if target_stocks.empty:
                print(f"    -> '{theme}'ì— í•´ë‹¹í•˜ëŠ” ì¢…ëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
                continue


            performance_data = analyze_top_performers(target_stocks, [(start_date, end_date)], (start_date, end_date))
            
            if not performance_data:
                print(f"    -> '{theme}' í…Œë§ˆì˜ ì„±ê³¼ë¥¼ ë¶„ì„í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
                continue

            valid_returns = [item['value'] for item in performance_data if 'value' in item and pd.notna(item['value'])]
            if not valid_returns:
                continue

            average_return = statistics.mean(valid_returns)
            
            comparison_results.append({
                "theme": theme,
                "average_return": round(average_return, 2)
            })
            print(f"    -> '{theme}' í…Œë§ˆ í‰ê·  ìˆ˜ìµë¥ : {average_return:.2f}%")

        if not comparison_results:
            return {"error": "ìš”ì²­í•˜ì‹  í…Œë§ˆë“¤ì˜ ìˆ˜ìµë¥ ì„ ë¶„ì„í•  ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤."}

        reverse_sort = "ë‚´ë¦°" not in action_str
        sorted_results = sorted(comparison_results, key=lambda x: x['average_return'], reverse=reverse_sort)
        
        result_text = f"**'{', '.join(targets)}' í…Œë§ˆ ë¹„êµ ë¶„ì„ ê²°ê³¼**<br><br>"
        result_text += f"**ë¶„ì„ ê¸°ê°„:** {analysis_period_info}<br><br>"
        
        result_text += "| ìˆœìœ„ | í…Œë§ˆ | ì£¼ìš” ì¢…ëª© í‰ê·  ìˆ˜ìµë¥  |\n"
        result_text += "| :--- | :--- | :--- |\n"
        for i, result in enumerate(sorted_results):
            result_text += f"| {i+1} | **{result['theme']}** | **{result['average_return']:.2f}%** |\n"
        
        result_text += "<br>*ë³¸ ë¶„ì„ì€ ê° í…Œë§ˆì— í¬í•¨ëœ ì‹œê°€ì´ì•¡ ìƒìœ„ ì¢…ëª©ë“¤ì„ ê¸°ì¤€ìœ¼ë¡œ ê³„ì‚°ë˜ì—ˆìœ¼ë©°, ì‹¤ì œ ìˆ˜ìµë¥ ê³¼ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ì •ë³´ëŠ” íˆ¬ì ì¶”ì²œì´ ì•„ë‹ˆë©°, ì°¸ê³  ìë£Œë¡œë§Œ í™œìš©í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.*"

        return {
            "query_intent": intent_json,
            "analysis_subject": "í…Œë§ˆ ë¹„êµ ë¶„ì„",
            "result": [result_text]
        }
    except Exception as e:
        traceback.print_exc()
        return {"error": f"ë¹„êµ ë¶„ì„ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"}
    
def execute_indicator_lookup(intent_json):
    """
    [ìµœì¢… ìˆ˜ì •] ì—¬ëŸ¬ ì†ŒìŠ¤ì˜ ê²½ì œ ì§€í‘œë¥¼ ì¡°íšŒí•˜ê³  ì±—ë´‡ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜
    """
    target_query = intent_json.get("target", "").lower() # ì‚¬ìš©ì ì¿¼ë¦¬ì˜ ëŒ€ìƒ ì§€í‘œ (ì†Œë¬¸ìë¡œ ë³€í™˜í•˜ì—¬ ë¹„êµ ìš©ì´)

    FDR_INDICATOR_MAP = {
        "í™˜ìœ¨": {"code": "USD/KRW", "name": "ì›/ë‹¬ëŸ¬ í™˜ìœ¨"},
        "ì›ë‹¬ëŸ¬í™˜ìœ¨": {"code": "USD/KRW", "name": "ì›/ë‹¬ëŸ¬ í™˜ìœ¨"},
        "ìœ ê°€": {"code": "CL=F", "name": "WTI êµ­ì œ ìœ ê°€"},
        "wti": {"code": "CL=F", "name": "WTI êµ­ì œ ìœ ê°€"},
        "ê¸ˆê°’": {"code": "GC=F", "name": "ê¸ˆ ì„ ë¬¼"},
        "ê¸ˆê°€ê²©": {"code": "GC=F", "name": "ê¸ˆ ì„ ë¬¼"},
        "ë¯¸êµ­ì±„10ë…„": {"code": "US10YT=X", "name": "ë¯¸ 10ë…„ë¬¼ êµ­ì±„ ê¸ˆë¦¬"},
        "ë¯¸êµ­10ë…„êµ­ì±„": {"code": "US10YT=X", "name": "ë¯¸ 10ë…„ë¬¼ êµ­ì±„ ê¸ˆë¦¬"},
        "ì½”ìŠ¤í”¼": {"code": "KS11", "name": "ì½”ìŠ¤í”¼ ì§€ìˆ˜"},
        "ì½”ìŠ¤ë‹¥": {"code": "KQ11", "name": "ì½”ìŠ¤ë‹¥ ì§€ìˆ˜"},
    }
    
    BOK_INDICATOR_MAP = {
        "cpi": {"stats_code": "901Y001", "item_code": "0", "name": "ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜"},
        "ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜": {"stats_code": "901Y001", "item_code": "0", "name": "ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜"},
        "ì†Œë¹„ìë¬¼ê°€": {"stats_code": "901Y001", "item_code": "0", "name": "ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜"}, # ì¶”ê°€
        "ë¬¼ê°€ì§€ìˆ˜": {"stats_code": "901Y001", "item_code": "0", "name": "ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜"}, # ì¶”ê°€
        "ë¬¼ê°€": {"stats_code": "901Y001", "item_code": "0", "name": "ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜"}, # ì¶”ê°€

        "ê¸°ì¤€ê¸ˆë¦¬": {"stats_code": "722Y001", "item_code": "0001000", "name": "í•œêµ­ ê¸°ì¤€ê¸ˆë¦¬"},
        "í•œêµ­ê¸°ì¤€ê¸ˆë¦¬": {"stats_code": "722Y001", "item_code": "0001000", "name": "í•œêµ­ ê¸°ì¤€ê¸ˆë¦¬"},
        "ê¸ˆë¦¬": {"stats_code": "722Y001", "item_code": "0001000", "name": "í•œêµ­ ê¸°ì¤€ê¸ˆë¦¬"}, # ì¶”ê°€
    }

    for key, indicator_info in FDR_INDICATOR_MAP.items():
        if key in target_query or indicator_info['name'].lower() in target_query:
            result = _get_fdr_indicator(indicator_info, intent_json)
            if result and "error" not in result:
                return result
            return {
                "query_intent": intent_json,
                "analysis_subject": "ì§€í‘œ ì¡°íšŒ ì‹¤íŒ¨",
                "result": [f"'{indicator_info['name']}' ì§€í‘œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” ë° ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."]
            }
    # ì„ì˜ë¡œ ìœ ì‚¬ë„ ì¶”ê°€
    for key, indicator_info in BOK_INDICATOR_MAP.items():
        similarity_score = fuzz.ratio(target_query, key.lower())
        if similarity_score > 70: 
            similarity_score_name = fuzz.ratio(target_query, indicator_info['name'].lower())
            if similarity_score_name > 70 or similarity_score > 70:
                result = _get_bok_indicator(indicator_info, intent_json)
                if result and "error" not in result:
                    return result
            return {
                "query_intent": intent_json,
                "analysis_subject": "ì§€í‘œ ì¡°íšŒ ì‹¤íŒ¨",
                "result": [f"'{indicator_info['name']}' ì§€í‘œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” ë° ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."]
            }

    return {
        "query_intent": intent_json,
        "analysis_subject": "ì§€í‘œ ì¡°íšŒ ì‹¤íŒ¨",
        "result": [f"ìš”ì²­í•˜ì‹  '{intent_json.get('target', 'ì§€í‘œ')}'ëŠ” ì§€ì›í•˜ì§€ ì•ŠëŠ” ì§€í‘œì´ê±°ë‚˜ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (ì§€ì› ì§€í‘œ: í™˜ìœ¨, ìœ ê°€, ê¸ˆê°’, ë¯¸êµ­ì±„10ë…„, ì½”ìŠ¤í”¼, ì½”ìŠ¤ë‹¥, CPI, ê¸°ì¤€ê¸ˆë¦¬)"]
    }




@askfin_bp.route('/stock/<code>/profile')
def get_stock_profile(code):
    """
    [ìˆ˜ì •] DART APIë¥¼ ì‚¬ìš©í•˜ì—¬ 'ì£¼ìš” ê³µì‹œ' ëª©ë¡ì„ ì¡°íšŒí•˜ë„ë¡ ë³€ê²½í•©ë‹ˆë‹¤.
    """
    response_data = {}
    company_name = None
    now = time.time()
    #  12ì‹œê°„(43200ì´ˆ)ì´ ì§€ë‚˜ì§€ ì•Šì•˜ìœ¼ë©´ ìºì‹œëœ ë°ì´í„° ì‚¬ìš©
    if code in STOCK_DETAIL_CACHE and (now - STOCK_DETAIL_CACHE[code]['timestamp'] < 43200):
        print(f"âœ… CACHE HIT: ì¢…ëª©ì½”ë“œ '{code}'ì˜ ìƒì„¸ ì •ë³´ë¥¼ ìºì‹œì—ì„œ ë°˜í™˜í•©ë‹ˆë‹¤.")
        return jsonify(STOCK_DETAIL_CACHE[code]['data'])

    print(f"ğŸ”¥ CACHE MISS: ì¢…ëª©ì½”ë“œ '{code}'ì˜ ìƒì„¸ ì •ë³´ë¥¼ APIë¥¼ í†µí•´ ìƒˆë¡œ ì¡°íšŒí•©ë‹ˆë‹¤.")

    try:
        profile_data = {}
        latest_business_day = stock.get_nearest_business_day_in_a_week()

        if GLOBAL_KRX_LISTING is None:
            initialize_global_data()
        
        krx_list = GLOBAL_KRX_LISTING
        target_info = krx_list[krx_list['Code'] == code]
        if target_info.empty:
            return jsonify({"error": f"ì¢…ëª©ì½”ë“œ '{code}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}), 404
        
        target_info = target_info.iloc[0]
        market = target_info.get('Market', 'KOSPI')
        sector = target_info.get('Sector')
        company_name = target_info.get('Name', 'N/A')
        
        profile_data['ê¸°ì—…ëª…'] = company_name
        profile_data['ì—…ì¢…'] = sector
        profile_data['ì£¼ìš”ì œí’ˆ'] = target_info.get('Industry', 'N/A')

        df_ohlcv = stock.get_market_ohlcv(latest_business_day, market=market)
        df_cap = stock.get_market_cap(latest_business_day, market=market)
        df_funda = stock.get_market_fundamental(latest_business_day, market=market)

        current_price = df_ohlcv.loc[code, 'ì¢…ê°€']
        market_cap = df_cap.loc[code, 'ì‹œê°€ì´ì•¡']
        funda = df_funda.loc[code]

        profile_data['í˜„ì¬ê°€'] = f"{current_price:,} ì›"
        profile_data['ì‹œê°€ì´ì•¡'] = f"{market_cap / 1_0000_0000_0000:.2f} ì¡°ì›" if market_cap > 1_0000_0000_0000 else f"{market_cap / 1_0000_0000:.2f} ì–µì›"
        
        eps = funda.get('EPS', 0)
        profile_data['PER'] = f"{funda.get('PER', 0):.2f} ë°°"
        profile_data['PBR'] = f"{funda.get('PBR', 0):.2f} ë°°"
        profile_data['ë°°ë‹¹ìˆ˜ìµë¥ '] = f"{funda.get('DIV', 0):.2f} %"
        
        response_data["company_profile"] = profile_data

    except Exception as e:
        traceback.print_exc()
        response_data["profile_error"] = f"ê¸°ì—… ê°œìš” ì •ë³´ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"


    if company_name:
        try:
            corp_list = dart.get_corp_list()
            
            # --- START OF MODIFICATION ---
            # Search for the corporation by name
            found_corps = corp_list.find_by_corp_name(company_name, exactly=True)
            
            corp = None
            if found_corps: # Check if the list is not empty
                corp = found_corps[0] # Assign the first found corporation
            
            if not corp: # Explicitly check if corp is None after assignment
                raise ValueError(f"DARTì—ì„œ '{company_name}'ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            # --- END OF MODIFICATION ---

            reports = corp.search_filings(bgn_de=(datetime.now() - timedelta(days=365)).strftime('%Y%m%d'), last_reprt_at='Y')
            response_data["report_list"] = [{
                'report_nm': r.report_nm, 'flr_nm': r.flr_nm, 'rcept_dt': r.rcept_dt,
                'url': f"http://dart.fss.or.kr/dsaf001/main.do?rcpNo={r.rcept_no}"
            } for r in reports[:15]] if reports else []
            
            api_url = "https://opendart.fss.or.kr/api/fnlttSinglAcnt.json"
            fs_data_list = [] 
            
            current_year = datetime.now().year
            years_to_fetch = [str(year) for year in range(current_year, current_year - 4, -1)]

            for year in years_to_fetch:
                for reprt_code in ['11011', '11013', '11012']:
                    params = {
                        'crtfc_key': DART_API_KEY,
                        'corp_code': corp.corp_code, # 'corp' is now guaranteed to be defined here
                        'bsns_year': str(year),
                        'reprt_code': reprt_code,
                        'fs_div': 'CFS'
                    }
                    res = requests.get(api_url, params=params)
                    data = res.json()

                    if data.get('status') == '000' and data.get('list'):
                        for item in data['list']:
                            if 'thstrm_end_dt' in item and item['thstrm_end_dt']:
                                item['report_date_key'] = item['thstrm_end_dt']
                            elif 'thstrm_dt' in item and item['thstrm_dt']:
                                date_range_str = item['thstrm_dt']
                                match = re.search(r'(\d{4}\.\d{2}\.\d{2})$', date_range_str.strip())
                                if match:
                                    item['report_date_key'] = match.group(1)
                                else:
                                    item['report_date_key'] = date_range_str.strip()
                            else:
                                item['report_date_key'] = None
                        fs_data_list.extend(data['list'])
                        break
                    elif params['fs_div'] == 'CFS':
                        params['fs_div'] = 'OFS'
                        res = requests.get(api_url, params=params)
                        data = res.json()
                        if data.get('status') == '000' and data.get('list'):
                             for item in data['list']:
                                if 'thstrm_end_dt' in item and item['thstrm_end_dt']:
                                    item['report_date_key'] = item['thstrm_end_dt']
                                elif 'thstrm_dt' in item and item['thstrm_dt']:
                                    date_range_str = item['thstrm_dt']
                                    match = re.search(r'(\d{4}\.\d{2}\.\d{2})$', date_range_str.strip())
                                    if match:
                                        item['report_date_key'] = match.group(1)
                                    else:
                                        item['report_date_key'] = date_range_str.strip()
                                else:
                                    item['report_date_key'] = None
                             fs_data_list.extend(data['list'])
                             break

            if fs_data_list:
                df = pd.DataFrame(fs_data_list)
                
                df = df.drop_duplicates(subset=['rcept_no', 'account_nm'], keep='last')
                
                df['thstrm_amount'] = df['thstrm_amount'].astype(str).str.replace(',', '', regex=False).str.strip()
                df['thstrm_amount'] = pd.to_numeric(df['thstrm_amount'], errors='coerce')
                df.dropna(subset=['thstrm_amount'], inplace=True)
                
                def extract_and_clean_date(date_string): # This helper function is now defined here again or needs to be outside if used multiple places
                    if not isinstance(date_string, str): # Add this check for safety
                        return None
                    match = re.search(r'(\d{4}\.\d{2}\.\d{2})$', date_string.strip())
                    if match:
                        return match.group(1)
                    parts = date_string.split('~')
                    if len(parts) > 1:
                        match = re.search(r'(\d{4}\.\d{2}\.\d{2})$', parts[-1].strip())
                        if match:
                            return match.group(1)
                    return None
                
                # Use the report_date_key if it was set, otherwise apply the cleaning to thstrm_dt
                if 'report_date_key' not in df.columns: # Fallback if direct assignment didn't happen for all
                    df['report_date_key'] = df['thstrm_dt'].astype(str).apply(extract_and_clean_date)


                df['report_date_key'] = pd.to_datetime(df['report_date_key'], format='%Y.%m.%d', errors='coerce')
                df.dropna(subset=['report_date_key'], inplace=True)
                
                df_pivot = df.pivot_table(index='account_nm', columns='report_date_key', values='thstrm_amount', aggfunc='first')
                df_pivot = df_pivot.fillna(0)

                df_pivot = df_pivot.sort_index(axis=1) 
                
                display_columns = df_pivot.columns[-4:] if len(df_pivot.columns) >= 4 else df_pivot.columns
                
                response_data["key_financial_info"] = json.dumps(
                    {
                        "columns": [col.strftime('%Y.%m') for col in display_columns],
                        "index": list(df_pivot.index),
                        "data": df_pivot[display_columns].values.tolist()
                    }, ensure_ascii=False
                )
                print("2024ë…„ë„ ì£¼ìš” ì¬ë¬´ì •ë³´ API í˜¸ì¶œ ì„±ê³µ ë° ì²˜ë¦¬ ì™„ë£Œ")
            else:
                response_data["financials_error"] = "ì£¼ìš” ì¬ë¬´ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. DART APIì—ì„œ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜ íŒŒì‹± ì˜¤ë¥˜."

        except Exception as e:
            print(f"ì¬ë¬´ì œí‘œ ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì¹˜ëª…ì ì¸ ì˜¤ë¥˜ ë°œìƒ: {e}")
            traceback.print_exc()
            response_data["financials_error"] = f"ì¬ë¬´ì œí‘œë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}"
    if "error" not in response_data:
        STOCK_DETAIL_CACHE[code] = {
            'data': response_data,
            'timestamp': now
        }
        
    return jsonify(response_data)

def get_target_stocks(target_str):
    """
    [ìˆ˜ì •ë¨] íƒ€ê²Ÿ ë¬¸ìì—´ì— í•´ë‹¹í•˜ëŠ” ì¢…ëª© ë¦¬ìŠ¤íŠ¸(DataFrame)ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜ (ìºì‹œëœ ë°ì´í„° ì‚¬ìš©)
    """
    global GLOBAL_KRX_LISTING, GLOBAL_NAME_TICKER_MAP

    if GLOBAL_KRX_LISTING is None:
        print("ê²½ê³ : get_target_stocks() í˜¸ì¶œ ì‹œ GLOBAL_KRX_LISTINGì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê°•ì œë¡œ ì´ˆê¸°í™” ì‹œë„.")
        initialize_global_data()
        if GLOBAL_KRX_LISTING is None:
            return pd.DataFrame(columns=['Name', 'Code']), "ì´ˆê¸°í™” ì‹¤íŒ¨"

    krx = GLOBAL_KRX_LISTING 

    GENERIC_TARGETS = {"ì£¼ì‹", "ì¢…ëª©", "ê¸‰ë“±ì£¼", "ìš°ëŸ‰ì£¼", "ì¸ê¸°ì£¼", "ì „ì²´"}
    
    analysis_subject = "ì‹œì¥ ì „ì²´"
    target_stocks = krx 

    import os 

    if target_str and target_str.strip() and target_str not in GENERIC_TARGETS:
        analysis_subject = f"'{target_str}'"
        
        keyword = target_str.replace(" ê´€ë ¨ì£¼", "").replace(" í…Œë§ˆì£¼", "").replace(" í…Œë§ˆ", "").replace("ì£¼", "").strip()
        lower_keyword = keyword.lower()

        print(f"--- ë””ë²„ê·¸ ì‹œì‘ (get_target_stocks) ---")
        print(f"ë””ë²„ê·¸: ì‚¬ìš©ì ì…ë ¥ í‚¤ì›Œë“œ: '{keyword}' (ì†Œë¬¸ì: '{lower_keyword}')")

        themes_from_file = {}
        try:
            themes_file_path = os.path.join(os.path.dirname(__file__), '..', 'cache', 'themes.json')

            print(f"ë””ë²„ê·¸: themes.jsonì„ ì°¾ì„ ê²½ë¡œ: {themes_file_path}") # ë””ë²„ê·¸ ì¶œë ¥ ì¶”ê°€

            with open(themes_file_path, 'r', encoding='utf-8') as f:
                themes_from_file = json.load(f)
            print(f"ë””ë²„ê·¸: 'themes.json' íŒŒì¼ ë¡œë“œ ì„±ê³µ. ì´ {len(themes_from_file)}ê°œ í…Œë§ˆ.")
            print(f"ë””ë²„ê·¸: themes.json í‚¤ ëª©ë¡ (ìƒìœ„ 5ê°œ): {list(themes_from_file.keys())[:5]}...")
        except FileNotFoundError:
            print("ê²½ê³ : 'themes.json' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        except Exception as e:
            print(f"ê²½ê³ : 'themes.json' íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

        found_by_theme_file = False
        target_codes_from_theme = []

        for theme_name_in_file, stock_list_in_file in themes_from_file.items():
            print(f"ë””ë²„ê·¸: '{lower_keyword}' vs '{theme_name_in_file.lower()}' ë§¤ì¹­ ì‹œë„...")
            
            if (lower_keyword == theme_name_in_file.lower() or 
                lower_keyword in theme_name_in_file.lower() or 
                theme_name_in_file.lower() in lower_keyword): 
                
                print(f"ë””ë²„ê·¸: themes.jsonì—ì„œ í…Œë§ˆ '{theme_name_in_file}'ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
                for stock_info in stock_list_in_file:
                    if isinstance(stock_info, dict) and 'code' in stock_info:
                        target_codes_from_theme.append(stock_info['code'])
                    elif isinstance(stock_info, str) and len(stock_info) == 6 and stock_info.isdigit(): # ì½”ë“œê°€ ë¬¸ìì—´ë¡œ ì§ì ‘ ì €ì¥ëœ ê²½ìš°
                        target_codes_from_theme.append(stock_info)
                analysis_subject = f"'{theme_name_in_file}' í…Œë§ˆ"
                found_by_theme_file = True
                break
        
        print(f"ë””ë²„ê·¸: themes.jsonì—ì„œ ì¶”ì¶œëœ ì¢…ëª© ì½”ë“œ ìˆ˜: {len(target_codes_from_theme)}")
        if len(target_codes_from_theme) > 0:
            print(f"ë””ë²„ê·¸: ì¶”ì¶œëœ ì²« 5ê°œ ì¢…ëª© ì½”ë“œ: {target_codes_from_theme[:5]}")

        if found_by_theme_file:

            print(f"ë””ë²„ê·¸: GLOBAL_KRX_LISTINGì˜ ì²« 5ê°œ í–‰:\n{krx.head()}")

            codes_in_krx_check = krx[krx['Code'].isin(target_codes_from_theme)]
            print(f"ë””ë²„ê·¸: GLOBAL_KRX_LISTINGì— ì¡´ì¬í•˜ëŠ” í…Œë§ˆ ì¢…ëª© ì½”ë“œ ìˆ˜: {len(codes_in_krx_check)}")
            if len(codes_in_krx_check) == 0 and len(target_codes_from_theme) > 0:
                print("ë””ë²„ê·¸: ê²½ê³ ! themes.jsonì˜ ì¢…ëª© ì½”ë“œ ì¤‘ GLOBAL_KRX_LISTINGì— ë§¤ì¹­ë˜ëŠ” ê²ƒì´ ì—†ìŠµë‹ˆë‹¤. ì½”ë“œ í˜•ì‹ ë¶ˆì¼ì¹˜ ê°€ëŠ¥ì„±.")
                if target_codes_from_theme:
                    print(f"ë””ë²„ê·¸: themes.json ì²« ì¢…ëª© ì½”ë“œ: '{target_codes_from_theme[0]}'")
                if not krx.empty:
                    print(f"ë””ë²„ê·¸: GLOBAL_KRX_LISTING ì²« ì¢…ëª© ì½”ë“œ: '{krx.iloc[0]['Code']}'")


            target_stocks = krx[krx['Code'].isin(target_codes_from_theme)]
            print(f"ë””ë²„ê·¸: ìµœì¢… í•„í„°ë§ëœ target_stocks ê°œìˆ˜: {len(target_stocks)}")
        
        else:
            # 2. ê¸°ì¡´ FinanceDataReader 'Industry' ì»¬ëŸ¼ (í˜¹ì‹œ ì¡´ì¬í•œë‹¤ë©´)ì„ í†µí•œ ê²€ìƒ‰ (í…Œë§ˆ íŒŒì¼ ì—†ì„ ë•Œì˜ í´ë°±)
            # í˜„ì¬ ë¡œê·¸ì— 'Industry' ì»¬ëŸ¼ì´ ì—†ë‹¤ê³  ë‚˜ì™”ì§€ë§Œ, ë¯¸ë˜ì— ì¶”ê°€ë  ê°€ëŠ¥ì„±ì„ ê³ ë ¤í•˜ì—¬ ë¡œì§ì€ ìœ ì§€í•˜ë˜,
            # ì‹¤ì œ ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ê±´ë„ˆë›°ë„ë¡ ì¡°ê±´ë¬¸ ì¶”ê°€
            found_by_industry = False
            if 'Industry' in krx.columns:
                INDUSTRY_KEYWORD_MAP = {
                    "ì œì•½": ["ì˜ì•½í’ˆ ì œì¡°ì—…", "ì˜ë£Œìš© ë¬¼ì§ˆ ë° ì˜ì•½í’ˆ ì œì¡°ì—…", "ìƒë¬¼í•™ì  ì œì œ ì œì¡°ì—…"], 
                    "ë°˜ë„ì²´": ["ë°˜ë„ì²´ ì œì¡°ì—…", "ì „ìë¶€í’ˆ ì œì¡°ì—…", "ë°˜ë„ì²´ ë° í‰íŒë””ìŠ¤í”Œë ˆì´ ì œì¡°ì—…"],
                    "ìë™ì°¨": ["ìë™ì°¨ìš© ì—”ì§„ ë° ìë™ì°¨ ì œì¡°ì—…", "ìë™ì°¨ ë¶€í’ˆ ì œì¡°ì—…"],
                    "IT": ["ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œ ë° ê³µê¸‰ì—…", "ì»´í“¨í„° í”„ë¡œê·¸ë˜ë°, ì‹œìŠ¤í…œ í†µí•© ë° ê´€ë¦¬ì—…", "ì •ë³´ì„œë¹„ìŠ¤ì—…"],
                    "ì€í–‰": ["ì€í–‰"],
                    "ì¦ê¶Œ": ["ì¦ê¶Œ ë° ì„ ë¬¼ ì¤‘ê°œì—…"],
                    "ë³´í—˜": ["ë³´í—˜ ë° ì—°ê¸ˆì—…"],
                    "ê±´ì„¤": ["ì¢…í•© ê±´ì„¤ì—…", "ê±´ë¬¼ ê±´ì„¤ì—…", "í† ëª© ê±´ì„¤ì—…"],
                    "í™”í•™": ["í™”í•™ë¬¼ì§ˆ ë° í™”í•™ì œí’ˆ ì œì¡°ì—…", "ê³ ë¬´ ë° í”Œë¼ìŠ¤í‹±ì œí’ˆ ì œì¡°ì—…"],
                    "ì½˜í…ì¸ ": ["ì˜í™”, ë¹„ë””ì˜¤ë¬¼, ë°©ì†¡í”„ë¡œê·¸ë¨ ì œì‘ ë° ë°°ê¸‰ì—…", "ìŒì•… ë° ê¸°íƒ€ ì—”í„°í…Œì¸ë¨¼íŠ¸ì—…", "ì¶œíŒì—…"], 
                    "ê²Œì„": ["ê²Œì„ ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œ ë° ê³µê¸‰ì—…", "ë°ì´í„°ë² ì´ìŠ¤ ë° ì˜¨ë¼ì¸ ì •ë³´ ì œê³µì—…"],
                    "ì² ê°•": ["1ì°¨ ì² ê°• ì œì¡°ì—…", "ê¸ˆì† ê°€ê³µì œí’ˆ ì œì¡°ì—…"],
                    "ì¡°ì„ ": ["ì„ ë°• ë° ë³´íŠ¸ ê±´ì¡°ì—…"],
                    "í•´ìš´": ["í•´ìƒ ìš´ì†¡ì—…"],
                    "í•­ê³µ": ["í•­ê³µ ìš´ì†¡ì—…"],
                    "ë°©ì‚°": ["í•­ê³µê¸°, ìš°ì£¼ì„  ë° ë³´ì¡°ì¥ë¹„ ì œì¡°ì—…"],
                    "ìŒì‹ë£Œ": ["ì‹ë£Œí’ˆ ì œì¡°ì—…", "ìŒë£Œ ì œì¡°ì—…", "ë‹´ë°° ì œì¡°ì—…"],
                    "ìœ í†µ": ["ì¢…í•© ì†Œë§¤ì—…", "ì „ë¬¸ ì†Œë§¤ì—…", "ë¬´ì í¬ ì†Œë§¤ì—…"],
                    # ... FinanceDataReaderì˜ 'Industry' ê³ ìœ ê°’ì„ ì°¸ê³ í•˜ì—¬ ì¶”ê°€/ìˆ˜ì •
                }
                for industry_key, industry_names in INDUSTRY_KEYWORD_MAP.items():
                    if lower_keyword == industry_key.lower() or any(name.lower() in lower_keyword for name in industry_names):
                        print(f"ë””ë²„ê·¸: ì—…ì¢… '{industry_key}'ì— í•´ë‹¹í•˜ëŠ” ì¢…ëª©ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤.")
                        target_stocks = krx[krx['Industry'].isin(industry_names)]
                        analysis_subject = f"'{industry_key}' ì—…ì¢…"
                        found_by_industry = True
                        break
            
            if not found_by_industry:
                # 3. Fallback to name-based search (ê°€ì¥ ë§ˆì§€ë§‰ ìˆœìœ„)
                print(f"ë””ë²„ê·¸: ì¢…ëª©ëª…ì— '{keyword}' í‚¤ì›Œë“œê°€ í¬í•¨ëœ ì¢…ëª©ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤. (ìµœì¢… í´ë°±)")
                target_stocks = krx[krx['Name'].str.contains(keyword, na=False)]
    
    elif target_str in GENERIC_TARGETS:
        analysis_subject = "ì‹œì¥ ì „ì²´"
    
    print(f"--- ë””ë²„ê·¸ ì¢…ë£Œ (get_target_stocks) ---")
    return target_stocks, analysis_subject


def parse_period(period_str):
    """'ì§€ë‚œ 3ë…„ê°„' ê°™ì€ ë¬¸ìì—´ì„ ì‹œì‘ì¼ê³¼ ì¢…ë£Œì¼ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜ (ê¸°ëŠ¥ í™•ì¥)"""
    today = datetime.now()
    if not period_str:
        return today - timedelta(days=365), today # ê¸°ë³¸ê°’: ìµœê·¼ 1ë…„

    try:
        # --- ë‹¨ê¸° ê¸°ê°„ ì²˜ë¦¬ ---
        if "ì˜¤ëŠ˜" in period_str:
            return today.replace(hour=0, minute=0, second=0, microsecond=0), today
        if "ì–´ì œ" in period_str:
            yesterday = today - timedelta(days=1)
            return yesterday.replace(hour=0, minute=0, second=0, microsecond=0), yesterday.replace(hour=23, minute=59, second=59)
        if "ì´ë²ˆì£¼" in period_str:
            start_of_week = today - timedelta(days=today.weekday()) # ì´ë²ˆ ì£¼ ì›”ìš”ì¼
            return start_of_week, today
        if "ì§€ë‚œ ë‹¬" in period_str or "ì§€ë‚œë‹¬" in period_str:
            first_day_of_current_month = today.replace(day=1)
            last_day_of_last_month = first_day_of_current_month - timedelta(days=1)
            first_day_of_last_month = last_day_of_last_month.replace(day=1)
            return first_day_of_last_month, last_day_of_last_month

        # --- ë¶„ê¸° ì²˜ë¦¬ (ì˜ˆ: "ì˜¬í•´ 1ë¶„ê¸°") ---
        if "ë¶„ê¸°" in period_str:
            quarter_match = re.search(r'(\d)ë¶„ê¸°', period_str)
            if quarter_match:
                quarter = int(quarter_match.group(1))
                year = today.year
                if "ì‘ë…„" in period_str:
                    year -= 1
                
                start_month = 3 * quarter - 2
                end_month = 3 * quarter
                start_date = datetime(year, start_month, 1)
                # ë‹¤ìŒ ë‹¬ì˜ ì²«ë‚ ì—ì„œ í•˜ë£¨ë¥¼ ë¹¼ì„œ ë§ˆì§€ë§‰ ë‚ ì„ êµ¬í•¨
                if end_month == 12:
                    end_date = datetime(year, 12, 31)
                else:
                    end_date = datetime(year, end_month + 1, 1) - timedelta(days=1)
                return start_date, end_date

        # --- ê¸°ì¡´ ë¡œì§ (ì¼/ê°œì›”/ë…„) ---
        if "ì¼" in period_str:
            days_match = re.search(r'(\d+)', period_str)
            if days_match:
                days = int(days_match.group(0))
                return today - timedelta(days=days), today
        if "ê°œì›”" in period_str:
            months_match = re.search(r'(\d+)', period_str)
            if months_match:
                months = int(months_match.group(0))
                return today - timedelta(days=30 * months), today
        if "ë…„ê°„" in period_str or "ë…„" in period_str:
            years_match = re.search(r'(\d+)', period_str)
            if years_match:
                years = int(years_match.group(0))
                return today - timedelta(days=365 * years), today

    except (ValueError, TypeError, AttributeError):
        pass 

    return today - timedelta(days=365), today

def get_interest_rate_hike_dates(api_key):
    """í•œêµ­ì€í–‰ APIë¡œ ê¸°ì¤€ê¸ˆë¦¬ ì¸ìƒì¼ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜."""
    stats_code, item_code = "722Y001", "0001000"
    end_date = datetime.now().strftime('%Y%m%d')
    start_date = (datetime.now() - timedelta(days=5*365)).strftime('%Y%m%d')
    url = f"https://ecos.bok.or.kr/api/StatisticSearch/{api_key}/json/kr/1/1000/{stats_code}/DD/{start_date}/{end_date}/{item_code}"
    
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        data = response.json()
        
        if "StatisticSearch" not in data or "row" not in data.get("StatisticSearch", {}):
            return []

        df = pd.DataFrame(data["StatisticSearch"]["row"])
        df['TIME'] = pd.to_datetime(df['TIME'], format='%Y%m%d')
        df['DATA_VALUE'] = pd.to_numeric(df['DATA_VALUE'])
        df = df.sort_values(by='TIME').reset_index(drop=True)
        df['PREV_VALUE'] = df['DATA_VALUE'].shift(1)
        
        hike_dates = df[df['DATA_VALUE'] > df['PREV_VALUE']]['TIME'].tolist()
        return hike_dates
    except Exception as e:
        print(f"í•œêµ­ì€í–‰ API ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        return []
    

def analyze_target_price_upside(target_stocks):
    """
    [ìµœì í™”] ë„¤ì´ë²„ ì¦ê¶Œ ì»¨ì„¼ì„œìŠ¤ í˜ì´ì§€ë¥¼ ì¼ê´„ ìŠ¤í¬ë ˆì´í•‘í•˜ì—¬ ëª©í‘œì£¼ê°€ ê´´ë¦¬ìœ¨ì„ ë¶„ì„í•©ë‹ˆë‹¤.
    """
    print("ëª©í‘œì£¼ê°€ ì»¨ì„¼ì„œìŠ¤ ë°ì´í„° ì¼ê´„ ì¡°íšŒ ì‹œì‘...")
    try:
        url = "https://finance.naver.com/sise/consensus.naver?&target=up"
        headers = {'User-Agent': 'Mozilla/5.0'}
        
        df_list = pd.read_html(requests.get(url, headers=headers, timeout=10).text)
        df = df_list[1]
        
        df = df.dropna(axis='index', how='all')
        df.columns = ['ì¢…ëª©ëª…', 'ëª©í‘œì£¼ê°€', 'íˆ¬ìì˜ê²¬', 'í˜„ì¬ê°€', 'ê´´ë¦¬ìœ¨', 'ì¦ê¶Œì‚¬', 'ì‘ì„±ì¼']
        df = df[df['ì¢…ëª©ëª…'].notna()]

        df['ëª©í‘œì£¼ê°€'] = pd.to_numeric(df['ëª©í‘œì£¼ê°€'], errors='coerce')
        df['í˜„ì¬ê°€'] = pd.to_numeric(df['í˜„ì¬ê°€'], errors='coerce')
        df['ê´´ë¦¬ìœ¨'] = df['ê´´ë¦¬ìœ¨'].str.strip('%').astype(float)
        df = df.dropna(subset=['ëª©í‘œì£¼ê°€', 'í˜„ì¬ê°€', 'ê´´ë¦¬ìœ¨'])

        if GLOBAL_KRX_LISTING is None:
            initialize_global_data()
        krx_list = GLOBAL_KRX_LISTING[['Name', 'Code']]
        df = pd.merge(df, krx_list, left_on='ì¢…ëª©ëª…', right_on='Name', how='inner')
        
        print("ë°ì´í„° ì¡°íšŒ ë° ê°€ê³µ ì™„ë£Œ.")
        
        analysis_results = []
        for index, row in df.iterrows():
            analysis_results.append({
                "code": row['Code'],
                "name": row['ì¢…ëª©ëª…'],
                "value": row['ê´´ë¦¬ìœ¨'],
                "label": "ëª©í‘œì£¼ê°€ ê´´ë¦¬ìœ¨(%)",
                "start_price": int(row['í˜„ì¬ê°€']),
                "end_price": int(row['ëª©í‘œì£¼ê°€'])
            })
            
        return analysis_results

    except Exception as e:
        print(f"ëª©í‘œì£¼ê°€ ì»¨ì„¼ì„œìŠ¤ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return []
def execute_single_stock_price(intent_json):
    """
    [íš¨ìœ¨ì„± ê°œì„  ë²„ì „]
    ë‹¨ì¼ ì¢…ëª©ì˜ í˜„ì¬ê°€ë¥¼ pykrxì˜ get_market_ohlcv_by_dateë¥¼ ì‚¬ìš©í•˜ì—¬ ë¹ ë¥´ê²Œ ì¡°íšŒí•©ë‹ˆë‹¤.
    """
    try:
        if GLOBAL_NAME_TICKER_MAP is None:
            initialize_global_data()

        target_name = intent_json.get("target")
        if not target_name:
            return {"error": "ì¢…ëª©ëª…ì´ ì§€ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}

        ticker = GLOBAL_NAME_TICKER_MAP.get(target_name)
        if not ticker:
            return {
                "analysis_subject": "ì˜¤ë¥˜",
                "result": [f"'{target_name}'ì— í•´ë‹¹í•˜ëŠ” ì¢…ëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì¢…ëª©ëª…ì„ í™•ì¸í•´ì£¼ì„¸ìš”."]
            }

        # ê°€ì¥ ê°€ê¹Œìš´ ì˜ì—…ì¼ ì°¾ê¸°
        latest_bday = stock.get_nearest_business_day_in_a_week()

        # íŠ¹ì • í‹°ì»¤(ì¢…ëª©ì½”ë“œ)ì˜ í•˜ë£¨ì¹˜ ë°ì´í„°ë§Œ íš¨ìœ¨ì ìœ¼ë¡œ ì¡°íšŒí•©ë‹ˆë‹¤.
        df = stock.get_market_ohlcv_by_date(fromdate=latest_bday, todate=latest_bday, ticker=ticker)

        if df.empty:
            return {
                "analysis_subject": "ì •ë³´ ì—†ìŒ",
                "result": [f"'{target_name}'ì˜ {latest_bday} ê±°ë˜ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."]
            }

        # ë°ì´í„°í”„ë ˆì„ì˜ ì²« ë²ˆì§¸ í–‰ì—ì„œ ì •ë³´ ì¶”ì¶œ
        stock_info = df.iloc[0]
        current_price = stock_info['ì¢…ê°€']
        change = stock_info['ì¢…ê°€'] - stock_info['ì‹œê°€']
        
        change_str = f"{abs(change):,}ì› ìƒìŠ¹" if change > 0 else f"{abs(change):,}ì› í•˜ë½" if change < 0 else "ë³€ë™ ì—†ìŒ"
        
        date_str = f"{latest_bday[:4]}-{latest_bday[4:6]}-{latest_bday[6:8]}"
        result_sentence = (
            f"**{target_name}**({ticker})ì˜ ê°€ì¥ ìµœê·¼ ì¢…ê°€({date_str})ëŠ”"
            f" **{current_price:,}ì›**ì´ë©°, ì‹œê°€ ëŒ€ë¹„ {change_str}í–ˆìŠµë‹ˆë‹¤."
        )

        return {
            "query_intent": intent_json,
            "analysis_subject": f"{target_name} í˜„ì¬ê°€",
            "result": [result_sentence]
        }
    except Exception as e:
        traceback.print_exc()
        return {"error": f"ë‹¨ì¼ ì¢…ëª© ê°€ê²© ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"}
    
def execute_stock_analysis(intent_json, page, user_query, cache_key=None):
    """
    [ìˆ˜ì •] 'ìˆœë§¤ìˆ˜' ë¶„ì„ê³¼ ê¸°ì¡´ 'ìˆ˜ìµë¥ /ë³€ë™ì„±' ë¶„ì„ì„ ë¶„ê¸° ì²˜ë¦¬í•˜ëŠ” ìµœì¢… í•¨ìˆ˜.
    """
    try:
        action_str = intent_json.get("action", "")

        # ìºì‹œ ì²˜ë¦¬ ë¡œì§ (ìƒëµ, ê¸°ì¡´ê³¼ ë™ì¼)
        if cache_key and cache_key in ANALYSIS_CACHE and 'full_result' in ANALYSIS_CACHE[cache_key]:
            sorted_result = ANALYSIS_CACHE[cache_key]['full_result']
            analysis_subject = ANALYSIS_CACHE[cache_key]['analysis_subject']
            print(f"âœ… CACHE HIT: ìºì‹œëœ ì „ì²´ ê²°ê³¼ {len(sorted_result)}ê°œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        else:
            # ìºì‹œê°€ ì—†ëŠ” ê²½ìš° ìƒˆë¡œìš´ ë¶„ì„ ì‹œì‘
            print(f"ğŸ”¥ CACHE MISS: ìƒˆë¡œìš´ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
            target_str = intent_json.get("target")
            condition_obj = intent_json.get("condition")
            target_stocks, analysis_subject = get_target_stocks(target_str)
            if target_stocks.empty: return {"result": [f"{analysis_subject}ì— í•´ë‹¹í•˜ëŠ” ì¢…ëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."]}

            start_date, end_date = parse_period(intent_json.get("period"))
            
            result_data = []

            # --- â–¼â–¼â–¼ [í•µì‹¬] action_strì— ë”°ë¼ ë‹¤ë¥¸ ë¶„ì„ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ëŠ” ë¶€ë¶„ â–¼â–¼â–¼ ---
            if "ìˆœë§¤ìˆ˜" in action_str and isinstance(condition_obj, dict) and condition_obj.get('who') == 'ê¸°ê´€':
                # "ê¸°ê´€ ìˆœë§¤ìˆ˜" ìš”ì²­ì¼ ê²½ìš°, ìƒˆë¡œ ë§Œë“  í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.
                result_data = analyze_institutional_buying(start_date.strftime('%Y%m%d'), end_date.strftime('%Y%m%d'))
                reverse_sort = True 
            else:
                # ê·¸ ì™¸ì˜ ëª¨ë“  ìš”ì²­ì€ ê¸°ì¡´ì˜ ìˆ˜ìµë¥ /ë³€ë™ì„± ë¶„ì„ ë¡œì§ì„ ë”°ë¦…ë‹ˆë‹¤.
                event_periods = []
                if isinstance(condition_obj, str) and any(s in condition_obj for s in ["ì—¬ë¦„", "ê²¨ìš¸"]):
                    season = "ì—¬ë¦„" if "ì—¬ë¦„" in condition_obj else "ê²¨ìš¸"
                    event_periods = handle_season_condition((start_date, end_date), season)
                elif isinstance(condition_obj, dict) and condition_obj.get("type") == "indicator":
                    event_periods = handle_indicator_condition(condition_obj, (start_date, end_date))
                else:
                    event_periods = [(start_date, end_date)]
                
                if "ì˜¤ë¥¸" in action_str or "ë‚´ë¦°" in action_str:
                    result_data = analyze_top_performers(target_stocks, event_periods, (start_date, end_date))
                elif "ë³€ë™ì„±" in action_str or "ë³€ë™" in action_str:
                    result_data = analyze_volatility(target_stocks, (start_date, end_date))
                elif "ëª©í‘œì£¼ê°€" in action_str:
                    result_data = analyze_target_price_upside(target_stocks)
                
                reverse_sort = False if "ë‚´ë¦°" in action_str else True
            # --- â–²â–²â–² ë¶„ê¸° ì²˜ë¦¬ ì¢…ë£Œ â–²â–²â–² ---

            sorted_result = sorted(result_data, key=lambda x: x.get('value', -99999), reverse=reverse_sort)
            
            if not cache_key: cache_key = str(hash(json.dumps(intent_json, sort_keys=True)))
            ANALYSIS_CACHE[cache_key] = {
                'intent_json': intent_json, 'analysis_subject': analysis_subject, 'full_result': sorted_result
            }
            print(f"ìƒˆë¡œìš´ ë¶„ì„ ê²°ê³¼ {len(sorted_result)}ê°œë¥¼ ìºì‹œì— ì €ì¥í–ˆìŠµë‹ˆë‹¤. (í‚¤: {cache_key})")


        # í˜ì´ì§€ë„¤ì´ì…˜ ë¡œì§ (ìƒëµ, ê¸°ì¡´ê³¼ ë™ì¼)
        items_per_page = 20
        total_items = len(sorted_result)
        total_pages = (total_items + items_per_page - 1) // items_per_page
        start_index = (page - 1) * items_per_page
        end_index = start_index + items_per_page
        paginated_result = sorted_result[start_index:end_index]
        
        # ì„¤ëª…(description) ìƒì„± ë¡œì§ (ìƒëµ, ê¸°ì¡´ê³¼ ë™ì¼)
        condition_str = intent_json.get("condition")
        description = ""
        if isinstance(condition_str, str):
            if "ì—¬ë¦„" in condition_str:
                description = "ì—¬ë¦„(6ì›”1ì¼~8ì›” 31ì¼) ê¸°ê°„ì˜ í‰ê·  ìˆ˜ìµë¥ ì„ ë¶„ì„í•œ ê²°ê³¼ì…ë‹ˆë‹¤. \n í˜„ì¬ ë‚˜ì˜¤ëŠ” ê³¼ê±°ê°€ê²©ê³¼ í˜„ì¬ê°€ê²©ì˜ ìˆ˜ìµë¥ ì´ ì•„ë‹™ë‹ˆë‹¤."
            elif "ê²¨ìš¸" in condition_str:
                description = "ê²¨ìš¸(12ì›”1ì¼~3ì›”1) ê¸°ê°„ì˜ í‰ê·  ìˆ˜ìµë¥ ì„ ë¶„ì„í•œ ê²°ê³¼ì…ë‹ˆë‹¤. \n í˜„ì¬ ë‚˜ì˜¤ëŠ” ê³¼ê±°ê°€ê²©ê³¼ í˜„ì¬ê°€ê²©ì˜ ìˆ˜ìµë¥ ì´ ì•„ë‹™ë‹ˆë‹¤."
        elif isinstance(condition_str, dict) and condition_str.get("type") == "indicator":
            description = f"{condition_str.get('name')} ì§€í‘œê°€ {condition_str.get('value')}{condition_str.get('operator')} ì¡°ê±´ ê¸°ê°„ì˜ í‰ê·  ìˆ˜ìµë¥ ì„ ë¶„ì„í•œ ê²°ê³¼ì…ë‹ˆë‹¤."

        return {
            "query_intent": intent_json,
            "analysis_subject": analysis_subject,
            "description": description,
            "result": paginated_result,
            "pagination": { "current_page": page, "total_pages": total_pages, "total_items": total_items },
            "cache_key": cache_key
        }
    except Exception as e:
        traceback.print_exc()
        return {"error": f"ë¶„ì„ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"}
    

def handle_season_condition(period_tuple, season):
    """'ì—¬ë¦„' ë˜ëŠ” 'ê²¨ìš¸' ì¡°ê±´ì— ë§ëŠ” ë‚ ì§œ êµ¬ê°„ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜ (ìµœì í™”)"""
    start_date, end_date = period_tuple
    event_periods = []
    
    for year in range(start_date.year, end_date.year + 1):
        if season == "ì—¬ë¦„":
            season_start = datetime(year, 6, 1)
            season_end = datetime(year, 8, 31)
            overlap_start = max(start_date, season_start)
            overlap_end = min(end_date, season_end)
            if overlap_start < overlap_end:
                event_periods.append((overlap_start, overlap_end))

        elif season == "ê²¨ìš¸":
            # Winter can span across two years (Dec of year Y-1 to Feb/Mar of year Y)
            # So we check for both the current year's winter and the previous year's winter that ends in the current year.
            for y in [year - 1, year]: 
                season_start = datetime(y, 12, 1)
                season_end = datetime(y + 1, 3, 1) - timedelta(days=1) # End of Feb or March 1st minus 1 day
                overlap_start = max(start_date, season_start)
                overlap_end = min(end_date, season_end)
                if overlap_start < overlap_end:
                    event_periods.append((overlap_start, overlap_end))

    # Remove duplicates and sort the periods
    return sorted(list(set(event_periods)))

def handle_interest_rate_condition(api_key, period_tuple):
    """ê¸ˆë¦¬ ì¸ìƒ ì¡°ê±´ì— ë§ëŠ” ë‚ ì§œ êµ¬ê°„ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜"""
    start_date, end_date = period_tuple
    hike_dates = get_interest_rate_hike_dates(api_key)
    
    event_periods = []
    for hike_date in hike_dates:
        event_start = hike_date
        event_end = event_start + timedelta(days=7) # Consider a 7-day window after hike
        
        # Ensure the event period is within the overall query period
        if event_start >= start_date and event_end <= end_date:
            event_periods.append((event_start, event_end))
            
    return event_periods

def _fetch_and_analyze_single_stock(stock_code, stock_name, overall_start, overall_end, event_periods):
    """
    ë‹¨ì¼ ì¢…ëª©ì˜ ì „ì²´ ê¸°ê°„ ë°ì´í„°ë¥¼ ì¡°íšŒí•˜ê³ , ê·¸ ì•ˆì—ì„œ ì´ë²¤íŠ¸ ê¸°ê°„ ìˆ˜ìµë¥ ì„ ë¶„ì„í•˜ëŠ” í—¬í¼ í•¨ìˆ˜ (ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•¨)
    """
    try:
        print(f"       ë°ì´í„° ì¡°íšŒ ì‹œì‘: {stock_name}({stock_code})")
        overall_prices = fdr.DataReader(stock_code, overall_start, overall_end)

        if overall_prices.empty:
            print(f"       -> [ë¶„ì„ ì‹¤íŒ¨] {stock_name}({stock_code}): fdr.DataReaderê°€ ë¹ˆ ë°ì´í„°ë¥¼ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤.")
            return None

        print(f"       -> [ë°ì´í„° í™•ì¸] {stock_name}({stock_code}): ì „ì²´ ê¸°ê°„({overall_start.strftime('%Y-%m-%d')}~{overall_end.strftime('%Y-%m-%d')}) ë°ì´í„° {len(overall_prices)}ê°œ ë¡œë“œ ì„±ê³µ.")
        print(f"       -> [ì´ë²¤íŠ¸ ê¸°ê°„ í™•ì¸] ë¶„ì„í•  ì´ë²¤íŠ¸ ê¸°ê°„ ìˆ˜: {len(event_periods)}ê°œ, ì²« ê¸°ê°„: {event_periods[0] if event_periods else 'N/A'}")

        start_price = int(overall_prices['Open'].iloc[0])
        end_price = int(overall_prices['Close'].iloc[-1])

        period_returns = []
        for i, (start, end) in enumerate(event_periods):
            start_ts = pd.to_datetime(start)
            end_ts = pd.to_datetime(end)

            prices_in_period = overall_prices.loc[start_ts:end_ts]
            print(f"       -> ì´ë²¤íŠ¸ ê¸°ê°„ {i+1} ({start_ts.date()}~{end_ts.date()}) ë°ì´í„° ìŠ¬ë¼ì´ì‹± ê²°ê³¼: {len(prices_in_period)}ê°œ")

            if len(prices_in_period) > 1:
                event_start_price = prices_in_period['Open'].iloc[0]
                event_end_price = prices_in_period['Close'].iloc[-1]
                if event_start_price > 0:
                    period_returns.append((event_end_price / event_start_price) - 1)

        if not period_returns:
            print(f"       -> [ë¶„ì„ ì‹¤íŒ¨] {stock_name}({stock_code}): ìœ íš¨í•œ ìˆ˜ìµë¥ ì„ ê³„ì‚°í•  ìˆ˜ ìˆëŠ” ì´ë²¤íŠ¸ ê¸°ê°„ì´ ì—†ìŠµë‹ˆë‹¤.")
            return None

        average_return = statistics.mean(period_returns)
        if pd.notna(average_return):
            return {
                "code": stock_code, "name": stock_name,
                "value": round(average_return * 100, 2), "label": "í‰ê·  ìˆ˜ìµë¥ (%)",
                "start_price": start_price,
                "end_price": end_price,
            }
    except Exception as e:
        print(f"       -> [ë¶„ì„ ì‹¤íŒ¨] {stock_name}({stock_code}) ë¶„ì„ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")

    return None

def analyze_top_performers(target_stocks, event_periods, overall_period):
    """
    [ì„±ëŠ¥ ìµœì í™”] ì „ì²´ ê¸°ê°„ ë°ì´í„°ë¥¼ í•œ ë²ˆì— ì¡°íšŒ í›„, ë©”ëª¨ë¦¬ì—ì„œ ì¡°ê±´ ê¸°ê°„ì„ ìŠ¬ë¼ì´ì‹±í•˜ì—¬ ë¶„ì„ ì†ë„ë¥¼ ê°œì„ í•©ë‹ˆë‹¤.
    ë˜í•œ, ì—¬ëŸ¬ ì¢…ëª©ì˜ ë°ì´í„° ì¡°íšŒë¥¼ ë³‘ë ¬ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    """
    analysis_results = []
    
    try:
        print("DEBUG: nlargest ì‹¤í–‰ ì „, target_stocks ì •ë³´:")
        target_stocks.info() 
        
        top_stocks = target_stocks.nlargest(min(len(target_stocks), 50), 'Marcap').reset_index(drop=True)

    except Exception as e:

        print(target_stocks)
        return [] 

    print(f"ì‹œê°€ì´ì•¡ ìƒìœ„ {len(top_stocks)}ê°œ ì¢…ëª©ì— ëŒ€í•œ ìˆ˜ìµë¥  ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    overall_start, overall_end = overall_period

    with concurrent.futures.ThreadPoolExecutor(max_workers=30) as executor:
        future_to_stock = {
            executor.submit(_fetch_and_analyze_single_stock, stock['Code'], stock['Name'], overall_start, overall_end, event_periods): stock
            for index, stock in top_stocks.iterrows()
        }

        for i, future in enumerate(concurrent.futures.as_completed(future_to_stock)):
            stock_info = future_to_stock[future]
            stock_code, stock_name = stock_info['Code'], stock_info['Name']
            try:
                result = future.result()
                if result:
                    analysis_results.append(result)
                print(f"   ({i + 1}/{len(top_stocks)}) {stock_name}({stock_code}) ë¶„ì„ ì™„ë£Œ.")
            except Exception as exc:
                print(f"   - {stock_name}({stock_code}) ë¶„ì„ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {exc}")
    
    return analysis_results

def analyze_volatility(target_stocks, period_tuple):
    """ë³€ë™ì„± ë¶„ì„ í•¨ìˆ˜ (ë”œë ˆì´ ì œê±°)"""
    analysis_results = []
    start_date, end_date = period_tuple
    top_stocks = target_stocks.nlargest(min(len(target_stocks), 50), 'Marcap').reset_index(drop=True)
    print(f"ì‹œê°€ì´ì•¡ ìƒìœ„ {len(top_stocks)}ê°œ ì¢…ëª©ì— ëŒ€í•œ ë³€ë™ì„± ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...")

    # ThreadPoolExecutorë¥¼ ì‚¬ìš©í•˜ì—¬ ë³€ë™ì„± ë¶„ì„ì„ ìœ„í•œ ë°ì´í„° ì¡°íšŒë¥¼ ë³‘ë ¬ë¡œ ì²˜ë¦¬
    with concurrent.futures.ThreadPoolExecutor(max_workers=30) as executor: # 20ì—ì„œ 30ìœ¼ë¡œ ì¦ê°€
        future_to_stock = {
            executor.submit(
                lambda code, name, s_date, e_date: _fetch_and_calculate_volatility(code, name, s_date, e_date),
                stock_info['Code'], stock_info['Name'], start_date, end_date
            ): stock_info
            for index, stock_info in top_stocks.iterrows()
        }

        for i, future in enumerate(concurrent.futures.as_completed(future_to_stock)):
            stock_info = future_to_stock[future]
            code, name = stock_info['Code'], stock_info['Name']
            try:
                result = future.result()
                if result:
                    analysis_results.append(result)
                print(f" Â  ({i + 1}/{len(top_stocks)}) {name}({code}) ë³€ë™ì„± ë¶„ì„ ì™„ë£Œ.")
            except Exception as exc:
                print(f" Â  - {name}({code}) ë³€ë™ì„± ë¶„ì„ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {exc}")
    
    return analysis_results

def _fetch_and_calculate_volatility(code, name, start_date, end_date):
    """
    ë‹¨ì¼ ì¢…ëª©ì˜ ë°ì´í„°ë¥¼ ì¡°íšŒí•˜ê³  ë³€ë™ì„±ì„ ê³„ì‚°í•˜ëŠ” í—¬í¼ í•¨ìˆ˜ (ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•¨)
    """
    try:
        print(f" Â  Â  Â ë³€ë™ì„± ë°ì´í„° ì¡°íšŒ ì‹œì‘: {name}({code}) - {start_date.strftime('%Y-%m-%d')} ~ {end_date.strftime('%Y-%m-%d')}")
        overall_prices = fdr.DataReader(code, start_date, end_date)
        print(f" Â  Â  Â ë³€ë™ì„± ë°ì´í„° ì¡°íšŒ ì™„ë£Œ: {name}({code})")

        if overall_prices.empty:
            return None
        
        daily_returns = overall_prices['Close'].pct_change().dropna()
        volatility = daily_returns.std()
        if pd.notna(volatility):
            return {
                "code": code, "name": name,
                "value": round(volatility * 100, 2), "label": "ë³€ë™ì„±(%)",
                "start_price": int(overall_prices['Open'].iloc[0]),
                "end_price": int(overall_prices['Close'].iloc[-1])
            }
    except Exception as e:
        print(f" Â  - {name}({code}) ë³€ë™ì„± ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    return None

def handle_indicator_condition(condition_obj, period_tuple):
    """CPI, ê¸ˆë¦¬ ë“± ì§€í‘œ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ë‚ ì§œ êµ¬ê°„ì„ ë°˜í™˜"""
    bok_api_key = os.getenv("ECOS_API_KEY")
    if not bok_api_key: return []
    INDICATOR_MAP = {
        "CPI": {"stats_code": "901Y001", "item_code": "0"},
        "ê¸°ì¤€ê¸ˆë¦¬": {"stats_code": "722Y001", "item_code": "0001000"},
    }

    indicator_name = condition_obj.get("name")
    if indicator_name not in INDICATOR_MAP: return []

    indicator_info = INDICATOR_MAP[indicator_name]
    data_series = get_bok_data(bok_api_key, indicator_info['stats_code'], indicator_info['item_code'], period_tuple[0], period_tuple[1])

    if data_series is None: return []

    op_str = condition_obj.get("operator")
    value = condition_obj.get("value")

    if op_str == '>': matching_series = data_series[data_series > value]
    elif op_str == '>=': matching_series = data_series[data_series >= value]
    else: return []

    return [(d.replace(day=1), (d.replace(day=1) + timedelta(days=32)).replace(day=1) - timedelta(days=1)) for d in matching_series.index]

def get_bok_data(bok_api_key, stats_code, item_code, start_date, end_date):
    """
    í•œêµ­ì€í–‰ ECOS APIë¥¼ í†µí•´ íŠ¹ì • ì§€í‘œì˜ ì‹œê³„ì—´ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ Pandas Seriesë¡œ ë°˜í™˜.
    """
    start_str = start_date.strftime('%Y%m')
    end_str = end_date.strftime('%Y%m')
    
    url = f"https://ecos.bok.or.kr/api/StatisticSearch/{bok_api_key}/json/kr/1/1000/{stats_code}/MM/{start_str}/{end_str}/{item_code}"
    
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()  # HTTP ì˜¤ë¥˜ ë°œìƒ ì‹œ ì˜ˆì™¸ ë°œìƒ
        data = response.json()

        if "StatisticSearch" not in data or "row" not in data.get("StatisticSearch", {}):
            print("BOK API ì‘ë‹µì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None

        rows = data["StatisticSearch"]["row"]
        if not rows:
            return None

        df = pd.DataFrame(rows)
        df['TIME'] = pd.to_datetime(df['TIME'], format='%Y%m') 
        df['DATA_VALUE'] = pd.to_numeric(df['DATA_VALUE']) 
        df = df.set_index('TIME')                             
        
        return df['DATA_VALUE'].sort_index()

    except requests.exceptions.RequestException as e:
        print(f"í•œêµ­ì€í–‰ API ìš”ì²­ ì˜¤ë¥˜: {e}")
        return None
    except Exception as e:
        print(f"ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        return None
    
@askfin_bp.route('/')
def askfin_page():
    return render_template('askfin.html')

@askfin_bp.route('/analyze', methods=['POST'])
def analyze_query():
    """
    [ìµœì¢… ê°œì„ ] AIê°€ ì¢…ëª©ëª…ì„ ì¸ì‹í–ˆì§€ë§Œ query_typeì„ ì˜ëª» íŒë‹¨í•œ ê²½ìš°,
    ë°±ì—”ë“œì—ì„œ ì¬ë¶„ë¥˜í•˜ì—¬ ì²˜ë¦¬í•˜ëŠ” ë¡œì§ì´ ì¶”ê°€ëœ ë²„ì „.
    """
    if not model:
        return jsonify({"error": "ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. API í‚¤ë¥¼ í™•ì¸í•˜ì„¸ìš”."}), 500
    
    data = request.get_json()
    user_query = data.get('query')
    page = data.get('page', 1)
    cache_key = data.get('cache_key')

    if not user_query:
        return jsonify({"error": "ì˜ëª»ëœ ìš”ì²­ì…ë‹ˆë‹¤."}), 400

    intent_json = None
    final_result = None

    if cache_key and cache_key in ANALYSIS_CACHE:
        print(f"âœ… CACHE HIT: ìºì‹œëœ ë¶„ì„ ê²°ê³¼ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. (í‚¤: {cache_key})")
        intent_json = ANALYSIS_CACHE[cache_key]['intent_json']
        if intent_json.get("query_type") == "stock_analysis":
             final_result = execute_stock_analysis(intent_json, page, user_query, cache_key)
             return jsonify(final_result)


    try:
        print(f"ğŸ”¥ CACHE MISS: '{user_query}'ì— ëŒ€í•´ Gemini API ë¶„ì„ì„ ìš”ì²­í•©ë‹ˆë‹¤.")
        prompt = PROMPT_TEMPLATE.format(user_query=user_query)
        response = model.generate_content(prompt)
        raw_text = response.text.strip()

        try:
            start = raw_text.find('{')
            end = raw_text.rfind('}') + 1
            cleaned_response = raw_text[start:end]
            intent_json = json.loads(cleaned_response)
            
            query_type = intent_json.get("query_type")
            
            if query_type == "stock_analysis":
                final_result = execute_stock_analysis(intent_json, page, user_query)
            elif query_type == "comparison_analysis":
                final_result = execute_comparison_analysis(intent_json)
            elif query_type == "indicator_lookup":
                final_result = execute_indicator_lookup(intent_json)
            elif query_type == "single_stock_price":
                final_result = execute_single_stock_price(intent_json)
            else:
                # --- â–¼â–¼â–¼ [í•µì‹¬] AIê°€ ì˜ëª» íŒë‹¨í–ˆì„ ë•Œë¥¼ ëŒ€ë¹„í•œ ë°©ì–´ ì½”ë“œ â–¼â–¼â–¼ ---
                # AIê°€ general_inquiryë¡œ íŒë‹¨í–ˆì§€ë§Œ, targetì´ ì‹¤ì œ ì£¼ì‹ ì¢…ëª©ëª…ì¸ì§€ í™•ì¸
                if query_type == "general_inquiry" and intent_json.get("target"):
                    target_name = intent_json.get("target")
                    if GLOBAL_NAME_TICKER_MAP is None: initialize_global_data()
                    
                    if target_name in GLOBAL_NAME_TICKER_MAP:
                        print(f"DEBUG: General inquiryë¥¼ single_stock_priceë¡œ ì¬ë¶„ë¥˜í•©ë‹ˆë‹¤. (Target: {target_name})")
                        # single_stock_price ìœ í˜•ìœ¼ë¡œ ê°•ì œ ë³€í™˜í•˜ì—¬ ì‹¤í–‰
                        new_intent = {"query_type": "single_stock_price", "target": target_name, "action": "í˜„ì¬ê°€ ì¡°íšŒ"}
                        final_result = execute_single_stock_price(new_intent)
                    else:
                        final_result = {"analysis_subject": "ì¼ë°˜ ë‹µë³€", "result": ["ì£„ì†¡í•©ë‹ˆë‹¤, í•´ë‹¹ ì§ˆë¬¸ì— ëŒ€í•´ì„œëŠ” ë‹µë³€ì„ ë“œë¦´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸ˆìœµ ê´€ë ¨ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”."]}
                else:
                    final_result = {"analysis_subject": "ì•Œë¦¼", "result": ["í•´ë‹¹ ìœ í˜•ì˜ ë¶„ì„ì€ ì•„ì§ ì§€ì›ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤."]}

            if final_result and (not final_result.get('result') or final_result.get("error")):
                 final_result = {
                    "analysis_subject": "ê²°ê³¼ ì—†ìŒ",
                    "result": [f"ìš”ì²­í•˜ì‹  '{user_query}'ì— ëŒ€í•œ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜ ë¶„ì„ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."]
                 }

        except (json.JSONDecodeError, IndexError):
            final_result = {
                "analysis_subject": "ì¼ë°˜ ë‹µë³€",
                "result": [raw_text.replace('\n', '<br>')]
            }
        
        return jsonify(final_result)

    except Exception as e:
        traceback.print_exc()
        return jsonify({"error": f"ë¶„ì„ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"}), 500
        
@askfin_bp.route('/new_chat', methods=['POST'])
def new_chat():
    """ëŒ€í™” ê¸°ë¡(ì„¸ì…˜)ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    session.pop('chat_history', None)
    return jsonify({"status": "success", "message": "ìƒˆ ëŒ€í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤."})