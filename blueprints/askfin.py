import dart_fss as dart
import time

import os
import json
import traceback
import requests
import google.generativeai as genai
import FinanceDataReader as fdr
import pandas as pd
import numpy as np
import statistics
from flask import Blueprint, render_template, request, jsonify, session
from dotenv import load_dotenv
from datetime import datetime, timedelta
import concurrent.futures

from pykrx import stock
import re
from bs4 import BeautifulSoup

from fuzzywuzzy import fuzz, process

# --- Global Caches for Initial Loading ---
GLOBAL_KRX_LISTING = None
GLOBAL_TICKER_NAME_MAP = None 
GLOBAL_NAME_TICKER_MAP = None
ANALYSIS_CACHE = {} 
GLOBAL_SECTOR_MASTER_DF = None 
GLOBAL_STOCK_SECTOR_MAP = None 
STOCK_DETAIL_CACHE = {} 
# --- Environment Variable Loading and API Key Setup ---
load_dotenv()

askfin_bp = Blueprint('askfin', __name__, url_prefix='/askfin')

try:
    DART_API_KEY = os.getenv("DART_API_KEY")
    if not DART_API_KEY:
        raise ValueError("DART API í‚¤ê°€ .env íŒŒì¼ì— ì—†ìŠµë‹ˆë‹¤.")
    dart.set_api_key(api_key=DART_API_KEY)
    print("DART API í‚¤ê°€ ì„±ê³µì ìœ¼ë¡œ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")
except Exception as e:
    print(f"[ê²½ê³ ] DART API í‚¤ ì„¤ì • ì‹¤íŒ¨: {e}")

try:
    API_KEY = os.getenv("GOOGLE_AI_API_KEY")
    if not API_KEY: raise ValueError("API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
    genai.configure(api_key=API_KEY)
    model = genai.GenerativeModel('gemini-1.5-flash')

    PROMPT_TEMPLATE = """
You are a financial analyst. Your primary task is to analyze a user's query and convert it into a structured JSON object.

- You MUST respond with a JSON object that follows the schema.
- **EXCEPTION**: If the user's query is a general question, a greeting, or something that CANNOT be structured into the JSON schema, you MUST respond with a conversational, friendly answer in plain text INSTEAD of JSON.
- For "comparison_analysis", the "target" MUST be an array of strings.
- Be specific with the "period" value. If the user says "ì´ë²ˆì£¼", use "ì´ë²ˆì£¼". If they say "ì§€ë‚œ 1ë¶„ê¸°", use "ì§€ë‚œ 1ë¶„ê¸°".

## JSON Schema:
{{"query_type": "string", "period": "string|null", "condition": "string|object|null", "target": "string|array|null", "action": "string|null"}}

## Examples (JSON Output):

# --- ê¸°ë³¸ ì˜ˆì‹œ ---
1.  User Query: "ì§€ë‚œ 3ë…„ ë™ì•ˆ ê²¨ìš¸ì— ì˜¤ë¥¸ ì½˜í…ì¸  ê´€ë ¨ ì£¼ì‹"
    JSON Output:
    ```json
    {{"query_type": "stock_analysis", "period": "ì§€ë‚œ 3ë…„", "condition": "ê²¨ìš¸", "target": "ì½˜í…ì¸  ê´€ë ¨ì£¼", "action": "ì˜¤ë¥¸ ì£¼ì‹"}}
    ```
2.  User Query: "ìµœê·¼ CPI ì§€ìˆ˜ ì•Œë ¤ì¤˜"
    JSON Output:
    ```json
    {{"query_type": "indicator_lookup", "period": "ìµœê·¼", "condition": null, "target": "CPI ì§€ìˆ˜", "action": "ì¡°íšŒ"}}
    ```
3.  User Query: "ì¸ê³µì§€ëŠ¥, 2ì°¨ì „ì§€ ì¤‘ ì§€ë‚œ 1ë…„ê°„ ê°€ì¥ ë§ì´ ì˜¤ë¥¸ í…Œë§ˆëŠ”?"
    JSON Output:
    ```json
    {{"query_type": "comparison_analysis", "period": "ì§€ë‚œ 1ë…„ê°„", "condition": null, "target": ["ì¸ê³µì§€ëŠ¥", "2ì°¨ì „ì§€"], "action": "ê°€ì¥ ë§ì´ ì˜¤ë¥¸ í…Œë§ˆ"}}
    ```

# --- êµ¬ì²´ì ì¸/ë‹¨ê¸° ê¸°ê°„ ì˜ˆì‹œ ---
4.  User Query: "ì´ë²ˆì£¼ ê°€ì¥ ë§ì´ ì˜¤ë¥¸ ì£¼ì‹ì€ ë­ì•¼?"
    JSON Output:
    ```json
    {{"query_type": "stock_analysis", "period": "ì´ë²ˆì£¼", "condition": null, "target": "ì£¼ì‹", "action": "ê°€ì¥ ë§ì´ ì˜¤ë¥¸ ì£¼ì‹"}}
    ```
5.  User Query: "ì˜¤ëŠ˜ ì œì¼ ë§ì´ ë‚´ë¦° ë°˜ë„ì²´ì£¼ëŠ”?"
    JSON Output:
    ```json
    {{"query_type": "stock_analysis", "period": "ì˜¤ëŠ˜", "condition": null, "target": "ë°˜ë„ì²´ì£¼", "action": "ì œì¼ ë§ì´ ë‚´ë¦° ì£¼ì‹"}}
    ```

# --- ë¶„ê¸°/ì‹¤ì  ë° ì¬ë¬´ì§€í‘œ ì¡°ê±´ ì˜ˆì‹œ ---
6.  User Query: "ì˜¬í•´ 1ë¶„ê¸° ì‹¤ì ì´ ì¢‹ì•˜ë˜ IT ì£¼ì‹ ì°¾ì•„ì¤˜"
    JSON Output:
    ```json
    {{"query_type": "stock_analysis", "period": "ì˜¬í•´ 1ë¶„ê¸°", "condition": {{"type": "earnings", "performance": "good"}}, "target": "IT ì£¼ì‹", "action": "ì°¾ì•„ì¤˜"}}
    ```
7.  User Query: "PBRì´ 1ë³´ë‹¤ ë‚®ì€ ìš°ëŸ‰ì£¼ ì•Œë ¤ì¤˜"
    JSON Output:
    ```json
    {{"query_type": "stock_analysis", "period": null, "condition": {{"type": "fundamental", "indicator": "PBR", "operator": "<", "value": 1}}, "target": "ìš°ëŸ‰ì£¼", "action": "ì•Œë ¤ì¤˜"}}
    ```

# --- ë‹¨ì¼ ì¢…ëª© í˜„ì¬ê°€ ì¡°íšŒ ì˜ˆì‹œ ---
8.  User Query: "ì‚¼ì„±ì „ì ì§€ê¸ˆ ì–¼ë§ˆì•¼?"
    JSON Output:
    ```json
    {{"query_type": "single_stock_price", "period": null, "condition": null, "target": "ì‚¼ì„±ì „ì", "action": "í˜„ì¬ê°€ ì¡°íšŒ"}}
    ```
9.  User Query: "í•œí™”ì˜¤ì…˜ ì£¼ê°€ ì•Œë ¤ì¤„ë˜"
    JSON Output:
    ```json
    {{"query_type": "single_stock_price", "period": null, "condition": null, "target": "í•œí™”ì˜¤ì…˜", "action": "í˜„ì¬ê°€ ì¡°íšŒ"}}
    ```
# --- â–¼â–¼â–¼ [ì¶”ê°€] ì½”ìŠ¤ë‹¥ ì¢…ëª© ì˜ˆì‹œ â–¼â–¼â–¼ ---
10. User Query: "ì—ì½”í”„ë¡œë¹„ì—  í˜„ì¬ ì£¼ê°€"
    JSON Output:
    ```json
    {{"query_type": "single_stock_price", "period": null, "condition": null, "target": "ì—ì½”í”„ë¡œë¹„ì— ", "action": "í˜„ì¬ê°€ ì¡°íšŒ"}}
    ```
11. User Query: "ì…€íŠ¸ë¦¬ì˜¨ì œì•½ ì£¼ê°€"
    JSON Output:
    ```json
    {{"query_type": "single_stock_price", "period": null, "condition": null, "target": "ì…€íŠ¸ë¦¬ì˜¨ì œì•½", "action": "í˜„ì¬ê°€ ì¡°íšŒ"}}
    ```
12. User Query: "ì¹´ì¹´ì˜¤ê²Œì„ì¦ˆ ì–¼ë§ˆì—ìš”?"
    JSON Output:
    ```json
    {{"query_type": "single_stock_price", "period": null, "condition": null, "target": "ì¹´ì¹´ì˜¤ê²Œì„ì¦ˆ", "action": "í˜„ì¬ê°€ ì¡°íšŒ"}}
    ```

13. User Query: "ì˜¤ëŠ˜ ê±°ë˜ëŸ‰ì´ ê°€ì¥ ë§ì´ í„°ì§„ ì£¼ì‹ì€?"
    JSON Output:
    ```json
    {{"query_type": "stock_analysis", "period": "ì˜¤ëŠ˜", "condition": {{"type": "volume", "level": "highest"}}, "target": "ì£¼ì‹", "action": "ê±°ë˜ëŸ‰ì´ ê°€ì¥ ë§ì´ í„°ì§„ ì£¼ì‹"}}
    ```
14. User Query: "ê¸ˆë¦¬ ì¸ìƒê¸°ì— ê°€ì¥ ë§ì´ ì˜¬ëë˜ ì€í–‰ì£¼ëŠ”?"
    JSON Output:
    ```json
    {{"query_type": "stock_analysis", "period": "ê¸ˆë¦¬ ì¸ìƒê¸°", "condition": null, "target": "ì€í–‰ì£¼", "action": "ê°€ì¥ ë§ì´ ì˜¬ëë˜ ì£¼ì‹"}}
    ```
15. User Query: "ë°°ë‹¹ìˆ˜ìµë¥  ë†’ì€ í†µì‹ ì£¼ ì•Œë ¤ì¤˜"
    JSON Output:
    ```json
    {{"query_type": "stock_analysis", "period": null, "condition": {{"type": "fundamental", "indicator": "dividend_yield", "operator": ">", "value": "high"}}, "target": "í†µì‹ ì£¼", "action": "ë°°ë‹¹ìˆ˜ìµë¥  ë†’ì€ ì£¼ì‹"}}
    ```
16. User Query: "ê°€ì¥ ë§ì´ ì˜¤ë¥¸ í…Œë§ˆë“¤ ë‚˜ì—´í•´ì¤˜"
    JSON Output:
    ```json
    {{"query_type": "theme_ranking", "period": null, "condition": null, "target": "í…Œë§ˆ", "action": "ê°€ì¥ ë§ì´ ì˜¤ë¥¸ í…Œë§ˆ"}}
    ```

## Example (Plain Text Output):

1.  User Query: "ì£¼ì‹ ì´ˆë³´ì¸ë° ì–´ë–¤ ì¢…ëª©ì´ ì¢‹ì•„?"
    Plain Text Output: ì£¼ì‹ íˆ¬ìë¥¼ ì²˜ìŒ ì‹œì‘í•˜ì‹œëŠ”êµ°ìš”! íŠ¹ì • ì¢…ëª©ì„ ì¶”ì²œí•´ë“œë¦¬ê¸°ë³´ë‹¤ëŠ”, ë¨¼ì € ì‹œê°€ì´ì•¡ì´ í¬ê³  ëª¨ë‘ê°€ ì˜ ì•„ëŠ” ìš°ëŸ‰ì£¼ë¶€í„° ì†Œì•¡ìœ¼ë¡œ ì‹œì‘í•´ë³´ì‹œëŠ” ê²ƒì„ ê¶Œí•´ë“œë ¤ìš”. ì‚¼ì„±ì „ìë‚˜ SKí•˜ì´ë‹‰ìŠ¤ ê°™ì€ ì¢…ëª©ë“¤ì˜ ìµœê·¼ ë‰´ìŠ¤ì™€ ë¦¬í¬íŠ¸ë¥¼ ê¾¸ì¤€íˆ ì‚´í´ë³´ì‹œëŠ” ê²ƒë„ ì¢‹ì€ ê³µë¶€ê°€ ë  ê±°ì˜ˆìš”.
2.  User Query: "3ë…„ ë’¤ì— ì‚¼ì„±ì „ì ì£¼ê°€ ì–¼ë§ˆì¼ê¹Œ?"
    Plain Text Output: ë¯¸ë˜ì˜ ì£¼ê°€ë¥¼ ì •í™•íˆ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì€ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤. ì£¼ê°€ëŠ” ìˆ˜ë§ì€ ê²½ì œ ì§€í‘œ, ì‹œì¥ ìƒí™©, ê¸°ì—…ì˜ ì‹¤ì  ë“±ì— ë”°ë¼ ë³€ë™í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ë‹¤ë§Œ, ê¸°ì—…ì˜ ì¬ë¬´ ìƒíƒœë‚˜ ì„±ì¥ ê°€ëŠ¥ì„±ì„ ë¶„ì„í•˜ë©° ì¥ê¸°ì ì¸ ê´€ì ì—ì„œ íˆ¬ì ê²°ì •ì„ ë‚´ë¦¬ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.
3.  User Query: "ë‹¨íƒ€ë‘ ì¥ê¸°íˆ¬ì ì¤‘ì— ë­ê°€ ë” ë‚˜ì•„?"
    Plain Text Output: ë‹¨íƒ€ì™€ ì¥ê¸°íˆ¬ìëŠ” ê°ìì˜ ì¥ë‹¨ì ì´ ìˆì–´ ì–´ëŠ í•œìª½ì´ ì ˆëŒ€ì ìœ¼ë¡œ ë‚«ë‹¤ê³  ë§í•˜ê¸°ëŠ” ì–´ë µìŠµë‹ˆë‹¤. ë‹¨íƒ€ëŠ” ë¹ ë¥¸ ìˆ˜ìµì„ ê¸°ëŒ€í•  ìˆ˜ ìˆì§€ë§Œ ë†’ì€ ìœ„í—˜ê³¼ ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ ë™ë°˜í•˜ë©°, ì¥ê¸°íˆ¬ìëŠ” ì•ˆì •ì ì´ì§€ë§Œ ìˆ˜ìµì„ ë³´ê¸°ê¹Œì§€ ì˜¤ëœ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë³¸ì¸ì˜ íˆ¬ì ì„±í–¥ê³¼ ëª©í‘œì— ë§ëŠ” ë°©ë²•ì„ ì„ íƒí•˜ëŠ” ê²ƒì´ ê°€ì¥ ì¤‘ìš”í•©ë‹ˆë‹¤.

4.  User Query: "PERì´ ë­ì•¼?"
    Plain Text Output: PER(ì£¼ê°€ìˆ˜ìµë¹„ìœ¨)ì€ ì£¼ê°€ë¥¼ ì£¼ë‹¹ìˆœì´ìµ(EPS)ìœ¼ë¡œ ë‚˜ëˆˆ ê°’ìœ¼ë¡œ, ê¸°ì—…ì´ ë²Œì–´ë“¤ì´ëŠ” ì´ìµì— ë¹„í•´ ì£¼ê°€ê°€ ë†’ê²Œ í˜¹ì€ ë‚®ê²Œ í‰ê°€ë˜ì—ˆëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ëŒ€í‘œì ì¸ íˆ¬ì ì§€í‘œì…ë‹ˆë‹¤. PERì´ ë‚®ì„ìˆ˜ë¡ ì£¼ê°€ê°€ ì €í‰ê°€ë˜ì—ˆë‹¤ê³  í•´ì„í•˜ëŠ” ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤.
5.  User Query: "ì•ˆë…•í•˜ì„¸ìš”"
    Plain Text Output: ì•ˆë…•í•˜ì„¸ìš”! ê¸ˆìœµ ë¶„ì„ AIì…ë‹ˆë‹¤. ì£¼ì‹ì´ë‚˜ ê²½ì œì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ì‹œë©´ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”.

## Task:
User Query: "{user_query}"
Your Output:
"""

except Exception as e:
    print(f"AskFin Blueprint: ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨ - {e}")
    model = None

def initialize_global_data():
    """
    ì„œë²„ ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ í˜¸ì¶œë˜ì–´ ì „ì—­ìœ¼ë¡œ ì‚¬ìš©ë  ì£¼ì‹ ê¸°ë³¸ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ìºì‹œí•©ë‹ˆë‹¤.
    """
    global GLOBAL_KRX_LISTING, GLOBAL_TICKER_NAME_MAP, GLOBAL_NAME_TICKER_MAP

    print("[ì• í”Œë¦¬ì¼€ì´ì…˜ ì´ˆê¸°í™”] í•„ìˆ˜ ì£¼ì‹ ë°ì´í„° ë¡œë”© ì‹œì‘...")
    try:
        print("  - KOSPI ë° KOSDAQ ì¢…ëª© ëª©ë¡ (FDR) ë¡œë”© ì¤‘...")
        GLOBAL_KRX_LISTING = fdr.StockListing('KRX')
        print(f"  - ì¢…ëª© ëª©ë¡ ë¡œë”© ì™„ë£Œ. ì´ {len(GLOBAL_KRX_LISTING)}ê°œ ì¢…ëª©.")
        print(f"  - GLOBAL_KRX_LISTING ì»¬ëŸ¼: {GLOBAL_KRX_LISTING.columns.tolist()}")

        GLOBAL_KRX_LISTING['FullCode'] = GLOBAL_KRX_LISTING.apply(
            lambda row: f"{row['Code']}.KQ" if row['Market'] == 'KOSDAQ' else f"{row['Code']}.KS", axis=1
        )
        print(f"  - GLOBAL_KRX_LISTINGì— 'FullCode' ì»¬ëŸ¼ ì¶”ê°€ ì™„ë£Œ.")

        # 2. ì¢…ëª© ì½”ë“œ <-> ì´ë¦„ ë§¤í•‘ ë¡œë”© (pykrx)
        print("  - ì¢…ëª© ì½”ë“œ/ì´ë¦„ ë§¤í•‘ (pykrx) ë¡œë”© ì¤‘...")
        all_tickers = stock.get_market_ticker_list(market="ALL")
        GLOBAL_TICKER_NAME_MAP = {ticker: stock.get_market_ticker_name(ticker) for ticker in all_tickers}
        GLOBAL_NAME_TICKER_MAP = {name: ticker for ticker, name in GLOBAL_TICKER_NAME_MAP.items()}
        print(f"  - ì¢…ëª© ì½”ë“œ/ì´ë¦„ ë§¤í•‘ ìƒì„± ì™„ë£Œ. ì´ {len(GLOBAL_NAME_TICKER_MAP)}ê°œ ë§¤í•‘.")


        print("[ì• í”Œë¦¬ì¼€ì´ì…˜ ì´ˆê¸°í™”] ëª¨ë“  í•„ìˆ˜ ì£¼ì‹ ë°ì´í„° ë¡œë”© ì™„ë£Œ.")

    except Exception as e:
        print(f"[ì´ˆê¸°í™” ì˜¤ë¥˜] í•„ìˆ˜ ì£¼ì‹ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {e}")
        traceback.print_exc()


def _load_ticker_maps():
    """
    ì¢…ëª© ì •ë³´ ë§µì„ ì „ì—­ ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ë„ë¡ ë³€ê²½.
    ì´ í•¨ìˆ˜ëŠ” initialize_global_data()ê°€ í˜¸ì¶œëœ í›„ì—ë§Œ ìœ íš¨í•©ë‹ˆë‹¤.
    """
    global GLOBAL_TICKER_NAME_MAP, GLOBAL_NAME_TICKER_MAP
    if GLOBAL_NAME_TICKER_MAP is None:
        print("ê²½ê³ : _load_ticker_maps() í˜¸ì¶œ ì‹œ ê¸€ë¡œë²Œ ì¢…ëª© ë§µì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê°•ì œë¡œ ì´ˆê¸°í™” ì‹œë„.")
        initialize_global_data()

def analyze_institutional_buying(start_date, end_date):
    """
    ì£¼ì–´ì§„ ê¸°ê°„ ë™ì•ˆ ê¸°ê´€ì˜ ìˆœë§¤ìˆ˜ ëŒ€ê¸ˆì„ ê¸°ì¤€ìœ¼ë¡œ ìƒìœ„ ì¢…ëª©ì„ ë¶„ì„í•©ë‹ˆë‹¤.
    """
    print(f"DEBUG: {start_date} ~ {end_date} ê¸°ê°„ì˜ ê¸°ê´€ ìˆœë§¤ìˆ˜ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    try:
        df_kospi = stock.get_market_trading_value_by_date(start_date, end_date, "KOSPI")
        df_kosdaq = stock.get_market_trading_value_by_date(start_date, end_date, "KOSDAQ")
        
        df_all = pd.concat([df_kospi, df_kosdaq]).reset_index()

        if 'ê¸°ê´€ê³„' in df_all.columns:
            df_all.rename(columns={'ê¸°ê´€ê³„': 'ê¸°ê´€'}, inplace=True)

        if 'ê¸°ê´€' not in df_all.columns:
            print("DEBUG: íˆ¬ììë³„ ê±°ë˜ëŒ€ê¸ˆ ë°ì´í„°ì— 'ê¸°ê´€' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return []

        institutional_net_buy = df_all.groupby('í‹°ì»¤')['ê¸°ê´€'].sum().sort_values(ascending=False)
        
        top_stocks = institutional_net_buy.head(50).reset_index()
        
        if GLOBAL_TICKER_NAME_MAP is None:
            initialize_global_data()
            
        analysis_results = []
        for index, row in top_stocks.iterrows():
            ticker = row['í‹°ì»¤']
            net_buy_value = row['ê¸°ê´€']
            
            analysis_results.append({
                "code": ticker,
                "name": GLOBAL_TICKER_NAME_MAP.get(ticker, "N/A"),
                "value": round(net_buy_value / 1_0000_0000, 2), # ì–µ ë‹¨ìœ„ë¡œ ë³€í™˜
                "label": "ê¸°ê´€ ìˆœë§¤ìˆ˜(ì–µ ì›)",
            })
        
        print(f"DEBUG: ê¸°ê´€ ìˆœë§¤ìˆ˜ ìƒìœ„ {len(analysis_results)}ê°œ ì¢…ëª© ë¶„ì„ ì™„ë£Œ.")
        return analysis_results

    except Exception as e:
        print(f"ê¸°ê´€ ìˆœë§¤ìˆ˜ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        traceback.print_exc()
        return []

def _get_fdr_indicator(indicator_info, intent_json):
    """FinanceDataReaderë¥¼ í†µí•´ ì¼ë³„ ì§€í‘œë¥¼ ì¡°íšŒí•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ëŠ” í—¬í¼ í•¨ìˆ˜"""
    try:
        name = indicator_info['name']
        code = indicator_info['code']


        period_str = intent_json.get("period")
        req_start_date, req_end_date = parse_period(period_str)
        query_start_date = req_end_date - timedelta(days=90) 
        query_end_date = req_end_date 
        
        data = fdr.DataReader(code, query_start_date, query_end_date) 
        
        if data.empty:
            return {"error": f"{name} ë°ì´í„° ì¡°íšŒì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."}

        target_data_in_period = data.loc[data.index <= req_end_date].sort_index(ascending=True)

        if target_data_in_period.empty:
             return {"error": f"{name} ì§€í‘œì— ëŒ€í•´ ìš”ì²­í•˜ì‹  ê¸°ê°„({req_end_date.strftime('%Yë…„ %mì›” %dì¼')})ì˜ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}

        latest_for_period = target_data_in_period['Close'].iloc[-1]
        latest_date_for_period = target_data_in_period.index[-1]

        previous_data_in_period = data.loc[data.index < latest_date_for_period].sort_index(ascending=True)
        previous_for_period = None
        if not previous_data_in_period.empty:
            previous_for_period = previous_data_in_period['Close'].iloc[-1]

        if previous_for_period is not None:
            change = latest_for_period - previous_for_period
            change_str = f"{abs(change):.2f} ìƒìŠ¹" if change > 0 else f"{abs(change):.2ff} í•˜ë½" if change < 0 else "ë³€ë™ ì—†ìŒ"
            result_sentence = f"ìš”ì²­í•˜ì‹  ê¸°ê°„ì˜ ë§ˆì§€ë§‰({latest_date_for_period.strftime('%Yë…„ %mì›” %dì¼')}) {name}ëŠ”(ì€) {latest_for_period:,.2f}ì´ë©°, ì§ì „ ì˜ì—…ì¼ ëŒ€ë¹„ {change_str}í–ˆìŠµë‹ˆë‹¤."
        else:
            result_sentence = f"ìš”ì²­í•˜ì‹  ê¸°ê°„ì˜ ë§ˆì§€ë§‰({latest_date_for_period.strftime('%Yë…„ %mì›” %dì¼')}) {name}ëŠ”(ì€) {latest_for_period:,.2f}ì…ë‹ˆë‹¤. ì§ì „ ì˜ì—…ì¼ ë°ì´í„°ê°€ ì—†ì–´ ë³€ë™ ì •ë³´ë¥¼ ì œê³µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        return {
            "query_intent": intent_json,
            "analysis_subject": name,
            "result": [result_sentence]
        }
    except Exception as e:
        return {"error": f"{indicator_info.get('name', 'ì•Œìˆ˜ì—†ëŠ”')} ì§€í‘œ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"}

def _get_bok_indicator(indicator_info, intent_json):
    """í•œêµ­ì€í–‰(BOK) APIë¥¼ í†µí•´ ì›”ë³„ ì§€í‘œë¥¼ ì¡°íšŒí•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ëŠ” í—¬í¼ í•¨ìˆ˜"""
    try:
        name = indicator_info['name']
        bok_api_key = os.getenv("ECOS_API_KEY")
        if not bok_api_key: return {"error": "í•œêµ­ì€í–‰ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}

        end_date = datetime.now().strftime('%Y%m')
        start_date = (datetime.now() - timedelta(days=120)).strftime('%Y%m')
        url = (f"https://ecos.bok.or.kr/api/StatisticSearch/{bok_api_key}/json/kr/1/10/"
               f"{indicator_info['stats_code']}/MM/{start_date}/{end_date}/{indicator_info['item_code']}")

        response = requests.get(url, timeout=10).json()
        rows = response.get("StatisticSearch", {}).get("row", [])
        
        if len(rows) < 2:
            return {"error": f"ìµœê·¼ {name} ë°ì´í„°ë¥¼ ë¹„êµí•  ë§Œí¼ ì¶©ë¶„íˆ ì¡°íšŒí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
            
        latest = rows[-1]
        previous = rows[-2]
        latest_date = f"{latest['TIME'][:4]}ë…„ {latest['TIME'][4:]}ì›”"
        change = float(latest['DATA_VALUE']) - float(previous['DATA_VALUE'])
        change_str = f"{abs(change):.2f} ìƒìŠ¹" if change > 0 else f"{abs(change):.2f} í•˜ë½" if change < 0 else "ë³€ë™ ì—†ìŒ"

        result_sentence = (f"ê°€ì¥ ìµœê·¼({latest_date}) {name}ëŠ”(ì€) {latest['DATA_VALUE']}ì´ë©°, ì „ì›” ëŒ€ë¹„ {change_str}í–ˆìŠµë‹ˆë‹¤.")
        
        return {
            "query_intent": intent_json,
            "analysis_subject": name,
            "result": [result_sentence]
        }
    except Exception as e:
        return {"error": f"í•œêµ­ì€í–‰(BOK) ì§€í‘œ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"}

def execute_comparison_analysis(intent_json):
    """
    [ìˆ˜ì •ë¨] ì—¬ëŸ¬ í…Œë§ˆë¥¼ ë¹„êµ ë¶„ì„í•˜ì—¬ ê²°ê³¼ë¥¼ HTML í…Œì´ë¸”ë¡œ ìƒì„±í•˜ëŠ” í•¨ìˆ˜.
    """
    try:
        targets = intent_json.get("target", [])
        period_str = intent_json.get("period")
        action_str = intent_json.get("action", "")

        if not isinstance(targets, list) or len(targets) < 2:
            return {"error": "ë¹„êµ ë¶„ì„ì„ ìœ„í•´ì„œëŠ” ë‘ ê°œ ì´ìƒì˜ ëŒ€ìƒì´ í•„ìš”í•©ë‹ˆë‹¤."}

        start_date, end_date = parse_period(period_str)
        analysis_period_info = f"{start_date.strftime('%Y-%m-%d')} ~ {end_date.strftime('%Y-%m-%d')}"
        
        comparison_results = []

        for theme in targets:
            target_stocks, _, _ = get_target_stocks(theme)
            if target_stocks.empty:
                continue

            performance_data = analyze_top_performers(target_stocks, [(start_date, end_date)], (start_date, end_date))
            if not performance_data:
                continue

            valid_returns = [item['value'] for item in performance_data if 'value' in item and pd.notna(item['value'])]
            if valid_returns:
                average_return = statistics.mean(valid_returns)
                comparison_results.append({"theme": theme, "average_return": round(average_return, 2)})

        if not comparison_results:
            return {"error": "ìš”ì²­í•˜ì‹  í…Œë§ˆë“¤ì˜ ìˆ˜ìµë¥ ì„ ë¶„ì„í•  ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤."}

        reverse_sort = "ë‚´ë¦°" not in action_str
        sorted_results = sorted(comparison_results, key=lambda x: x['average_return'], reverse=reverse_sort)
        
        # --- HTML í…Œì´ë¸” êµ¬ì¡°ë¡œ ì§ì ‘ ìƒì„± ---
        table_html = '<div class="table-responsive"><table class="table table-sm table-hover financial-table">'
        table_html += '<thead><tr><th>ìˆœìœ„</th><th>í…Œë§ˆ</th><th>ì£¼ìš” ì¢…ëª© í‰ê·  ìˆ˜ìµë¥ </th></tr></thead><tbody>'
        for i, result in enumerate(sorted_results):
            table_html += f"<tr><td>{i+1}</td><td><strong>{result['theme']}</strong></td><td><strong>{result['average_return']:.2f}%</strong></td></tr>"
        table_html += '</tbody></table></div>'
        
        result_text = table_html
        result_text += f"<br><small><i>*ë¶„ì„ ê¸°ê°„: {analysis_period_info}<br>*ë³¸ ë¶„ì„ì€ ê° í…Œë§ˆì— í¬í•¨ëœ ì‹œê°€ì´ì•¡ ìƒìœ„ ì¢…ëª©ë“¤ì„ ê¸°ì¤€ìœ¼ë¡œ ê³„ì‚°ë˜ì—ˆìœ¼ë©°, ì‹¤ì œ ìˆ˜ìµë¥ ê³¼ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ì •ë³´ëŠ” íˆ¬ì ì¶”ì²œì´ ì•„ë‹ˆë©°, ì°¸ê³  ìë£Œë¡œë§Œ í™œìš©í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.</i></small>"

        return {
            "query_intent": intent_json,
            "analysis_subject": f"'{', '.join(targets)}' í…Œë§ˆ ë¹„êµ ë¶„ì„",
            "result": [result_text]
        }
    except Exception as e:
        traceback.print_exc()
        return {"error": f"ë¹„êµ ë¶„ì„ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"}

def execute_theme_ranking(intent_json, page, user_query, cache_key=None):
    """
    [ê°œì„ ] ì „ì²´ í…Œë§ˆ ì„±ê³¼ë¥¼ ë¶„ì„í•˜ê³  ìºì‹œì— ì €ì¥í•˜ì—¬ í˜ì´ì§€ë„¤ì´ì…˜ì„ ì§€ì›í•˜ëŠ” í•¨ìˆ˜.
    """
    try:
        if cache_key and cache_key in ANALYSIS_CACHE and 'full_result' in ANALYSIS_CACHE[cache_key]:
            sorted_result = ANALYSIS_CACHE[cache_key]['full_result']
            analysis_subject = ANALYSIS_CACHE[cache_key]['analysis_subject']
            description = ANALYSIS_CACHE[cache_key]['description']
            print(f" CACHE HIT: í…Œë§ˆ ë­í‚¹ ì „ì²´ ê²°ê³¼ {len(sorted_result)}ê°œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        else:
            print(f" CACHE MISS: ìƒˆë¡œìš´ ì „ì²´ í…Œë§ˆ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
            period_str = intent_json.get("period")
            action_str = intent_json.get("action", "")
            start_date, end_date = parse_period(period_str if period_str else "ìµœê·¼ 1ê°œì›”")
            analysis_period_info = f"{start_date.strftime('%Y-%m-%d')} ~ {end_date.strftime('%Y-%m-%d')}"
            
            themes_from_file = {}
            try:
                themes_file_path = os.path.join(os.path.dirname(__file__), '..', 'cache', 'themes.json')
                with open(themes_file_path, 'r', encoding='utf-8') as f:
                    themes_from_file = json.load(f)
            except Exception as e:
                return {"error": f"í…Œë§ˆ ëª©ë¡ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}"}

            all_themes_results = []
            # --- [í•µì‹¬ ìˆ˜ì •] í…Œë§ˆ ìˆ˜ ì œí•œ ì œê±° ---
            for theme in themes_from_file.keys():
                target_stocks, _, _ = get_target_stocks(theme)
                if target_stocks.empty: continue

                performance_data = analyze_top_performers(target_stocks, [(start_date, end_date)], (start_date, end_date))
                if not performance_data: continue

                valid_returns = [item['value'] for item in performance_data if 'value' in item and pd.notna(item['value'])]
                if valid_returns:
                    average_return = statistics.mean(valid_returns)
                    all_themes_results.append({"theme": theme, "average_return": round(average_return, 2)})

            if not all_themes_results:
                return {"error": "ì „ì²´ í…Œë§ˆì˜ ìˆ˜ìµë¥ ì„ ë¶„ì„í•  ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤."}

            reverse_sort = "ë‚´ë¦°" not in action_str
            sorted_results = sorted(all_themes_results, key=lambda x: x['average_return'], reverse=reverse_sort)
            
            analysis_subject = "ì „ì²´ í…Œë§ˆ ì„±ê³¼ ìˆœìœ„"
            description = f"ë¶„ì„ ê¸°ê°„: {analysis_period_info}"
            
            if not cache_key: cache_key = str(hash(user_query + str(intent_json)))
            ANALYSIS_CACHE[cache_key] = {
                'intent_json': intent_json, 
                'analysis_subject': analysis_subject,
                'description': description,
                'full_result': sorted_results
            }
        
        items_per_page = 20
        total_items = len(sorted_results)
        total_pages = (total_items + items_per_page - 1) // items_per_page
        start_index = (page - 1) * items_per_page
        end_index = start_index + items_per_page
        paginated_result = sorted_results[start_index:end_index]

        final_result_list = []
        for item in paginated_result:
            final_result_list.append({
                "name": item['theme'],
                "value": item['average_return'],
                "label": "í‰ê·  ìˆ˜ìµë¥ (%)"
            })

        return {
            "query_intent": intent_json,
            "analysis_subject": analysis_subject,
            "description": description,
            "result": final_result_list,
            "pagination": { "current_page": page, "total_pages": total_pages, "total_items": total_items },
            "cache_key": cache_key
        }
    except Exception as e:
        traceback.print_exc()
        return {"error": f"í…Œë§ˆ ë­í‚¹ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"}
            

def execute_indicator_lookup(intent_json):
    """
    [ìµœì¢… ìˆ˜ì •] ì—¬ëŸ¬ ì†ŒìŠ¤ì˜ ê²½ì œ ì§€í‘œë¥¼ ì¡°íšŒí•˜ê³  ì±—ë´‡ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜
    """
    target_query = intent_json.get("target", "").lower() # ì‚¬ìš©ì ì¿¼ë¦¬ì˜ ëŒ€ìƒ ì§€í‘œ (ì†Œë¬¸ìë¡œ ë³€í™˜í•˜ì—¬ ë¹„êµ ìš©ì´)

    FDR_INDICATOR_MAP = {
        "í™˜ìœ¨": {"code": "USD/KRW", "name": "ì›/ë‹¬ëŸ¬ í™˜ìœ¨"},
        "ì›ë‹¬ëŸ¬í™˜ìœ¨": {"code": "USD/KRW", "name": "ì›/ë‹¬ëŸ¬ í™˜ìœ¨"},
        "ìœ ê°€": {"code": "CL=F", "name": "WTI êµ­ì œ ìœ ê°€"},
        "wti": {"code": "CL=F", "name": "WTI êµ­ì œ ìœ ê°€"},
        "ê¸ˆê°’": {"code": "GC=F", "name": "ê¸ˆ ì„ ë¬¼"},
        "ê¸ˆê°€ê²©": {"code": "GC=F", "name": "ê¸ˆ ì„ ë¬¼"},
        "ë¯¸êµ­ì±„10ë…„": {"code": "US10YT=X", "name": "ë¯¸ 10ë…„ë¬¼ êµ­ì±„ ê¸ˆë¦¬"},
        "ë¯¸êµ­10ë…„êµ­ì±„": {"code": "US10YT=X", "name": "ë¯¸ 10ë…„ë¬¼ êµ­ì±„ ê¸ˆë¦¬"},
        "ì½”ìŠ¤í”¼": {"code": "KS11", "name": "ì½”ìŠ¤í”¼ ì§€ìˆ˜"},
        "ì½”ìŠ¤ë‹¥": {"code": "KQ11", "name": "ì½”ìŠ¤ë‹¥ ì§€ìˆ˜"},
    }
    
    BOK_INDICATOR_MAP = {
        "cpi": {"stats_code": "901Y001", "item_code": "0", "name": "ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜"},
        "ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜": {"stats_code": "901Y001", "item_code": "0", "name": "ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜"},
        "ì†Œë¹„ìë¬¼ê°€": {"stats_code": "901Y001", "item_code": "0", "name": "ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜"}, # ì¶”ê°€
        "ë¬¼ê°€ì§€ìˆ˜": {"stats_code": "901Y001", "item_code": "0", "name": "ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜"}, # ì¶”ê°€
        "ë¬¼ê°€": {"stats_code": "901Y001", "item_code": "0", "name": "ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜"}, # ì¶”ê°€

        "ê¸°ì¤€ê¸ˆë¦¬": {"stats_code": "722Y001", "item_code": "0001000", "name": "í•œêµ­ ê¸°ì¤€ê¸ˆë¦¬"},
        "í•œêµ­ê¸°ì¤€ê¸ˆë¦¬": {"stats_code": "722Y001", "item_code": "0001000", "name": "í•œêµ­ ê¸°ì¤€ê¸ˆë¦¬"},
        "ê¸ˆë¦¬": {"stats_code": "722Y001", "item_code": "0001000", "name": "í•œêµ­ ê¸°ì¤€ê¸ˆë¦¬"}, # ì¶”ê°€
    }

    for key, indicator_info in FDR_INDICATOR_MAP.items():
        if key in target_query or indicator_info['name'].lower() in target_query:
            result = _get_fdr_indicator(indicator_info, intent_json)
            if result and "error" not in result:
                return result
            return {
                "query_intent": intent_json,
                "analysis_subject": "ì§€í‘œ ì¡°íšŒ ì‹¤íŒ¨",
                "result": [f"'{indicator_info['name']}' ì§€í‘œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” ë° ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."]
            }
    # ì„ì˜ë¡œ ìœ ì‚¬ë„ ì¶”ê°€
    for key, indicator_info in BOK_INDICATOR_MAP.items():
        similarity_score = fuzz.ratio(target_query, key.lower())
        if similarity_score > 70: 
            similarity_score_name = fuzz.ratio(target_query, indicator_info['name'].lower())
            if similarity_score_name > 70 or similarity_score > 70:
                result = _get_bok_indicator(indicator_info, intent_json)
                if result and "error" not in result:
                    return result
            return {
                "query_intent": intent_json,
                "analysis_subject": "ì§€í‘œ ì¡°íšŒ ì‹¤íŒ¨",
                "result": [f"'{indicator_info['name']}' ì§€í‘œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” ë° ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."]
            }

    return {
        "query_intent": intent_json,
        "analysis_subject": "ì§€í‘œ ì¡°íšŒ ì‹¤íŒ¨",
        "result": [f"ìš”ì²­í•˜ì‹  '{intent_json.get('target', 'ì§€í‘œ')}'ëŠ” ì§€ì›í•˜ì§€ ì•ŠëŠ” ì§€í‘œì´ê±°ë‚˜ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (ì§€ì› ì§€í‘œ: í™˜ìœ¨, ìœ ê°€, ê¸ˆê°’, ë¯¸êµ­ì±„10ë…„, ì½”ìŠ¤í”¼, ì½”ìŠ¤ë‹¥, CPI, ê¸°ì¤€ê¸ˆë¦¬)"]
    }




@askfin_bp.route('/stock/<code>/profile')
def get_stock_profile(code):
    """
    [ìˆ˜ì •] DART APIë¥¼ ì‚¬ìš©í•˜ì—¬ 'ì£¼ìš” ê³µì‹œ' ëª©ë¡ì„ ì¡°íšŒí•˜ë„ë¡ ë³€ê²½í•©ë‹ˆë‹¤.
    """
    response_data = {}
    company_name = None
    now = time.time()
    #  12ì‹œê°„(43200ì´ˆ)ì´ ì§€ë‚˜ì§€ ì•Šì•˜ìœ¼ë©´ ìºì‹œëœ ë°ì´í„° ì‚¬ìš©
    if code in STOCK_DETAIL_CACHE and (now - STOCK_DETAIL_CACHE[code]['timestamp'] < 43200):
        print(f"âœ… CACHE HIT: ì¢…ëª©ì½”ë“œ '{code}'ì˜ ìƒì„¸ ì •ë³´ë¥¼ ìºì‹œì—ì„œ ë°˜í™˜í•©ë‹ˆë‹¤.")
        return jsonify(STOCK_DETAIL_CACHE[code]['data'])

    print(f"ğŸ”¥ CACHE MISS: ì¢…ëª©ì½”ë“œ '{code}'ì˜ ìƒì„¸ ì •ë³´ë¥¼ APIë¥¼ í†µí•´ ìƒˆë¡œ ì¡°íšŒí•©ë‹ˆë‹¤.")

    try:
        profile_data = {}
        latest_business_day = stock.get_nearest_business_day_in_a_week()

        if GLOBAL_KRX_LISTING is None:
            initialize_global_data()
        
        krx_list = GLOBAL_KRX_LISTING
        target_info = krx_list[krx_list['Code'] == code]
        if target_info.empty:
            return jsonify({"error": f"ì¢…ëª©ì½”ë“œ '{code}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}), 404
        
        target_info = target_info.iloc[0]
        market = target_info.get('Market', 'KOSPI')
        sector = target_info.get('Sector')
        company_name = target_info.get('Name', 'N/A')
        
        profile_data['ê¸°ì—…ëª…'] = company_name
        profile_data['ì—…ì¢…'] = sector
        profile_data['ì£¼ìš”ì œí’ˆ'] = target_info.get('Industry', 'N/A')

        df_ohlcv = stock.get_market_ohlcv(latest_business_day, market=market)
        df_cap = stock.get_market_cap(latest_business_day, market=market)
        df_funda = stock.get_market_fundamental(latest_business_day, market=market)

        current_price = df_ohlcv.loc[code, 'ì¢…ê°€']
        market_cap = df_cap.loc[code, 'ì‹œê°€ì´ì•¡']
        funda = df_funda.loc[code]

        profile_data['í˜„ì¬ê°€'] = f"{current_price:,} ì›"
        profile_data['ì‹œê°€ì´ì•¡'] = f"{market_cap / 1_0000_0000_0000:.2f} ì¡°ì›" if market_cap > 1_0000_0000_0000 else f"{market_cap / 1_0000_0000:.2f} ì–µì›"
        
        eps = funda.get('EPS', 0)
        profile_data['PER'] = f"{funda.get('PER', 0):.2f} ë°°"
        profile_data['PBR'] = f"{funda.get('PBR', 0):.2f} ë°°"
        profile_data['ë°°ë‹¹ìˆ˜ìµë¥ '] = f"{funda.get('DIV', 0):.2f} %"
        
        response_data["company_profile"] = profile_data

    except Exception as e:
        traceback.print_exc()
        response_data["profile_error"] = f"ê¸°ì—… ê°œìš” ì •ë³´ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"


    if company_name:
        try:
            corp_list = dart.get_corp_list()
            
            # --- START OF MODIFICATION ---
            # Search for the corporation by name
            found_corps = corp_list.find_by_corp_name(company_name, exactly=True)
            
            corp = None
            if found_corps: # Check if the list is not empty
                corp = found_corps[0] # Assign the first found corporation
            
            if not corp: # Explicitly check if corp is None after assignment
                raise ValueError(f"DARTì—ì„œ '{company_name}'ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            # --- END OF MODIFICATION ---

            reports = corp.search_filings(bgn_de=(datetime.now() - timedelta(days=365)).strftime('%Y%m%d'), last_reprt_at='Y')
            response_data["report_list"] = [{
                'report_nm': r.report_nm, 'flr_nm': r.flr_nm, 'rcept_dt': r.rcept_dt,
                'url': f"http://dart.fss.or.kr/dsaf001/main.do?rcpNo={r.rcept_no}"
            } for r in reports[:15]] if reports else []
            
            api_url = "https://opendart.fss.or.kr/api/fnlttSinglAcnt.json"
            fs_data_list = [] 
            
            current_year = datetime.now().year
            years_to_fetch = [str(year) for year in range(current_year, current_year - 4, -1)]

            for year in years_to_fetch:
                for reprt_code in ['11011', '11013', '11012']:
                    params = {
                        'crtfc_key': DART_API_KEY,
                        'corp_code': corp.corp_code, # 'corp' is now guaranteed to be defined here
                        'bsns_year': str(year),
                        'reprt_code': reprt_code,
                        'fs_div': 'CFS'
                    }
                    res = requests.get(api_url, params=params)
                    data = res.json()

                    if data.get('status') == '000' and data.get('list'):
                        for item in data['list']:
                            if 'thstrm_end_dt' in item and item['thstrm_end_dt']:
                                item['report_date_key'] = item['thstrm_end_dt']
                            elif 'thstrm_dt' in item and item['thstrm_dt']:
                                date_range_str = item['thstrm_dt']
                                match = re.search(r'(\d{4}\.\d{2}\.\d{2})$', date_range_str.strip())
                                if match:
                                    item['report_date_key'] = match.group(1)
                                else:
                                    item['report_date_key'] = date_range_str.strip()
                            else:
                                item['report_date_key'] = None
                        fs_data_list.extend(data['list'])
                        break
                    elif params['fs_div'] == 'CFS':
                        params['fs_div'] = 'OFS'
                        res = requests.get(api_url, params=params)
                        data = res.json()
                        if data.get('status') == '000' and data.get('list'):
                             for item in data['list']:
                                if 'thstrm_end_dt' in item and item['thstrm_end_dt']:
                                    item['report_date_key'] = item['thstrm_end_dt']
                                elif 'thstrm_dt' in item and item['thstrm_dt']:
                                    date_range_str = item['thstrm_dt']
                                    match = re.search(r'(\d{4}\.\d{2}\.\d{2})$', date_range_str.strip())
                                    if match:
                                        item['report_date_key'] = match.group(1)
                                    else:
                                        item['report_date_key'] = date_range_str.strip()
                                else:
                                    item['report_date_key'] = None
                             fs_data_list.extend(data['list'])
                             break

            if fs_data_list:
                df = pd.DataFrame(fs_data_list)
                
                df = df.drop_duplicates(subset=['rcept_no', 'account_nm'], keep='last')
                
                df['thstrm_amount'] = df['thstrm_amount'].astype(str).str.replace(',', '', regex=False).str.strip()
                df['thstrm_amount'] = pd.to_numeric(df['thstrm_amount'], errors='coerce')
                df.dropna(subset=['thstrm_amount'], inplace=True)
                
                def extract_and_clean_date(date_string): # This helper function is now defined here again or needs to be outside if used multiple places
                    if not isinstance(date_string, str): # Add this check for safety
                        return None
                    match = re.search(r'(\d{4}\.\d{2}\.\d{2})$', date_string.strip())
                    if match:
                        return match.group(1)
                    parts = date_string.split('~')
                    if len(parts) > 1:
                        match = re.search(r'(\d{4}\.\d{2}\.\d{2})$', parts[-1].strip())
                        if match:
                            return match.group(1)
                    return None
                
                # Use the report_date_key if it was set, otherwise apply the cleaning to thstrm_dt
                if 'report_date_key' not in df.columns: # Fallback if direct assignment didn't happen for all
                    df['report_date_key'] = df['thstrm_dt'].astype(str).apply(extract_and_clean_date)


                df['report_date_key'] = pd.to_datetime(df['report_date_key'], format='%Y.%m.%d', errors='coerce')
                df.dropna(subset=['report_date_key'], inplace=True)
                
                df_pivot = df.pivot_table(index='account_nm', columns='report_date_key', values='thstrm_amount', aggfunc='first')
                df_pivot = df_pivot.fillna(0)

                df_pivot = df_pivot.sort_index(axis=1) 
                
                display_columns = df_pivot.columns[-4:] if len(df_pivot.columns) >= 4 else df_pivot.columns
                
                response_data["key_financial_info"] = json.dumps(
                    {
                        "columns": [col.strftime('%Y.%m') for col in display_columns],
                        "index": list(df_pivot.index),
                        "data": df_pivot[display_columns].values.tolist()
                    }, ensure_ascii=False
                )
                print("2024ë…„ë„ ì£¼ìš” ì¬ë¬´ì •ë³´ API í˜¸ì¶œ ì„±ê³µ ë° ì²˜ë¦¬ ì™„ë£Œ")
            else:
                response_data["financials_error"] = "ì£¼ìš” ì¬ë¬´ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. DART APIì—ì„œ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜ íŒŒì‹± ì˜¤ë¥˜."

        except Exception as e:
            print(f"ì¬ë¬´ì œí‘œ ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì¹˜ëª…ì ì¸ ì˜¤ë¥˜ ë°œìƒ: {e}")
            traceback.print_exc()
            response_data["financials_error"] = f"ì¬ë¬´ì œí‘œë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}"
    if "error" not in response_data:
        STOCK_DETAIL_CACHE[code] = {
            'data': response_data,
            'timestamp': now
        }
        
    return jsonify(response_data)

def get_target_stocks(target_str):
    """
    [ê°œì„ ] FuzzyWuzzyì™€ Sector(ì—…ì¢…) ê¸°ë°˜ ê²€ìƒ‰, ê·¸ë¦¬ê³  ì¢…ëª©ëª… ì§ì ‘ ê²€ìƒ‰ ë¡œì§ ê°•í™”
    """
    global GLOBAL_KRX_LISTING
    if GLOBAL_KRX_LISTING is None:
        initialize_global_data()
        if GLOBAL_KRX_LISTING is None:
            return pd.DataFrame(), "ì´ˆê¸°í™” ì‹¤íŒ¨", None

    krx = GLOBAL_KRX_LISTING
    analysis_subject = "ì‹œì¥ ì „ì²´"
    disambiguation_candidates = None
    GENERIC_TARGETS = {"ì£¼ì‹", "ì¢…ëª©", "ê¸‰ë“±ì£¼", "ìš°ëŸ‰ì£¼", "ì¸ê¸°ì£¼", "ì „ì²´"}
    
    if not target_str or target_str.strip() in GENERIC_TARGETS:
        return krx, analysis_subject, disambiguation_candidates

    keyword = target_str.replace(" ê´€ë ¨ì£¼", "").replace(" í…Œë§ˆì£¼", "").replace(" í…Œë§ˆ", "").replace("ì£¼", "").strip()

    # 1. í…Œë§ˆ íŒŒì¼ì—ì„œ Fuzzy Matching
    try:
        themes_file_path = os.path.join(os.path.dirname(__file__), '..', 'cache', 'themes.json')
        with open(themes_file_path, 'r', encoding='utf-8') as f:
            themes_from_file = json.load(f)
        
        best_match = process.extractOne(keyword, themes_from_file.keys(), scorer=fuzz.token_sort_ratio)
        if best_match and best_match[1] > 80:
            matched_theme_name = best_match[0]
            target_codes = [stock.get('code') for stock in themes_from_file[matched_theme_name] if stock.get('code')]
            target_stocks = krx[krx['Code'].isin(target_codes)]
            if not target_stocks.empty:
                analysis_subject = f"'{matched_theme_name}' í…Œë§ˆ"
                return target_stocks, analysis_subject, None
    except Exception as e:
        print(f"ê²½ê³ : 'themes.json' íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

    # 2. Sector(ì—…ì¢…) ê¸°ë°˜ ê²€ìƒ‰
    if 'Sector' in krx.columns:
        sector_matches = krx[krx['Sector'].str.contains(keyword, na=False)]
        if not sector_matches.empty:
            analysis_subject = f"'{keyword}' ì—…ì¢…"
            return sector_matches, analysis_subject, None

    # 3. ì¢…ëª©ëª… ì§ì ‘ ê²€ìƒ‰
    partial_matches = krx[krx['Name'].str.contains(keyword, na=False)]
    if not partial_matches.empty:
        analysis_subject = f"'{keyword}' í¬í•¨ ì¢…ëª©"
        return partial_matches, analysis_subject, None
    
    return pd.DataFrame(), f"'{target_str}'", None


def parse_period(period_str):
    """'ì§€ë‚œ 3ë…„ê°„' ê°™ì€ ë¬¸ìì—´ì„ ì‹œì‘ì¼ê³¼ ì¢…ë£Œì¼ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜ (ê¸°ëŠ¥ í™•ì¥)"""
    today = datetime.now()
    if not period_str:
        return today - timedelta(days=365), today # ê¸°ë³¸ê°’: ìµœê·¼ 1ë…„

    try:
        if "ì˜¤ëŠ˜" in period_str:
            return today - timedelta(days=1), today
        if "ì–´ì œ" in period_str:
            yesterday = today - timedelta(days=1)
            return yesterday.replace(hour=0, minute=0, second=0, microsecond=0), yesterday.replace(hour=23, minute=59, second=59)
        if "ì´ë²ˆì£¼" in period_str:
            start_of_week = today - timedelta(days=today.weekday()) # ì´ë²ˆ ì£¼ ì›”ìš”ì¼
            return start_of_week, today
        if "ì§€ë‚œì£¼" in period_str:
            end_of_last_week = today - timedelta(days=today.weekday() + 1)
            start_of_last_week = end_of_last_week - timedelta(days=6)
            return start_of_last_week.replace(hour=0, minute=0), end_of_last_week.replace(hour=23, minute=59)
        if "ì§€ë‚œ ë‹¬" in period_str or "ì§€ë‚œë‹¬" in period_str:
            first_day_of_current_month = today.replace(day=1)
            last_day_of_last_month = first_day_of_current_month - timedelta(days=1)
            first_day_of_last_month = last_day_of_last_month.replace(day=1)
            return first_day_of_last_month, last_day_of_last_month

        if "ë¶„ê¸°" in period_str:
            quarter_match = re.search(r'(\d)ë¶„ê¸°', period_str)
            if quarter_match:
                quarter = int(quarter_match.group(1))
                year = today.year
                if "ì‘ë…„" in period_str:
                    year -= 1
                
                start_month = 3 * quarter - 2
                end_month = 3 * quarter
                start_date = datetime(year, start_month, 1)
                # ë‹¤ìŒ ë‹¬ì˜ ì²«ë‚ ì—ì„œ í•˜ë£¨ë¥¼ ë¹¼ì„œ ë§ˆì§€ë§‰ ë‚ ì„ êµ¬í•¨
                if end_month == 12:
                    end_date = datetime(year, 12, 31)
                else:
                    end_date = datetime(year, end_month + 1, 1) - timedelta(days=1)
                return start_date, end_date

        # --- ê¸°ì¡´ ë¡œì§ (ì¼/ê°œì›”/ë…„) ---
        if "ì¼" in period_str:
            days_match = re.search(r'(\d+)', period_str)
            if days_match:
                days = int(days_match.group(0))
                return today - timedelta(days=days), today
        if "ê°œì›”" in period_str:
            months_match = re.search(r'(\d+)', period_str)
            if months_match:
                months = int(months_match.group(0))
                return today - timedelta(days=30 * months), today
        if "ë…„ê°„" in period_str or "ë…„" in period_str:
            years_match = re.search(r'(\d+)', period_str)
            if years_match:
                years = int(years_match.group(0))
                return today - timedelta(days=365 * years), today

    except (ValueError, TypeError, AttributeError):
        pass 

    return today - timedelta(days=365), today

def get_interest_rate_hike_dates(api_key):
    """í•œêµ­ì€í–‰ APIë¡œ ê¸°ì¤€ê¸ˆë¦¬ ì¸ìƒì¼ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜."""
    stats_code, item_code = "722Y001", "0001000"
    end_date = datetime.now().strftime('%Y%m%d')
    start_date = (datetime.now() - timedelta(days=5*365)).strftime('%Y%m%d')
    url = f"https://ecos.bok.or.kr/api/StatisticSearch/{api_key}/json/kr/1/1000/{stats_code}/DD/{start_date}/{end_date}/{item_code}"
    
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        data = response.json()
        
        if "StatisticSearch" not in data or "row" not in data.get("StatisticSearch", {}):
            return []

        df = pd.DataFrame(data["StatisticSearch"]["row"])
        df['TIME'] = pd.to_datetime(df['TIME'], format='%Y%m%d')
        df['DATA_VALUE'] = pd.to_numeric(df['DATA_VALUE'])
        df = df.sort_values(by='TIME').reset_index(drop=True)
        df['PREV_VALUE'] = df['DATA_VALUE'].shift(1)
        
        hike_dates = df[df['DATA_VALUE'] > df['PREV_VALUE']]['TIME'].tolist()
        return hike_dates
    except Exception as e:
        print(f"í•œêµ­ì€í–‰ API ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        return []
    

def analyze_target_price_upside(target_stocks):
    """
    [ìµœì í™”] ë„¤ì´ë²„ ì¦ê¶Œ ì»¨ì„¼ì„œìŠ¤ í˜ì´ì§€ë¥¼ ì¼ê´„ ìŠ¤í¬ë ˆì´í•‘í•˜ì—¬ ëª©í‘œì£¼ê°€ ê´´ë¦¬ìœ¨ì„ ë¶„ì„í•©ë‹ˆë‹¤.
    """
    print("ëª©í‘œì£¼ê°€ ì»¨ì„¼ì„œìŠ¤ ë°ì´í„° ì¼ê´„ ì¡°íšŒ ì‹œì‘...")
    try:
        url = "https://finance.naver.com/sise/consensus.naver?&target=up"
        headers = {'User-Agent': 'Mozilla/5.0'}
        
        df_list = pd.read_html(requests.get(url, headers=headers, timeout=10).text)
        df = df_list[1]
        
        df = df.dropna(axis='index', how='all')
        df.columns = ['ì¢…ëª©ëª…', 'ëª©í‘œì£¼ê°€', 'íˆ¬ìì˜ê²¬', 'í˜„ì¬ê°€', 'ê´´ë¦¬ìœ¨', 'ì¦ê¶Œì‚¬', 'ì‘ì„±ì¼']
        df = df[df['ì¢…ëª©ëª…'].notna()]

        df['ëª©í‘œì£¼ê°€'] = pd.to_numeric(df['ëª©í‘œì£¼ê°€'], errors='coerce')
        df['í˜„ì¬ê°€'] = pd.to_numeric(df['í˜„ì¬ê°€'], errors='coerce')
        df['ê´´ë¦¬ìœ¨'] = df['ê´´ë¦¬ìœ¨'].str.strip('%').astype(float)
        df = df.dropna(subset=['ëª©í‘œì£¼ê°€', 'í˜„ì¬ê°€', 'ê´´ë¦¬ìœ¨'])

        if GLOBAL_KRX_LISTING is None:
            initialize_global_data()
        krx_list = GLOBAL_KRX_LISTING[['Name', 'Code']]
        df = pd.merge(df, krx_list, left_on='ì¢…ëª©ëª…', right_on='Name', how='inner')
        
        print("ë°ì´í„° ì¡°íšŒ ë° ê°€ê³µ ì™„ë£Œ.")
        
        analysis_results = []
        for index, row in df.iterrows():
            analysis_results.append({
                "code": row['Code'],
                "name": row['ì¢…ëª©ëª…'],
                "value": row['ê´´ë¦¬ìœ¨'],
                "label": "ëª©í‘œì£¼ê°€ ê´´ë¦¬ìœ¨(%)",
                "start_price": int(row['í˜„ì¬ê°€']),
                "end_price": int(row['ëª©í‘œì£¼ê°€'])
            })
            
        return analysis_results

    except Exception as e:
        print(f"ëª©í‘œì£¼ê°€ ì»¨ì„¼ì„œìŠ¤ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return []

def execute_single_stock_price(intent_json):
    """
    [ì™„ì „íŒ] ë„ì–´ì“°ê¸° ëŒ€ì‘, ê·¸ë˜í”„ ë°ì´í„° ìƒì„± ê¸°ëŠ¥ì´ ëª¨ë‘ í¬í•¨ëœ ë²„ì „
    """
    try:
        if GLOBAL_NAME_TICKER_MAP is None:
            initialize_global_data()

        target_name = intent_json.get("target")
        if not target_name:
            return {"error": "ì¢…ëª©ëª…ì´ ì§€ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}

        cleaned_name = target_name.replace(" ", "")
        ticker = GLOBAL_NAME_TICKER_MAP.get(cleaned_name)
        
        if not ticker:
            ticker = GLOBAL_NAME_TICKER_MAP.get(target_name) 
            if not ticker:
                return {
                    "analysis_subject": "ì˜¤ë¥˜",
                    "result": [f"'{target_name}'ì— í•´ë‹¹í•˜ëŠ” ì¢…ëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì¢…ëª©ëª…ì„ í™•ì¸í•´ì£¼ì„¸ìš”."]
                }

        latest_bday_dt = datetime.strptime(stock.get_nearest_business_day_in_a_week(), '%Y%m%d')
        start_date_for_chart = (latest_bday_dt - timedelta(days=45)).strftime('%Y%m%d')
        latest_bday_str = latest_bday_dt.strftime('%Y%m%d')

        df = stock.get_market_ohlcv_by_date(fromdate=start_date_for_chart, todate=latest_bday_str, ticker=ticker)

        if df.empty:
            return {
                "analysis_subject": "ì •ë³´ ì—†ìŒ",
                "result": [f"'{target_name}'ì˜ ê±°ë˜ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."]
            }

        chart_data = []
        df_for_chart = df.reset_index()
        for index, row in df_for_chart.iterrows():
            chart_data.append({
                "date": row['ë‚ ì§œ'].strftime('%Y-%m-%d'),
                "price": row['ì¢…ê°€']
            })

        stock_info = df.iloc[-1]
        current_price = stock_info['ì¢…ê°€']
        change = stock_info['ì¢…ê°€'] - stock_info['ì‹œê°€']
        
        price_color_class = "price-up" if change > 0 else "price-down" if change < 0 else ""
        change_str = f"{abs(change):,}ì› ìƒìŠ¹" if change > 0 else f"{abs(change):,}ì› í•˜ë½" if change < 0 else "ë³€ë™ ì—†ìŒ"
        date_str = df.index[-1].strftime('%Y-%m-%d')
        
        result_sentence = (
            f"{target_name}({ticker})ì˜ ê°€ì¥ ìµœê·¼ ì¢…ê°€({date_str})ëŠ” "
            f"<span class='{price_color_class}'>{current_price:,}ì›</span>ì´ë©°, ì‹œê°€ ëŒ€ë¹„ <span class='{price_color_class}'>{change_str}</span>í–ˆìŠµë‹ˆë‹¤."
        )

        return {
            "query_intent": intent_json,
            "analysis_subject": f"{target_name} í˜„ì¬ê°€",
            "result": [result_sentence],
            "chart_data": chart_data,
            "stock_code": ticker,
            "stock_name": target_name
        }
    except Exception as e:
        traceback.print_exc()
        return {"error": f"ë‹¨ì¼ ì¢…ëª© ê°€ê²© ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"}
        
def execute_stock_analysis(intent_json, page, user_query, cache_key=None):
    """
    [ê°œì„ ] ì¬ë¬´ ì§€í‘œ í•„í„°ë§ ë° ìºì‹± ë¡œì§ ê°•í™”
    """
    try:
        action_str = intent_json.get("action", "")
        
        if cache_key and cache_key in ANALYSIS_CACHE and 'full_result' in ANALYSIS_CACHE[cache_key]:
            sorted_result = ANALYSIS_CACHE[cache_key]['full_result']
            analysis_subject = ANALYSIS_CACHE[cache_key]['analysis_subject']
            print(f" CACHE HIT: ìºì‹œëœ ì „ì²´ ê²°ê³¼ {len(sorted_result)}ê°œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        else:
            print(f" CACHE MISS: ìƒˆë¡œìš´ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
            target_str = intent_json.get("target")
            condition_obj = intent_json.get("condition")
            target_stocks, analysis_subject, disambiguation_candidates = get_target_stocks(target_str)
            
            if disambiguation_candidates:
                return {"analysis_subject": f"'{target_str}' ì¢…ëª© ëª…í™•í™” í•„ìš”", "result_type": "disambiguation", "candidates": disambiguation_candidates, "result": []}

            if target_stocks.empty:
                return {"analysis_subject": analysis_subject, "result": [f"{analysis_subject}ì— í•´ë‹¹í•˜ëŠ” ì¢…ëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."]}

            if isinstance(condition_obj, dict) and condition_obj.get("type") == "fundamental":
                today_str = stock.get_nearest_business_day_in_a_week()
                funda_df = stock.get_market_fundamental(today_str, market="ALL")
                target_stocks = pd.merge(target_stocks, funda_df, left_on='Code', right_index=True, how='inner')
                if not target_stocks.empty:
                    indicator = condition_obj.get("indicator", "").upper()
                    operator = condition_obj.get("operator")
                    value = float(condition_obj.get("value"))
                    if indicator in target_stocks.columns:
                        if operator == "<": target_stocks = target_stocks[target_stocks[indicator] < value]
                        elif operator == ">": target_stocks = target_stocks[target_stocks[indicator] > value]
                        analysis_subject += f" ({indicator} {operator} {value})"

            if target_stocks.empty: return {"analysis_subject": analysis_subject, "result": ["ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ì¢…ëª©ì´ ì—†ìŠµë‹ˆë‹¤."]}

            start_date, end_date = parse_period(intent_json.get("period"))
            
            result_data = analyze_top_performers(target_stocks, [(start_date, end_date)], (start_date, end_date))
            reverse_sort = False if "ë‚´ë¦°" in action_str else True
            sorted_result = sorted(result_data, key=lambda x: x.get('value', -99999), reverse=reverse_sort)
            
            if not cache_key: cache_key = str(hash(user_query + str(intent_json)))
            ANALYSIS_CACHE[cache_key] = {'intent_json': intent_json, 'analysis_subject': analysis_subject, 'full_result': sorted_result}
            print(f"ìƒˆë¡œìš´ ë¶„ì„ ê²°ê³¼ {len(sorted_result)}ê°œë¥¼ ìºì‹œì— ì €ì¥í–ˆìŠµë‹ˆë‹¤. (í‚¤: {cache_key})")
        
        items_per_page = 20
        total_items = len(sorted_result)
        total_pages = (total_items + items_per_page - 1) // items_per_page
        start_index = (page - 1) * items_per_page
        end_index = start_index + items_per_page
        paginated_result = sorted_result[start_index:end_index]

        return {
            "query_intent": intent_json,
            "analysis_subject": analysis_subject,
            "result": paginated_result,
            "pagination": { "current_page": page, "total_pages": total_pages, "total_items": total_items },
            "cache_key": cache_key
        }
    except Exception as e:
        traceback.print_exc()
        return {"error": f"ë¶„ì„ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"}

def handle_season_condition(period_tuple, season):
    """'ì—¬ë¦„' ë˜ëŠ” 'ê²¨ìš¸' ì¡°ê±´ì— ë§ëŠ” ë‚ ì§œ êµ¬ê°„ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜ (ìµœì í™”)"""
    start_date, end_date = period_tuple
    event_periods = []
    
    for year in range(start_date.year, end_date.year + 1):
        if season == "ì—¬ë¦„":
            season_start = datetime(year, 6, 1)
            season_end = datetime(year, 8, 31)
            overlap_start = max(start_date, season_start)
            overlap_end = min(end_date, season_end)
            if overlap_start < overlap_end:
                event_periods.append((overlap_start, overlap_end))

        elif season == "ê²¨ìš¸":
            # Winter can span across two years (Dec of year Y-1 to Feb/Mar of year Y)
            # So we check for both the current year's winter and the previous year's winter that ends in the current year.
            for y in [year - 1, year]: 
                season_start = datetime(y, 12, 1)
                season_end = datetime(y + 1, 3, 1) - timedelta(days=1) # End of Feb or March 1st minus 1 day
                overlap_start = max(start_date, season_start)
                overlap_end = min(end_date, season_end)
                if overlap_start < overlap_end:
                    event_periods.append((overlap_start, overlap_end))

    # Remove duplicates and sort the periods
    return sorted(list(set(event_periods)))

def handle_interest_rate_condition(api_key, period_tuple):
    """ê¸ˆë¦¬ ì¸ìƒ ì¡°ê±´ì— ë§ëŠ” ë‚ ì§œ êµ¬ê°„ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜"""
    start_date, end_date = period_tuple
    hike_dates = get_interest_rate_hike_dates(api_key)
    
    event_periods = []
    for hike_date in hike_dates:
        event_start = hike_date
        event_end = event_start + timedelta(days=7) # Consider a 7-day window after hike
        
        # Ensure the event period is within the overall query period
        if event_start >= start_date and event_end <= end_date:
            event_periods.append((event_start, event_end))
            
    return event_periods

def _fetch_and_analyze_single_stock(stock_code, stock_name, overall_start, overall_end, event_periods):
    """
    ë‹¨ì¼ ì¢…ëª©ì˜ ì „ì²´ ê¸°ê°„ ë°ì´í„°ë¥¼ ì¡°íšŒí•˜ê³ , ê·¸ ì•ˆì—ì„œ ì´ë²¤íŠ¸ ê¸°ê°„ ìˆ˜ìµë¥ ì„ ë¶„ì„í•˜ëŠ” í—¬í¼ í•¨ìˆ˜ (ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•¨)
    """
    try:
        print(f"       ë°ì´í„° ì¡°íšŒ ì‹œì‘: {stock_name}({stock_code})")
        overall_prices = fdr.DataReader(stock_code, overall_start, overall_end)

        if overall_prices.empty:
            print(f"       -> [ë¶„ì„ ì‹¤íŒ¨] {stock_name}({stock_code}): fdr.DataReaderê°€ ë¹ˆ ë°ì´í„°ë¥¼ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤.")
            return None

        print(f"       -> [ë°ì´í„° í™•ì¸] {stock_name}({stock_code}): ì „ì²´ ê¸°ê°„({overall_start.strftime('%Y-%m-%d')}~{overall_end.strftime('%Y-%m-%d')}) ë°ì´í„° {len(overall_prices)}ê°œ ë¡œë“œ ì„±ê³µ.")
        print(f"       -> [ì´ë²¤íŠ¸ ê¸°ê°„ í™•ì¸] ë¶„ì„í•  ì´ë²¤íŠ¸ ê¸°ê°„ ìˆ˜: {len(event_periods)}ê°œ, ì²« ê¸°ê°„: {event_periods[0] if event_periods else 'N/A'}")

        start_price = int(overall_prices['Open'].iloc[0])
        end_price = int(overall_prices['Close'].iloc[-1])

        period_returns = []
        for i, (start, end) in enumerate(event_periods):
            start_ts = pd.to_datetime(start)
            end_ts = pd.to_datetime(end)

            prices_in_period = overall_prices.loc[start_ts:end_ts]
            print(f"       -> ì´ë²¤íŠ¸ ê¸°ê°„ {i+1} ({start_ts.date()}~{end_ts.date()}) ë°ì´í„° ìŠ¬ë¼ì´ì‹± ê²°ê³¼: {len(prices_in_period)}ê°œ")

            if len(prices_in_period) > 1:
                event_start_price = prices_in_period['Open'].iloc[0]
                event_end_price = prices_in_period['Close'].iloc[-1]
                if event_start_price > 0:
                    period_returns.append((event_end_price / event_start_price) - 1)

        if not period_returns:
            print(f"       -> [ë¶„ì„ ì‹¤íŒ¨] {stock_name}({stock_code}): ìœ íš¨í•œ ìˆ˜ìµë¥ ì„ ê³„ì‚°í•  ìˆ˜ ìˆëŠ” ì´ë²¤íŠ¸ ê¸°ê°„ì´ ì—†ìŠµë‹ˆë‹¤.")
            return None

        average_return = statistics.mean(period_returns)
        if pd.notna(average_return):
            return {
                "code": stock_code, "name": stock_name,
                "value": round(average_return * 100, 2), "label": "í‰ê·  ìˆ˜ìµë¥ (%)",
                "start_price": start_price,
                "end_price": end_price,
            }
    except Exception as e:
        print(f"       -> [ë¶„ì„ ì‹¤íŒ¨] {stock_name}({stock_code}) ë¶„ì„ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")

    return None

def analyze_top_performers(target_stocks, event_periods, overall_period):
    """
    [ì„±ëŠ¥ ìµœì í™”] ì „ì²´ ê¸°ê°„ ë°ì´í„°ë¥¼ í•œ ë²ˆì— ì¡°íšŒ í›„, ë©”ëª¨ë¦¬ì—ì„œ ì¡°ê±´ ê¸°ê°„ì„ ìŠ¬ë¼ì´ì‹±í•˜ì—¬ ë¶„ì„ ì†ë„ë¥¼ ê°œì„ í•©ë‹ˆë‹¤.
    ë˜í•œ, ì—¬ëŸ¬ ì¢…ëª©ì˜ ë°ì´í„° ì¡°íšŒë¥¼ ë³‘ë ¬ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    """
    analysis_results = []
    
    try:
        print("DEBUG: nlargest ì‹¤í–‰ ì „, target_stocks ì •ë³´:")
        target_stocks.info() 
        
        top_stocks = target_stocks.nlargest(min(len(target_stocks), 50), 'Marcap').reset_index(drop=True)

    except Exception as e:

        print(target_stocks)
        return [] 

    print(f"ì‹œê°€ì´ì•¡ ìƒìœ„ {len(top_stocks)}ê°œ ì¢…ëª©ì— ëŒ€í•œ ìˆ˜ìµë¥  ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    overall_start, overall_end = overall_period

    with concurrent.futures.ThreadPoolExecutor(max_workers=30) as executor:
        future_to_stock = {
            executor.submit(_fetch_and_analyze_single_stock, stock['Code'], stock['Name'], overall_start, overall_end, event_periods): stock
            for index, stock in top_stocks.iterrows()
        }

        for i, future in enumerate(concurrent.futures.as_completed(future_to_stock)):
            stock_info = future_to_stock[future]
            stock_code, stock_name = stock_info['Code'], stock_info['Name']
            try:
                result = future.result()
                if result:
                    analysis_results.append(result)
                print(f"   ({i + 1}/{len(top_stocks)}) {stock_name}({stock_code}) ë¶„ì„ ì™„ë£Œ.")
            except Exception as exc:
                print(f"   - {stock_name}({stock_code}) ë¶„ì„ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {exc}")
    
    return analysis_results

def analyze_volatility(target_stocks, period_tuple):
    """ë³€ë™ì„± ë¶„ì„ í•¨ìˆ˜ (ë”œë ˆì´ ì œê±°)"""
    analysis_results = []
    start_date, end_date = period_tuple
    top_stocks = target_stocks.nlargest(min(len(target_stocks), 50), 'Marcap').reset_index(drop=True)
    print(f"ì‹œê°€ì´ì•¡ ìƒìœ„ {len(top_stocks)}ê°œ ì¢…ëª©ì— ëŒ€í•œ ë³€ë™ì„± ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...")

    # ThreadPoolExecutorë¥¼ ì‚¬ìš©í•˜ì—¬ ë³€ë™ì„± ë¶„ì„ì„ ìœ„í•œ ë°ì´í„° ì¡°íšŒë¥¼ ë³‘ë ¬ë¡œ ì²˜ë¦¬
    with concurrent.futures.ThreadPoolExecutor(max_workers=30) as executor: # 20ì—ì„œ 30ìœ¼ë¡œ ì¦ê°€
        future_to_stock = {
            executor.submit(
                lambda code, name, s_date, e_date: _fetch_and_calculate_volatility(code, name, s_date, e_date),
                stock_info['Code'], stock_info['Name'], start_date, end_date
            ): stock_info
            for index, stock_info in top_stocks.iterrows()
        }

        for i, future in enumerate(concurrent.futures.as_completed(future_to_stock)):
            stock_info = future_to_stock[future]
            code, name = stock_info['Code'], stock_info['Name']
            try:
                result = future.result()
                if result:
                    analysis_results.append(result)
                print(f" Â  ({i + 1}/{len(top_stocks)}) {name}({code}) ë³€ë™ì„± ë¶„ì„ ì™„ë£Œ.")
            except Exception as exc:
                print(f" Â  - {name}({code}) ë³€ë™ì„± ë¶„ì„ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {exc}")
    
    return analysis_results

def _fetch_and_calculate_volatility(code, name, start_date, end_date):
    """
    ë‹¨ì¼ ì¢…ëª©ì˜ ë°ì´í„°ë¥¼ ì¡°íšŒí•˜ê³  ë³€ë™ì„±ì„ ê³„ì‚°í•˜ëŠ” í—¬í¼ í•¨ìˆ˜ (ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•¨)
    """
    try:
        print(f" Â  Â  Â ë³€ë™ì„± ë°ì´í„° ì¡°íšŒ ì‹œì‘: {name}({code}) - {start_date.strftime('%Y-%m-%d')} ~ {end_date.strftime('%Y-%m-%d')}")
        overall_prices = fdr.DataReader(code, start_date, end_date)
        print(f" Â  Â  Â ë³€ë™ì„± ë°ì´í„° ì¡°íšŒ ì™„ë£Œ: {name}({code})")

        if overall_prices.empty:
            return None
        
        daily_returns = overall_prices['Close'].pct_change().dropna()
        volatility = daily_returns.std()
        if pd.notna(volatility):
            return {
                "code": code, "name": name,
                "value": round(volatility * 100, 2), "label": "ë³€ë™ì„±(%)",
                "start_price": int(overall_prices['Open'].iloc[0]),
                "end_price": int(overall_prices['Close'].iloc[-1])
            }
    except Exception as e:
        print(f" Â  - {name}({code}) ë³€ë™ì„± ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    return None
def analyze_top_volume_stocks(date_str):
    """
    ì£¼ì–´ì§„ ë‚ ì§œì˜ ê±°ë˜ëŸ‰ ìƒìœ„ ì¢…ëª©ì„ ë¶„ì„í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    print(f"DEBUG: {date_str} ê¸°ì¤€ ê±°ë˜ëŸ‰ ìƒìœ„ ì¢…ëª© ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    try:
        # KOSPIì™€ KOSDAQ ì‹œì¥ì˜ ì „ì²´ ì‹œì„¸ ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
        df_all = stock.get_market_ohlcv(date_str, market="ALL")

        if 'ê±°ë˜ëŸ‰' not in df_all.columns:
            print("DEBUG: OHLCV ë°ì´í„°ì— 'ê±°ë˜ëŸ‰' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return []

        # ê±°ë˜ëŸ‰ì„ ê¸°ì¤€ìœ¼ë¡œ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬í•˜ê³  ìƒìœ„ 50ê°œ ì„ íƒ
        top_stocks = df_all.sort_values(by='ê±°ë˜ëŸ‰', ascending=False).head(50)
        
        if GLOBAL_TICKER_NAME_MAP is None:
            initialize_global_data()
            
        analysis_results = []
        for ticker, row in top_stocks.iterrows():
            analysis_results.append({
                "code": ticker,
                "name": GLOBAL_TICKER_NAME_MAP.get(ticker, "N/A"),
                "value": int(row['ê±°ë˜ëŸ‰']),
                "label": "ê±°ë˜ëŸ‰",
                "end_price": int(row['ì¢…ê°€']),
                "start_price": int(row['ì‹œê°€']) # í…Œì´ë¸” í˜•ì‹ í†µì¼ì„ ìœ„í•´ ì¶”ê°€
            })
        
        print(f"DEBUG: ê±°ë˜ëŸ‰ ìƒìœ„ {len(analysis_results)}ê°œ ì¢…ëª© ë¶„ì„ ì™„ë£Œ.")
        return analysis_results

    except Exception as e:
        print(f"ê±°ë˜ëŸ‰ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        traceback.print_exc()
        return []
    

def handle_indicator_condition(condition_obj, period_tuple):
    """CPI, ê¸ˆë¦¬ ë“± ì§€í‘œ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ë‚ ì§œ êµ¬ê°„ì„ ë°˜í™˜"""
    bok_api_key = os.getenv("ECOS_API_KEY")
    if not bok_api_key: return []
    INDICATOR_MAP = {
        "CPI": {"stats_code": "901Y001", "item_code": "0"},
        "ê¸°ì¤€ê¸ˆë¦¬": {"stats_code": "722Y001", "item_code": "0001000"},
    }

    indicator_name = condition_obj.get("name")
    if indicator_name not in INDICATOR_MAP: return []

    indicator_info = INDICATOR_MAP[indicator_name]
    data_series = get_bok_data(bok_api_key, indicator_info['stats_code'], indicator_info['item_code'], period_tuple[0], period_tuple[1])

    if data_series is None: return []

    op_str = condition_obj.get("operator")
    value = condition_obj.get("value")

    if op_str == '>': matching_series = data_series[data_series > value]
    elif op_str == '>=': matching_series = data_series[data_series >= value]
    else: return []

    return [(d.replace(day=1), (d.replace(day=1) + timedelta(days=32)).replace(day=1) - timedelta(days=1)) for d in matching_series.index]

def get_bok_data(bok_api_key, stats_code, item_code, start_date, end_date):
    """
    í•œêµ­ì€í–‰ ECOS APIë¥¼ í†µí•´ íŠ¹ì • ì§€í‘œì˜ ì‹œê³„ì—´ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ Pandas Seriesë¡œ ë°˜í™˜.
    """
    start_str = start_date.strftime('%Y%m')
    end_str = end_date.strftime('%Y%m')
    
    url = f"https://ecos.bok.or.kr/api/StatisticSearch/{bok_api_key}/json/kr/1/1000/{stats_code}/MM/{start_str}/{end_str}/{item_code}"
    
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()  # HTTP ì˜¤ë¥˜ ë°œìƒ ì‹œ ì˜ˆì™¸ ë°œìƒ
        data = response.json()

        if "StatisticSearch" not in data or "row" not in data.get("StatisticSearch", {}):
            print("BOK API ì‘ë‹µì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None

        rows = data["StatisticSearch"]["row"]
        if not rows:
            return None

        df = pd.DataFrame(rows)
        df['TIME'] = pd.to_datetime(df['TIME'], format='%Y%m') 
        df['DATA_VALUE'] = pd.to_numeric(df['DATA_VALUE']) 
        df = df.set_index('TIME')                             
        
        return df['DATA_VALUE'].sort_index()

    except requests.exceptions.RequestException as e:
        print(f"í•œêµ­ì€í–‰ API ìš”ì²­ ì˜¤ë¥˜: {e}")
        return None
    except Exception as e:
        print(f"ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        return None

QUERY_HANDLERS = {
    "stock_analysis": execute_stock_analysis,
    "comparison_analysis": execute_comparison_analysis,
    "indicator_lookup": execute_indicator_lookup,
    "single_stock_price": execute_single_stock_price,
    "theme_ranking": execute_theme_ranking  
}

@askfin_bp.route('/')
def askfin_page():
    return render_template('askfin.html')

@askfin_bp.route('/analyze', methods=['POST'])
def analyze_query():
    if not model:
        return jsonify({"error": "ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. API í‚¤ë¥¼ í™•ì¸í•˜ì„¸ìš”."}), 500
    
    data = request.get_json()
    user_query = data.get('query')
    page = data.get('page', 1)
    cache_key = data.get('cache_key')

    if not user_query:
        return jsonify({"error": "ì˜ëª»ëœ ìš”ì²­ì…ë‹ˆë‹¤."}), 400

    raw_text = ""
    try:
        if cache_key and cache_key in ANALYSIS_CACHE:
            print(f"âœ… CACHE HIT: ìºì‹œëœ ë¶„ì„ ê²°ê³¼ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. (í‚¤: {cache_key})")
            intent_json = ANALYSIS_CACHE[cache_key]['intent_json']
            query_type = intent_json.get("query_type") # ìºì‹œì—ì„œ query_type ê°€ì ¸ì˜¤ê¸°
            handler = QUERY_HANDLERS.get(query_type)
            if handler:
                 # ìºì‹œë¥¼ ì‚¬ìš©í•  ë•ŒëŠ” í˜ì´ì§€ ì •ë³´ë§Œ ë„˜ê²¨ì„œ ì¬ê³„ì‚°
                 return jsonify(handler(intent_json, page, user_query, cache_key))
        
        print(f"ğŸ”¥ CACHE MISS: '{user_query}'ì— ëŒ€í•´ Gemini API ë¶„ì„ì„ ìš”ì²­í•©ë‹ˆë‹¤.")
        prompt = PROMPT_TEMPLATE.format(user_query=user_query)
        response = model.generate_content(prompt)
        raw_text = response.text.strip()
        try:
            start = raw_text.find('{')
            end = raw_text.rfind('}') + 1
            cleaned_response = raw_text[start:end]
            intent_json = json.loads(cleaned_response)
        except (json.JSONDecodeError, IndexError):
            print("DEBUG: JSON íŒŒì‹± ì‹¤íŒ¨. ì¼ë°˜ í…ìŠ¤íŠ¸ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
            return jsonify({"analysis_subject": "ì¼ë°˜ ë‹µë³€", "result": [raw_text.replace('\n', '<br>')]} )

        query_type = intent_json.get("query_type")
        
        if not query_type and intent_json.get("target") and intent_json.get("condition"):
            query_type = "stock_analysis"
            intent_json["query_type"] = query_type
        
        handler = QUERY_HANDLERS.get(query_type)

        if handler:
            if query_type in ["stock_analysis", "theme_ranking"]:
                final_result = handler(intent_json, page, user_query)
            else:
                final_result = handler(intent_json)
        else:
            final_result = {"analysis_subject": "ì¼ë°˜ ë‹µë³€", "result": [raw_text.replace('\n', '<br>')]}
        
        return jsonify(final_result)

    except Exception as e:
        traceback.print_exc()
        return jsonify({"error": f"ë¶„ì„ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"}), 500
    
            
@askfin_bp.route('/new_chat', methods=['POST'])
def new_chat():
    """ëŒ€í™” ê¸°ë¡(ì„¸ì…˜)ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    session.pop('chat_history', None)
    return jsonify({"status": "success", "message": "ìƒˆ ëŒ€í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤."})