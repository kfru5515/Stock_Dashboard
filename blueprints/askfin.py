import dart_fss as dart
import time

import os
import json
import traceback
import requests
import google.generativeai as genai
import FinanceDataReader as fdr
import pandas as pd
import numpy as np
import statistics
from flask import Blueprint, render_template, request, jsonify, session
from dotenv import load_dotenv
from datetime import datetime, timedelta
import concurrent.futures

from pykrx import stock
import re
from bs4 import BeautifulSoup

# --- Global Caches for Initial Loading ---
GLOBAL_KRX_LISTING = None
GLOBAL_TICKER_NAME_MAP = None 
GLOBAL_NAME_TICKER_MAP = None
ANALYSIS_CACHE = {} 
GLOBAL_SECTOR_MASTER_DF = None 
GLOBAL_STOCK_SECTOR_MAP = None 

# --- Environment Variable Loading and API Key Setup ---
load_dotenv()

askfin_bp = Blueprint('askfin', __name__, url_prefix='/askfin')

try:
    DART_API_KEY = os.getenv("DART_API_KEY")
    if not DART_API_KEY:
        raise ValueError("DART API í‚¤ê°€ .env íŒŒì¼ì— ì—†ìŠµë‹ˆë‹¤.")
    dart.set_api_key(api_key=DART_API_KEY)
    print("DART API í‚¤ê°€ ì„±ê³µì ìœ¼ë¡œ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")
except Exception as e:
    print(f"[ê²½ê³ ] DART API í‚¤ ì„¤ì • ì‹¤íŒ¨: {e}")

try:
    API_KEY = os.getenv("GOOGLE_AI_API_KEY")
    if not API_KEY: raise ValueError("API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
    genai.configure(api_key=API_KEY)
    model = genai.GenerativeModel('gemini-1.5-flash')

    PROMPT_TEMPLATE = """
You are a financial analyst. Your task is to analyze a user's query and convert it into a structured JSON object.
First, classify the query_type as "stock_analysis" or "indicator_lookup".

- "stock_analysis": For questions about stock performance under certain conditions (e.g., "most risen", "most fallen", "volatility").
- "indicator_lookup": For questions asking for a specific economic indicator's value (e.g., "CPI", "interest rate", "oil price").
- "general_inquiry": For questions asking for general financial advice, trends, recommendations, or information not directly about specific stock analysis or indicator lookup. This includes questions about market sentiment, international affairs, or general advice.

- You MUST only respond with a JSON object. No other text.
- For a "condition" involving an indicator, use a condition object.


## JSON Schema:
{{"query_type": "stock_analysis|indicator_lookup", "period": "string|null", "condition": "string|object|null", "target": "string|null", "action": "string|null"}}

## Examples:
1. User Query: "ì§€ë‚œ 3ë…„ ë™ì•ˆ ê²¨ìš¸ì— ì˜¤ë¥¸ ì½˜í…ì¸  ê´€ë ¨ ì£¼ì‹"
    JSON Output:
    ```json
    {{"query_type": "stock_analysis", "period": "ì§€ë‚œ 3ë…„", "condition": "ê²¨ìš¸", "target": "ì½˜í…ì¸  ê´€ë ¨ì£¼", "action": "ì˜¤ë¥¸ ì£¼ì‹"}}
    ```
2. User Query: "ìµœê·¼ CPI ì§€ìˆ˜ ì•Œë ¤ì¤˜"
    JSON Output:
    ```json
    {{"query_type": "indicator_lookup", "period": "ìµœê·¼", "condition": null, "target": "CPI ì§€ìˆ˜", "action": "ì¡°íšŒ"}}
    ```
3. User Query: "ì§€ë‚œ 3ë…„ ë™ì•ˆ ê²¨ìš¸ì— ì˜¤ë¥¸ ì½˜í…ì¸  ê´€ë ¨ ì£¼ì‹ì„ ë³´ì—¬ì¤˜"
    JSON Output:
    ```json
    {{"query_type": "stock_analysis", "period": "ì§€ë‚œ 3ë…„", "condition": "ê²¨ìš¸", "target": "ì½˜í…ì¸  ê´€ë ¨ì£¼", "action": "ì˜¤ë¥¸ ì£¼ì‹"}}
    ```
4. User Query: "ìµœê·¼ CPI ì§€ìˆ˜ê°€ 3.5%ë³´ë‹¤ ë†’ì•˜ì„ ë•Œ ê°€ì¥ ë§ì´ ì˜¤ë¥¸ ì£¼ì‹ì€?"
    JSON Output:
    ```json
    {{"query_type": "stock_analysis", "period": "ìµœê·¼", "condition": {{"type": "indicator", "name": "CPI", "operator": ">", "value": 3.5}}, "target": "ì£¼ì‹", "action": "ê°€ì¥ ë§ì´ ì˜¤ë¥¸ ì£¼ì‹"}}
    ```
5. User Query: "ì§€ë‚œ 1ë…„ê°„ 2ì°¨ì „ì§€ì£¼ ì¤‘ ê°€ì¥ ë§ì´ ë‚´ë¦° ì£¼ì‹ì€?"
    JSON Output:
    ```json
    {{"query_type": "stock_analysis", "period": "ì§€ë‚œ 1ë…„ê°„", "condition": null, "target": "2ì°¨ì „ì§€ì£¼", "action": "ê°€ì¥ ë§ì´ ë‚´ë¦° ì£¼ì‹"}}
    ```
6. User Query: "ìµœê·¼ ìœ í–‰í•˜ëŠ” í…Œë§ˆì£¼ ì¶”ì²œí•´ì¤„ë˜?"
    JSON Output:
    ```json
    {{"query_type": "general_inquiry", "period": "ìµœê·¼", "target": "í…Œë§ˆì£¼", "action": "ì¶”ì²œ", "recommendation_type": "ìœ í–‰"}}
    ```
7. User Query: "ìš”ì¦˜ êµ­ì œ ì •ì„¸ë¥¼ ì•Œë ¤ì¤„ë˜?"
    JSON Output:
    ```json
    {{"query_type": "general_inquiry", "period": "ìµœê·¼", "target": "êµ­ì œ ì •ì„¸", "action": "ì •ë³´ ì œê³µ"}}
    ```
8. User Query: "ì£¼ì‹ ì´ˆë³´ì¸ë° ì–´ë–¤ ì¢…ëª©ì´ ì¢‹ì•„?"
    JSON Output:
    ```json
    {{"query_type": "general_inquiry", "target": "ì£¼ì‹", "action": "ì¶”ì²œ", "recommendation_type": "ì´ˆë³´"}}
    ```
9. User Query: "CPIì§€ìˆ˜ëŠ”"
    JSON Output:
    ```json
    {{"query_type": "indicator_lookup", "period": "ìµœê·¼", "condition": null, "target": "CPI ì§€ìˆ˜", "action": "ì¡°íšŒ"}}
    ```
10. User Query: "í™˜ìœ¨"
    JSON Output:
    ```json
    {{"query_type": "indicator_lookup", "period": "ìµœê·¼", "condition": null, "target": "í™˜ìœ¨", "action": "ì¡°íšŒ"}}
    ```
11. User Query: "ê¸°ì¤€ê¸ˆë¦¬ ì–¼ë§ˆ"
    JSON Output:
    ```json
    {{"query_type": "indicator_lookup", "period": "ìµœê·¼", "condition": null, "target": "ê¸°ì¤€ê¸ˆë¦¬", "action": "ì¡°íšŒ"}}
    ```

## Task:
User Query: "{user_query}"
JSON Output:
"""
except Exception as e:
    print(f"AskFin Blueprint: ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨ - {e}")
    model = None

# --- Initial Data Loading Function ---
def initialize_global_data():
    """
    ì„œë²„ ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ í˜¸ì¶œë˜ì–´ ì „ì—­ìœ¼ë¡œ ì‚¬ìš©ë  ì£¼ì‹ ê¸°ë³¸ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ìºì‹œí•©ë‹ˆë‹¤.
    """
    global GLOBAL_KRX_LISTING, GLOBAL_TICKER_NAME_MAP, GLOBAL_NAME_TICKER_MAP

    print("[ì• í”Œë¦¬ì¼€ì´ì…˜ ì´ˆê¸°í™”] í•„ìˆ˜ ì£¼ì‹ ë°ì´í„° ë¡œë”© ì‹œì‘...")
    try:
        print("  - KOSPI ë° KOSDAQ ì¢…ëª© ëª©ë¡ (FDR) ë¡œë”© ì¤‘...")
        GLOBAL_KRX_LISTING = fdr.StockListing('KRX')
        print(f"  - ì¢…ëª© ëª©ë¡ ë¡œë”© ì™„ë£Œ. ì´ {len(GLOBAL_KRX_LISTING)}ê°œ ì¢…ëª©.")
        print(f"  - GLOBAL_KRX_LISTING ì»¬ëŸ¼: {GLOBAL_KRX_LISTING.columns.tolist()}")

        if 'Industry' in GLOBAL_KRX_LISTING.columns:
            print("\n  - FinanceDataReader 'Industry' ì»¬ëŸ¼ ê³ ìœ ê°’ (ìƒìœ„ 20ê°œ):")
            for industry in GLOBAL_KRX_LISTING['Industry'].dropna().unique().tolist()[:20]:
                print(f"    - {industry}")
            if len(GLOBAL_KRX_LISTING['Industry'].dropna().unique()) > 20:
                print("    ... (ë” ë§ì€ ì—…ì¢…ì´ ìˆìŠµë‹ˆë‹¤)")
        else:
            print("  - GLOBAL_KRX_LISTINGì— 'Industry' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")

        # 2. ì¢…ëª© ì½”ë“œ <-> ì´ë¦„ ë§¤í•‘ ë¡œë”© (pykrx)
        print("  - ì¢…ëª© ì½”ë“œ/ì´ë¦„ ë§¤í•‘ (pykrx) ë¡œë”© ì¤‘...")
        all_tickers = stock.get_market_ticker_list(market="ALL")
        GLOBAL_TICKER_NAME_MAP = {ticker: stock.get_market_ticker_name(ticker) for ticker in all_tickers}
        GLOBAL_NAME_TICKER_MAP = {name: ticker for ticker, name in GLOBAL_TICKER_NAME_MAP.items()}
        print(f"  - ì¢…ëª© ì½”ë“œ/ì´ë¦„ ë§¤í•‘ ìƒì„± ì™„ë£Œ. ì´ {len(GLOBAL_NAME_TICKER_MAP)}ê°œ ë§¤í•‘.")
        

        print("[ì• í”Œë¦¬ì¼€ì´ì…˜ ì´ˆê¸°í™”] ëª¨ë“  í•„ìˆ˜ ì£¼ì‹ ë°ì´í„° ë¡œë”© ì™„ë£Œ.")

    except Exception as e:
        print(f"[ì´ˆê¸°í™” ì˜¤ë¥˜] í•„ìˆ˜ ì£¼ì‹ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {e}")
        traceback.print_exc()


def _load_ticker_maps():
    """
    ì¢…ëª© ì •ë³´ ë§µì„ ì „ì—­ ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ë„ë¡ ë³€ê²½.
    ì´ í•¨ìˆ˜ëŠ” initialize_global_data()ê°€ í˜¸ì¶œëœ í›„ì—ë§Œ ìœ íš¨í•©ë‹ˆë‹¤.
    """
    global GLOBAL_TICKER_NAME_MAP, GLOBAL_NAME_TICKER_MAP
    if GLOBAL_NAME_TICKER_MAP is None:
        print("ê²½ê³ : _load_ticker_maps() í˜¸ì¶œ ì‹œ ê¸€ë¡œë²Œ ì¢…ëª© ë§µì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê°•ì œë¡œ ì´ˆê¸°í™” ì‹œë„.")
        initialize_global_data()

def _get_fdr_indicator(indicator_info, intent_json):
    """FinanceDataReaderë¥¼ í†µí•´ ì¼ë³„ ì§€í‘œë¥¼ ì¡°íšŒí•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ëŠ” í—¬í¼ í•¨ìˆ˜"""
    try:
        name = indicator_info['name']
        code = indicator_info['code']


        period_str = intent_json.get("period")
        req_start_date, req_end_date = parse_period(period_str)
        query_start_date = req_end_date - timedelta(days=90) 
        query_end_date = req_end_date 
        
        data = fdr.DataReader(code, query_start_date, query_end_date) 
        
        if data.empty:
            return {"error": f"{name} ë°ì´í„° ì¡°íšŒì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."}

        target_data_in_period = data.loc[data.index <= req_end_date].sort_index(ascending=True)

        if target_data_in_period.empty:
             return {"error": f"{name} ì§€í‘œì— ëŒ€í•´ ìš”ì²­í•˜ì‹  ê¸°ê°„({req_end_date.strftime('%Yë…„ %mì›” %dì¼')})ì˜ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}

        latest_for_period = target_data_in_period['Close'].iloc[-1]
        latest_date_for_period = target_data_in_period.index[-1]

        previous_data_in_period = data.loc[data.index < latest_date_for_period].sort_index(ascending=True)
        previous_for_period = None
        if not previous_data_in_period.empty:
            previous_for_period = previous_data_in_period['Close'].iloc[-1]

        if previous_for_period is not None:
            change = latest_for_period - previous_for_period
            change_str = f"{abs(change):.2f} ìƒìŠ¹" if change > 0 else f"{abs(change):.2ff} í•˜ë½" if change < 0 else "ë³€ë™ ì—†ìŒ"
            result_sentence = f"ìš”ì²­í•˜ì‹  ê¸°ê°„ì˜ ë§ˆì§€ë§‰({latest_date_for_period.strftime('%Yë…„ %mì›” %dì¼')}) {name}ëŠ”(ì€) {latest_for_period:,.2f}ì´ë©°, ì§ì „ ì˜ì—…ì¼ ëŒ€ë¹„ {change_str}í–ˆìŠµë‹ˆë‹¤."
        else:
            result_sentence = f"ìš”ì²­í•˜ì‹  ê¸°ê°„ì˜ ë§ˆì§€ë§‰({latest_date_for_period.strftime('%Yë…„ %mì›” %dì¼')}) {name}ëŠ”(ì€) {latest_for_period:,.2f}ì…ë‹ˆë‹¤. ì§ì „ ì˜ì—…ì¼ ë°ì´í„°ê°€ ì—†ì–´ ë³€ë™ ì •ë³´ë¥¼ ì œê³µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        return {
            "query_intent": intent_json,
            "analysis_subject": name,
            "result": [result_sentence]
        }
    except Exception as e:
        return {"error": f"{indicator_info.get('name', 'ì•Œìˆ˜ì—†ëŠ”')} ì§€í‘œ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"}

def _get_bok_indicator(indicator_info, intent_json):
    """í•œêµ­ì€í–‰(BOK) APIë¥¼ í†µí•´ ì›”ë³„ ì§€í‘œë¥¼ ì¡°íšŒí•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ëŠ” í—¬í¼ í•¨ìˆ˜"""
    try:
        name = indicator_info['name']
        bok_api_key = os.getenv("ECOS_API_KEY")
        if not bok_api_key: return {"error": "í•œêµ­ì€í–‰ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}

        end_date = datetime.now().strftime('%Y%m')
        start_date = (datetime.now() - timedelta(days=120)).strftime('%Y%m')
        url = (f"https://ecos.bok.or.kr/api/StatisticSearch/{bok_api_key}/json/kr/1/10/"
               f"{indicator_info['stats_code']}/MM/{start_date}/{end_date}/{indicator_info['item_code']}")

        response = requests.get(url, timeout=10).json()
        rows = response.get("StatisticSearch", {}).get("row", [])
        
        if len(rows) < 2:
            return {"error": f"ìµœê·¼ {name} ë°ì´í„°ë¥¼ ë¹„êµí•  ë§Œí¼ ì¶©ë¶„íˆ ì¡°íšŒí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
            
        latest = rows[-1]
        previous = rows[-2]
        latest_date = f"{latest['TIME'][:4]}ë…„ {latest['TIME'][4:]}ì›”"
        change = float(latest['DATA_VALUE']) - float(previous['DATA_VALUE'])
        change_str = f"{abs(change):.2f} ìƒìŠ¹" if change > 0 else f"{abs(change):.2f} í•˜ë½" if change < 0 else "ë³€ë™ ì—†ìŒ"

        result_sentence = (f"ê°€ì¥ ìµœê·¼({latest_date}) {name}ëŠ”(ì€) {latest['DATA_VALUE']}ì´ë©°, ì „ì›” ëŒ€ë¹„ {change_str}í–ˆìŠµë‹ˆë‹¤.")
        
        return {
            "query_intent": intent_json,
            "analysis_subject": name,
            "result": [result_sentence]
        }
    except Exception as e:
        return {"error": f"í•œêµ­ì€í–‰(BOK) ì§€í‘œ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"}


def execute_indicator_lookup(intent_json):
    """
    [ìµœì¢… ìˆ˜ì •] ì—¬ëŸ¬ ì†ŒìŠ¤ì˜ ê²½ì œ ì§€í‘œë¥¼ ì¡°íšŒí•˜ëŠ” ë©”ì¸ í•¨ìˆ˜
    """
    target = intent_json.get("target", "")

    FDR_INDICATOR_MAP = {
        "í™˜ìœ¨": {"code": "USD/KRW", "name": "ì›/ë‹¬ëŸ¬ í™˜ìœ¨"},
        "ìœ ê°€": {"code": "WTI", "name": "WTI êµ­ì œ ìœ ê°€"},
        "ê¸ˆê°’": {"code": "GC", "name": "ê¸ˆ ì„ ë¬¼"},
        "ë¯¸êµ­ì±„10ë…„": {"code": "US10YT", "name": "ë¯¸ 10ë…„ë¬¼ êµ­ì±„ ê¸ˆë¦¬"},
        "ì½”ìŠ¤í”¼": {"code": "KS11", "name": "ì½”ìŠ¤í”¼ ì§€ìˆ˜"},
        "ì½”ìŠ¤ë‹¥": {"code": "KQ11", "name": "ì½”ìŠ¤ë‹¥ ì§€ìˆ˜"},
    }
    
    for key, value in FDR_INDICATOR_MAP.items():
        if key in target or value['name'] in target:
            return _get_fdr_indicator(value, intent_json)
            
    BOK_INDICATOR_MAP = {
        "CPI": {"stats_code": "901Y001", "item_code": "0", "name": "ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜"},
        "ê¸°ì¤€ê¸ˆë¦¬": {"stats_code": "722Y001", "item_code": "0001000", "name": "í•œêµ­ ê¸°ì¤€ê¸ˆë¦¬"},
    }
    
    for key, value in BOK_INDICATOR_MAP.items():
        if key in target or value['name'] in target:
            return _get_bok_indicator(value, intent_json)

    return {
        "query_intent": intent_json,
        "analysis_subject": "ì•Œ ìˆ˜ ì—†ëŠ” ì§€í‘œ",
        "result": [f"'{target}' ì§€í‘œëŠ” ì•„ì§ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."]
    }

@askfin_bp.route('/stock/<code>/profile')
def get_stock_profile(code):
    """
    [ìˆ˜ì •] DART APIë¥¼ ì‚¬ìš©í•˜ì—¬ 'ì£¼ìš” ê³µì‹œ' ëª©ë¡ì„ ì¡°íšŒí•˜ë„ë¡ ë³€ê²½í•©ë‹ˆë‹¤.
    """
    response_data = {}
    company_name = None

    try:
        profile_data = {}
        latest_business_day = stock.get_nearest_business_day_in_a_week()

        if GLOBAL_KRX_LISTING is None:
            initialize_global_data()
        
        krx_list = GLOBAL_KRX_LISTING
        target_info = krx_list[krx_list['Code'] == code]
        if target_info.empty:
            return jsonify({"error": f"ì¢…ëª©ì½”ë“œ '{code}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}), 404
        
        target_info = target_info.iloc[0]
        market = target_info.get('Market', 'KOSPI')
        sector = target_info.get('Sector')
        company_name = target_info.get('Name', 'N/A')
        
        profile_data['ê¸°ì—…ëª…'] = company_name
        profile_data['ì—…ì¢…'] = sector
        profile_data['ì£¼ìš”ì œí’ˆ'] = target_info.get('Industry', 'N/A')

        df_ohlcv = stock.get_market_ohlcv(latest_business_day, market=market)
        df_cap = stock.get_market_cap(latest_business_day, market=market)
        df_funda = stock.get_market_fundamental(latest_business_day, market=market)

        current_price = df_ohlcv.loc[code, 'ì¢…ê°€']
        market_cap = df_cap.loc[code, 'ì‹œê°€ì´ì•¡']
        funda = df_funda.loc[code]

        profile_data['í˜„ì¬ê°€'] = f"{current_price:,} ì›"
        profile_data['ì‹œê°€ì´ì•¡'] = f"{market_cap / 1_0000_0000_0000:.2f} ì¡°ì›" if market_cap > 1_0000_0000_0000 else f"{market_cap / 1_0000_0000:.2f} ì–µì›"
        
        eps = funda.get('EPS', 0)
        profile_data['PER'] = f"{funda.get('PER', 0):.2f} ë°°"
        profile_data['PBR'] = f"{funda.get('PBR', 0):.2f} ë°°"
        profile_data['ë°°ë‹¹ìˆ˜ìµë¥ '] = f"{funda.get('DIV', 0):.2f} %"
        
        response_data["company_profile"] = profile_data

    except Exception as e:
        traceback.print_exc()
        response_data["profile_error"] = f"ê¸°ì—… ê°œìš” ì •ë³´ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"


    if company_name:
        try:
            corp_list = dart.get_corp_list()
            
            # --- START OF MODIFICATION ---
            # Search for the corporation by name
            found_corps = corp_list.find_by_corp_name(company_name, exactly=True)
            
            corp = None
            if found_corps: # Check if the list is not empty
                corp = found_corps[0] # Assign the first found corporation
            
            if not corp: # Explicitly check if corp is None after assignment
                raise ValueError(f"DARTì—ì„œ '{company_name}'ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            # --- END OF MODIFICATION ---

            reports = corp.search_filings(bgn_de=(datetime.now() - timedelta(days=365)).strftime('%Y%m%d'), last_reprt_at='Y')
            response_data["report_list"] = [{
                'report_nm': r.report_nm, 'flr_nm': r.flr_nm, 'rcept_dt': r.rcept_dt,
                'url': f"http://dart.fss.or.kr/dsaf001/main.do?rcpNo={r.rcept_no}"
            } for r in reports[:15]] if reports else []

            api_url = "https://opendart.fss.or.kr/api/fnlttSinglAcnt.json"
            fs_data_list = [] 
            
            current_year = datetime.now().year
            years_to_fetch = [str(year) for year in range(current_year, current_year - 4, -1)]

            for year in years_to_fetch:
                for reprt_code in ['11011', '11013', '11012']:
                    params = {
                        'crtfc_key': DART_API_KEY,
                        'corp_code': corp.corp_code, # 'corp' is now guaranteed to be defined here
                        'bsns_year': str(year),
                        'reprt_code': reprt_code,
                        'fs_div': 'CFS'
                    }
                    res = requests.get(api_url, params=params)
                    data = res.json()

                    if data.get('status') == '000' and data.get('list'):
                        for item in data['list']:
                            if 'thstrm_end_dt' in item and item['thstrm_end_dt']:
                                item['report_date_key'] = item['thstrm_end_dt']
                            elif 'thstrm_dt' in item and item['thstrm_dt']:
                                date_range_str = item['thstrm_dt']
                                match = re.search(r'(\d{4}\.\d{2}\.\d{2})$', date_range_str.strip())
                                if match:
                                    item['report_date_key'] = match.group(1)
                                else:
                                    item['report_date_key'] = date_range_str.strip()
                            else:
                                item['report_date_key'] = None
                        fs_data_list.extend(data['list'])
                        break
                    elif params['fs_div'] == 'CFS':
                        params['fs_div'] = 'OFS'
                        res = requests.get(api_url, params=params)
                        data = res.json()
                        if data.get('status') == '000' and data.get('list'):
                             for item in data['list']:
                                if 'thstrm_end_dt' in item and item['thstrm_end_dt']:
                                    item['report_date_key'] = item['thstrm_end_dt']
                                elif 'thstrm_dt' in item and item['thstrm_dt']:
                                    date_range_str = item['thstrm_dt']
                                    match = re.search(r'(\d{4}\.\d{2}\.\d{2})$', date_range_str.strip())
                                    if match:
                                        item['report_date_key'] = match.group(1)
                                    else:
                                        item['report_date_key'] = date_range_str.strip()
                                else:
                                    item['report_date_key'] = None
                             fs_data_list.extend(data['list'])
                             break

            if fs_data_list:
                df = pd.DataFrame(fs_data_list)
                
                df = df.drop_duplicates(subset=['rcept_no', 'account_nm'], keep='last')
                
                df['thstrm_amount'] = df['thstrm_amount'].astype(str).str.replace(',', '', regex=False).str.strip()
                df['thstrm_amount'] = pd.to_numeric(df['thstrm_amount'], errors='coerce')
                df.dropna(subset=['thstrm_amount'], inplace=True)
                
                def extract_and_clean_date(date_string): # This helper function is now defined here again or needs to be outside if used multiple places
                    if not isinstance(date_string, str): # Add this check for safety
                        return None
                    match = re.search(r'(\d{4}\.\d{2}\.\d{2})$', date_string.strip())
                    if match:
                        return match.group(1)
                    parts = date_string.split('~')
                    if len(parts) > 1:
                        match = re.search(r'(\d{4}\.\d{2}\.\d{2})$', parts[-1].strip())
                        if match:
                            return match.group(1)
                    return None
                
                # Use the report_date_key if it was set, otherwise apply the cleaning to thstrm_dt
                if 'report_date_key' not in df.columns: # Fallback if direct assignment didn't happen for all
                    df['report_date_key'] = df['thstrm_dt'].astype(str).apply(extract_and_clean_date)


                df['report_date_key'] = pd.to_datetime(df['report_date_key'], format='%Y.%m.%d', errors='coerce')
                df.dropna(subset=['report_date_key'], inplace=True)
                
                df_pivot = df.pivot_table(index='account_nm', columns='report_date_key', values='thstrm_amount', aggfunc='first')
                df_pivot = df_pivot.fillna(0)

                df_pivot = df_pivot.sort_index(axis=1) 
                
                display_columns = df_pivot.columns[-4:] if len(df_pivot.columns) >= 4 else df_pivot.columns
                
                response_data["key_financial_info"] = json.dumps(
                    {
                        "columns": [col.strftime('%Y.%m') for col in display_columns],
                        "index": list(df_pivot.index),
                        "data": df_pivot[display_columns].values.tolist()
                    }, ensure_ascii=False
                )
                print("2024ë…„ë„ ì£¼ìš” ì¬ë¬´ì •ë³´ API í˜¸ì¶œ ì„±ê³µ ë° ì²˜ë¦¬ ì™„ë£Œ")
            else:
                response_data["financials_error"] = "ì£¼ìš” ì¬ë¬´ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. DART APIì—ì„œ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜ íŒŒì‹± ì˜¤ë¥˜."

        except Exception as e:
            print(f"ì¬ë¬´ì œí‘œ ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì¹˜ëª…ì ì¸ ì˜¤ë¥˜ ë°œìƒ: {e}")
            traceback.print_exc()
            response_data["financials_error"] = f"ì¬ë¬´ì œí‘œë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}"
    
    return jsonify(response_data)

def get_target_stocks(target_str):
    """
    [ìˆ˜ì •ë¨] íƒ€ê²Ÿ ë¬¸ìì—´ì— í•´ë‹¹í•˜ëŠ” ì¢…ëª© ë¦¬ìŠ¤íŠ¸(DataFrame)ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜ (ìºì‹œëœ ë°ì´í„° ì‚¬ìš©)
    """
    global GLOBAL_KRX_LISTING, GLOBAL_NAME_TICKER_MAP

    if GLOBAL_KRX_LISTING is None:
        print("ê²½ê³ : get_target_stocks() í˜¸ì¶œ ì‹œ GLOBAL_KRX_LISTINGì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê°•ì œë¡œ ì´ˆê¸°í™” ì‹œë„.")
        initialize_global_data()
        if GLOBAL_KRX_LISTING is None:
            return pd.DataFrame(columns=['Name', 'Code']), "ì´ˆê¸°í™” ì‹¤íŒ¨"

    krx = GLOBAL_KRX_LISTING 

    GENERIC_TARGETS = {"ì£¼ì‹", "ì¢…ëª©", "ê¸‰ë“±ì£¼", "ìš°ëŸ‰ì£¼", "ì¸ê¸°ì£¼", "ì „ì²´"}
    
    analysis_subject = "ì‹œì¥ ì „ì²´"
    target_stocks = krx 

    import os 

    if target_str and target_str.strip() and target_str not in GENERIC_TARGETS:
        analysis_subject = f"'{target_str}'"
        
        keyword = target_str.replace(" ê´€ë ¨ì£¼", "").replace(" í…Œë§ˆì£¼", "").replace(" í…Œë§ˆ", "").replace("ì£¼", "").strip()
        lower_keyword = keyword.lower()

        print(f"--- ë””ë²„ê·¸ ì‹œì‘ (get_target_stocks) ---")
        print(f"ë””ë²„ê·¸: ì‚¬ìš©ì ì…ë ¥ í‚¤ì›Œë“œ: '{keyword}' (ì†Œë¬¸ì: '{lower_keyword}')")

        themes_from_file = {}
        try:
            themes_file_path = os.path.join(os.path.dirname(__file__), '..', 'cache', 'themes.json')

            print(f"ë””ë²„ê·¸: themes.jsonì„ ì°¾ì„ ê²½ë¡œ: {themes_file_path}") # ë””ë²„ê·¸ ì¶œë ¥ ì¶”ê°€

            with open(themes_file_path, 'r', encoding='utf-8') as f:
                themes_from_file = json.load(f)
            print(f"ë””ë²„ê·¸: 'themes.json' íŒŒì¼ ë¡œë“œ ì„±ê³µ. ì´ {len(themes_from_file)}ê°œ í…Œë§ˆ.")
            print(f"ë””ë²„ê·¸: themes.json í‚¤ ëª©ë¡ (ìƒìœ„ 5ê°œ): {list(themes_from_file.keys())[:5]}...")
        except FileNotFoundError:
            print("ê²½ê³ : 'themes.json' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        except Exception as e:
            print(f"ê²½ê³ : 'themes.json' íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

        found_by_theme_file = False
        target_codes_from_theme = []

        for theme_name_in_file, stock_list_in_file in themes_from_file.items():
            print(f"ë””ë²„ê·¸: '{lower_keyword}' vs '{theme_name_in_file.lower()}' ë§¤ì¹­ ì‹œë„...")
            
            if (lower_keyword == theme_name_in_file.lower() or 
                lower_keyword in theme_name_in_file.lower() or 
                theme_name_in_file.lower() in lower_keyword): 
                
                print(f"ë””ë²„ê·¸: themes.jsonì—ì„œ í…Œë§ˆ '{theme_name_in_file}'ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
                for stock_info in stock_list_in_file:
                    if isinstance(stock_info, dict) and 'code' in stock_info:
                        target_codes_from_theme.append(stock_info['code'])
                    elif isinstance(stock_info, str) and len(stock_info) == 6 and stock_info.isdigit(): # ì½”ë“œê°€ ë¬¸ìì—´ë¡œ ì§ì ‘ ì €ì¥ëœ ê²½ìš°
                        target_codes_from_theme.append(stock_info)
                analysis_subject = f"'{theme_name_in_file}' í…Œë§ˆ"
                found_by_theme_file = True
                break
        
        print(f"ë””ë²„ê·¸: themes.jsonì—ì„œ ì¶”ì¶œëœ ì¢…ëª© ì½”ë“œ ìˆ˜: {len(target_codes_from_theme)}")
        if len(target_codes_from_theme) > 0:
            print(f"ë””ë²„ê·¸: ì¶”ì¶œëœ ì²« 5ê°œ ì¢…ëª© ì½”ë“œ: {target_codes_from_theme[:5]}")

        if found_by_theme_file:

            print(f"ë””ë²„ê·¸: GLOBAL_KRX_LISTINGì˜ ì²« 5ê°œ í–‰:\n{krx.head()}")

            codes_in_krx_check = krx[krx['Code'].isin(target_codes_from_theme)]
            print(f"ë””ë²„ê·¸: GLOBAL_KRX_LISTINGì— ì¡´ì¬í•˜ëŠ” í…Œë§ˆ ì¢…ëª© ì½”ë“œ ìˆ˜: {len(codes_in_krx_check)}")
            if len(codes_in_krx_check) == 0 and len(target_codes_from_theme) > 0:
                print("ë””ë²„ê·¸: ê²½ê³ ! themes.jsonì˜ ì¢…ëª© ì½”ë“œ ì¤‘ GLOBAL_KRX_LISTINGì— ë§¤ì¹­ë˜ëŠ” ê²ƒì´ ì—†ìŠµë‹ˆë‹¤. ì½”ë“œ í˜•ì‹ ë¶ˆì¼ì¹˜ ê°€ëŠ¥ì„±.")
                if target_codes_from_theme:
                    print(f"ë””ë²„ê·¸: themes.json ì²« ì¢…ëª© ì½”ë“œ: '{target_codes_from_theme[0]}'")
                if not krx.empty:
                    print(f"ë””ë²„ê·¸: GLOBAL_KRX_LISTING ì²« ì¢…ëª© ì½”ë“œ: '{krx.iloc[0]['Code']}'")


            target_stocks = krx[krx['Code'].isin(target_codes_from_theme)]
            print(f"ë””ë²„ê·¸: ìµœì¢… í•„í„°ë§ëœ target_stocks ê°œìˆ˜: {len(target_stocks)}")
        
        else:
            # 2. ê¸°ì¡´ FinanceDataReader 'Industry' ì»¬ëŸ¼ (í˜¹ì‹œ ì¡´ì¬í•œë‹¤ë©´)ì„ í†µí•œ ê²€ìƒ‰ (í…Œë§ˆ íŒŒì¼ ì—†ì„ ë•Œì˜ í´ë°±)
            # í˜„ì¬ ë¡œê·¸ì— 'Industry' ì»¬ëŸ¼ì´ ì—†ë‹¤ê³  ë‚˜ì™”ì§€ë§Œ, ë¯¸ë˜ì— ì¶”ê°€ë  ê°€ëŠ¥ì„±ì„ ê³ ë ¤í•˜ì—¬ ë¡œì§ì€ ìœ ì§€í•˜ë˜,
            # ì‹¤ì œ ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ê±´ë„ˆë›°ë„ë¡ ì¡°ê±´ë¬¸ ì¶”ê°€
            found_by_industry = False
            if 'Industry' in krx.columns:
                INDUSTRY_KEYWORD_MAP = {
                    "ì œì•½": ["ì˜ì•½í’ˆ ì œì¡°ì—…", "ì˜ë£Œìš© ë¬¼ì§ˆ ë° ì˜ì•½í’ˆ ì œì¡°ì—…", "ìƒë¬¼í•™ì  ì œì œ ì œì¡°ì—…"], 
                    "ë°˜ë„ì²´": ["ë°˜ë„ì²´ ì œì¡°ì—…", "ì „ìë¶€í’ˆ ì œì¡°ì—…", "ë°˜ë„ì²´ ë° í‰íŒë””ìŠ¤í”Œë ˆì´ ì œì¡°ì—…"],
                    "ìë™ì°¨": ["ìë™ì°¨ìš© ì—”ì§„ ë° ìë™ì°¨ ì œì¡°ì—…", "ìë™ì°¨ ë¶€í’ˆ ì œì¡°ì—…"],
                    "IT": ["ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œ ë° ê³µê¸‰ì—…", "ì»´í“¨í„° í”„ë¡œê·¸ë˜ë°, ì‹œìŠ¤í…œ í†µí•© ë° ê´€ë¦¬ì—…", "ì •ë³´ì„œë¹„ìŠ¤ì—…"],
                    "ì€í–‰": ["ì€í–‰"],
                    "ì¦ê¶Œ": ["ì¦ê¶Œ ë° ì„ ë¬¼ ì¤‘ê°œì—…"],
                    "ë³´í—˜": ["ë³´í—˜ ë° ì—°ê¸ˆì—…"],
                    "ê±´ì„¤": ["ì¢…í•© ê±´ì„¤ì—…", "ê±´ë¬¼ ê±´ì„¤ì—…", "í† ëª© ê±´ì„¤ì—…"],
                    "í™”í•™": ["í™”í•™ë¬¼ì§ˆ ë° í™”í•™ì œí’ˆ ì œì¡°ì—…", "ê³ ë¬´ ë° í”Œë¼ìŠ¤í‹±ì œí’ˆ ì œì¡°ì—…"],
                    "ì½˜í…ì¸ ": ["ì˜í™”, ë¹„ë””ì˜¤ë¬¼, ë°©ì†¡í”„ë¡œê·¸ë¨ ì œì‘ ë° ë°°ê¸‰ì—…", "ìŒì•… ë° ê¸°íƒ€ ì—”í„°í…Œì¸ë¨¼íŠ¸ì—…", "ì¶œíŒì—…"], 
                    "ê²Œì„": ["ê²Œì„ ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œ ë° ê³µê¸‰ì—…", "ë°ì´í„°ë² ì´ìŠ¤ ë° ì˜¨ë¼ì¸ ì •ë³´ ì œê³µì—…"],
                    "ì² ê°•": ["1ì°¨ ì² ê°• ì œì¡°ì—…", "ê¸ˆì† ê°€ê³µì œí’ˆ ì œì¡°ì—…"],
                    "ì¡°ì„ ": ["ì„ ë°• ë° ë³´íŠ¸ ê±´ì¡°ì—…"],
                    "í•´ìš´": ["í•´ìƒ ìš´ì†¡ì—…"],
                    "í•­ê³µ": ["í•­ê³µ ìš´ì†¡ì—…"],
                    "ë°©ì‚°": ["í•­ê³µê¸°, ìš°ì£¼ì„  ë° ë³´ì¡°ì¥ë¹„ ì œì¡°ì—…"],
                    "ìŒì‹ë£Œ": ["ì‹ë£Œí’ˆ ì œì¡°ì—…", "ìŒë£Œ ì œì¡°ì—…", "ë‹´ë°° ì œì¡°ì—…"],
                    "ìœ í†µ": ["ì¢…í•© ì†Œë§¤ì—…", "ì „ë¬¸ ì†Œë§¤ì—…", "ë¬´ì í¬ ì†Œë§¤ì—…"],
                    # ... FinanceDataReaderì˜ 'Industry' ê³ ìœ ê°’ì„ ì°¸ê³ í•˜ì—¬ ì¶”ê°€/ìˆ˜ì •
                }
                for industry_key, industry_names in INDUSTRY_KEYWORD_MAP.items():
                    if lower_keyword == industry_key.lower() or any(name.lower() in lower_keyword for name in industry_names):
                        print(f"ë””ë²„ê·¸: ì—…ì¢… '{industry_key}'ì— í•´ë‹¹í•˜ëŠ” ì¢…ëª©ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤.")
                        target_stocks = krx[krx['Industry'].isin(industry_names)]
                        analysis_subject = f"'{industry_key}' ì—…ì¢…"
                        found_by_industry = True
                        break
            
            if not found_by_industry:
                # 3. Fallback to name-based search (ê°€ì¥ ë§ˆì§€ë§‰ ìˆœìœ„)
                print(f"ë””ë²„ê·¸: ì¢…ëª©ëª…ì— '{keyword}' í‚¤ì›Œë“œê°€ í¬í•¨ëœ ì¢…ëª©ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤. (ìµœì¢… í´ë°±)")
                target_stocks = krx[krx['Name'].str.contains(keyword, na=False)]
    
    elif target_str in GENERIC_TARGETS:
        analysis_subject = "ì‹œì¥ ì „ì²´"
    
    print(f"--- ë””ë²„ê·¸ ì¢…ë£Œ (get_target_stocks) ---")
    return target_stocks, analysis_subject




def parse_period(period_str):
    """'ì§€ë‚œ 3ë…„ê°„' ê°™ì€ ë¬¸ìì—´ì„ ì‹œì‘ì¼ê³¼ ì¢…ë£Œì¼ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜"""
    today = datetime.now()
    if not period_str:
        return today - timedelta(days=365), today
    try:
        if "ë…„ê°„" in period_str:
            years = int(period_str.replace("ì§€ë‚œ", "").replace("ë…„ê°„", "").strip())
            return today - timedelta(days=365 * years), today
        elif "ê°œì›”" in period_str:
            months = int(period_str.replace("ì§€ë‚œ", "").replace("ê°œì›”", "").strip())
            return today - timedelta(days=30 * months), today
        elif "ì‘ë…„" in period_str:
            last_year = today.year - 1
            return datetime(last_year, 1, 1), datetime(last_year, 12, 31)
        elif "ì˜¬í•´" in period_str:
            return datetime(today.year, 1, 1), today
    except (ValueError, TypeError):
        pass

    return today - timedelta(days=365), today 


def get_interest_rate_hike_dates(api_key):
    """í•œêµ­ì€í–‰ APIë¡œ ê¸°ì¤€ê¸ˆë¦¬ ì¸ìƒì¼ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜."""
    stats_code, item_code = "722Y001", "0001000"
    end_date = datetime.now().strftime('%Y%m%d')
    start_date = (datetime.now() - timedelta(days=5*365)).strftime('%Y%m%d')
    url = f"https://ecos.bok.or.kr/api/StatisticSearch/{api_key}/json/kr/1/1000/{stats_code}/DD/{start_date}/{end_date}/{item_code}"
    
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        data = response.json()
        
        if "StatisticSearch" not in data or "row" not in data.get("StatisticSearch", {}):
            return []

        df = pd.DataFrame(data["StatisticSearch"]["row"])
        df['TIME'] = pd.to_datetime(df['TIME'], format='%Y%m%d')
        df['DATA_VALUE'] = pd.to_numeric(df['DATA_VALUE'])
        df = df.sort_values(by='TIME').reset_index(drop=True)
        df['PREV_VALUE'] = df['DATA_VALUE'].shift(1)
        
        hike_dates = df[df['DATA_VALUE'] > df['PREV_VALUE']]['TIME'].tolist()
        return hike_dates
    except Exception as e:
        print(f"í•œêµ­ì€í–‰ API ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        return []
    

def analyze_target_price_upside(target_stocks):
    """
    [ìµœì í™”] ë„¤ì´ë²„ ì¦ê¶Œ ì»¨ì„¼ì„œìŠ¤ í˜ì´ì§€ë¥¼ ì¼ê´„ ìŠ¤í¬ë ˆì´í•‘í•˜ì—¬ ëª©í‘œì£¼ê°€ ê´´ë¦¬ìœ¨ì„ ë¶„ì„í•©ë‹ˆë‹¤.
    """
    print("ëª©í‘œì£¼ê°€ ì»¨ì„¼ì„œìŠ¤ ë°ì´í„° ì¼ê´„ ì¡°íšŒ ì‹œì‘...")
    try:
        url = "https://finance.naver.com/sise/consensus.naver?&target=up"
        headers = {'User-Agent': 'Mozilla/5.0'}
        
        df_list = pd.read_html(requests.get(url, headers=headers, timeout=10).text)
        df = df_list[1]
        
        df = df.dropna(axis='index', how='all')
        df.columns = ['ì¢…ëª©ëª…', 'ëª©í‘œì£¼ê°€', 'íˆ¬ìì˜ê²¬', 'í˜„ì¬ê°€', 'ê´´ë¦¬ìœ¨', 'ì¦ê¶Œì‚¬', 'ì‘ì„±ì¼']
        df = df[df['ì¢…ëª©ëª…'].notna()]

        df['ëª©í‘œì£¼ê°€'] = pd.to_numeric(df['ëª©í‘œì£¼ê°€'], errors='coerce')
        df['í˜„ì¬ê°€'] = pd.to_numeric(df['í˜„ì¬ê°€'], errors='coerce')
        df['ê´´ë¦¬ìœ¨'] = df['ê´´ë¦¬ìœ¨'].str.strip('%').astype(float)
        df = df.dropna(subset=['ëª©í‘œì£¼ê°€', 'í˜„ì¬ê°€', 'ê´´ë¦¬ìœ¨'])

        if GLOBAL_KRX_LISTING is None:
            initialize_global_data()
        krx_list = GLOBAL_KRX_LISTING[['Name', 'Code']]
        df = pd.merge(df, krx_list, left_on='ì¢…ëª©ëª…', right_on='Name', how='inner')
        
        print("ë°ì´í„° ì¡°íšŒ ë° ê°€ê³µ ì™„ë£Œ.")
        
        analysis_results = []
        for index, row in df.iterrows():
            analysis_results.append({
                "code": row['Code'],
                "name": row['ì¢…ëª©ëª…'],
                "value": row['ê´´ë¦¬ìœ¨'],
                "label": "ëª©í‘œì£¼ê°€ ê´´ë¦¬ìœ¨(%)",
                "start_price": int(row['í˜„ì¬ê°€']),
                "end_price": int(row['ëª©í‘œì£¼ê°€'])
            })
            
        return analysis_results

    except Exception as e:
        print(f"ëª©í‘œì£¼ê°€ ì»¨ì„¼ì„œìŠ¤ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return []
    
def execute_stock_analysis(intent_json, page, user_query, cache_key=None):
    """
    [ìµœì¢… ì™„ì„±] ìºì‹±, í˜ì´ì§€ë„¤ì´ì…˜, ë‹¤ì¤‘ ë¶„ì„, í´ë°±, ì„¤ëª… ê¸°ëŠ¥ì„ ëª¨ë‘ í¬í•¨í•œ ì£¼ì‹ ë¶„ì„ ì‹¤í–‰ í•¨ìˆ˜.
    """
    try:
        action_str = intent_json.get("action", "")

        supported_actions = ["ì˜¤ë¥¸", "ë‚´ë¦°", "ë³€ë™ì„±", "ë³€ë™", "ëª©í‘œì£¼ê°€"]
        # Removed the fallback block as per previous request

        if cache_key and cache_key in ANALYSIS_CACHE and 'full_result' in ANALYSIS_CACHE[cache_key]:
            sorted_result = ANALYSIS_CACHE[cache_key]['full_result']
            analysis_subject = ANALYSIS_CACHE[cache_key]['analysis_subject']
            print(f"âœ… CACHE HIT: ìºì‹œëœ ì „ì²´ ê²°ê³¼ {len(sorted_result)}ê°œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        else:
            print(f"ğŸ”¥ CACHE MISS: ìƒˆë¡œìš´ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
            target_str = intent_json.get("target")
            condition_str = intent_json.get("condition")
            target_stocks, analysis_subject = get_target_stocks(target_str) # get_target_stocksëŠ” ì´ì œ ìºì‹œëœ GLOBAL_KRX_LISTING ì‚¬ìš©
            if target_stocks.empty: return {"result": [f"{analysis_subject}ì— í•´ë‹¹í•˜ëŠ” ì¢…ëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."]}

            start_date, end_date = parse_period(intent_json.get("period"))
            
            event_periods = []
            if isinstance(condition_str, str) and any(s in condition_str for s in ["ì—¬ë¦„", "ê²¨ìš¸"]):
                season = "ì—¬ë¦„" if "ì—¬ë¦„" in condition_str else "ê²¨ìš¸"
                event_periods = handle_season_condition((start_date, end_date), season)
            elif isinstance(condition_str, dict) and condition_str.get("type") == "indicator":
                # Handle indicator-based conditions, e.g., "CPI ì§€ìˆ˜ê°€ 3.5%ë³´ë‹¤ ë†’ì•˜ì„ ë•Œ"
                event_periods = handle_indicator_condition(condition_str, (start_date, end_date))
            else:
                event_periods = [(start_date, end_date)]

            result_data = []
            if "ì˜¤ë¥¸" in action_str or "ë‚´ë¦°" in action_str:
                result_data = analyze_top_performers(target_stocks, event_periods, (start_date, end_date))
            elif "ë³€ë™ì„±" in action_str or "ë³€ë™" in action_str:
                result_data = analyze_volatility(target_stocks, (start_date, end_date))
            elif "ëª©í‘œì£¼ê°€" in action_str:
                result_data = analyze_target_price_upside(target_stocks)

            reverse_sort = False if "ë‚´ë¦°" in action_str else True
            sorted_result = sorted(result_data, key=lambda x: x.get('value', -999), reverse=reverse_sort)
            
            if not cache_key: cache_key = str(hash(json.dumps(intent_json, sort_keys=True)))
            ANALYSIS_CACHE[cache_key] = {
                'intent_json': intent_json, 'analysis_subject': analysis_subject, 'full_result': sorted_result
            }
            print(f"ìƒˆë¡œìš´ ë¶„ì„ ê²°ê³¼ {len(sorted_result)}ê°œë¥¼ ìºì‹œì— ì €ì¥í–ˆìŠµë‹ˆë‹¤. (í‚¤: {cache_key})")

        items_per_page = 20
        total_items = len(sorted_result)
        total_pages = (total_items + items_per_page - 1) // items_per_page
        start_index = (page - 1) * items_per_page
        end_index = start_index + items_per_page
        paginated_result = sorted_result[start_index:end_index]
        
        condition_str = intent_json.get("condition")
        description = ""
        if isinstance(condition_str, str):
            if "ì—¬ë¦„" in condition_str:
                description = "ì—¬ë¦„(6ì›”1ì¼~8ì›” 31ì¼) ê¸°ê°„ì˜ í‰ê·  ìˆ˜ìµë¥ ì„ ë¶„ì„í•œ ê²°ê³¼ì…ë‹ˆë‹¤. \n í˜„ì¬ ë‚˜ì˜¤ëŠ” ê³¼ê±°ê°€ê²©ê³¼ í˜„ì¬ê°€ê²©ì˜ ìˆ˜ìµë¥ ì´ ì•„ë‹™ë‹ˆë‹¤."
            elif "ê²¨ìš¸" in condition_str:
                description = "ê²¨ìš¸(12ì›”1ì¼~3ì›”1) ê¸°ê°„ì˜ í‰ê·  ìˆ˜ìµë¥ ì„ ë¶„ì„í•œ ê²°ê³¼ì…ë‹ˆë‹¤. \n í˜„ì¬ ë‚˜ì˜¤ëŠ” ê³¼ê±°ê°€ê²©ê³¼ í˜„ì¬ê°€ê²©ì˜ ìˆ˜ìµë¥ ì´ ì•„ë‹™ë‹ˆë‹¤."
        elif isinstance(condition_str, dict) and condition_str.get("type") == "indicator":
            description = f"{condition_str.get('name')} ì§€í‘œê°€ {condition_str.get('value')}{condition_str.get('operator')} ì¡°ê±´ ê¸°ê°„ì˜ í‰ê·  ìˆ˜ìµë¥ ì„ ë¶„ì„í•œ ê²°ê³¼ì…ë‹ˆë‹¤."

        return {
            "query_intent": intent_json,
            "analysis_subject": analysis_subject,
            "description": description,
            "result": paginated_result,
            "pagination": { "current_page": page, "total_pages": total_pages, "total_items": total_items },
            "cache_key": cache_key
        }
    except Exception as e:
        traceback.print_exc()
        return {"error": f"ë¶„ì„ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"}
    

def handle_season_condition(period_tuple, season):
    """'ì—¬ë¦„' ë˜ëŠ” 'ê²¨ìš¸' ì¡°ê±´ì— ë§ëŠ” ë‚ ì§œ êµ¬ê°„ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜ (ìµœì í™”)"""
    start_date, end_date = period_tuple
    event_periods = []
    
    for year in range(start_date.year, end_date.year + 1):
        if season == "ì—¬ë¦„":
            season_start = datetime(year, 6, 1)
            season_end = datetime(year, 8, 31)
            overlap_start = max(start_date, season_start)
            overlap_end = min(end_date, season_end)
            if overlap_start < overlap_end:
                event_periods.append((overlap_start, overlap_end))

        elif season == "ê²¨ìš¸":
            # Winter can span across two years (Dec of year Y-1 to Feb/Mar of year Y)
            # So we check for both the current year's winter and the previous year's winter that ends in the current year.
            for y in [year - 1, year]: 
                season_start = datetime(y, 12, 1)
                season_end = datetime(y + 1, 3, 1) - timedelta(days=1) # End of Feb or March 1st minus 1 day
                overlap_start = max(start_date, season_start)
                overlap_end = min(end_date, season_end)
                if overlap_start < overlap_end:
                    event_periods.append((overlap_start, overlap_end))

    # Remove duplicates and sort the periods
    return sorted(list(set(event_periods)))

def handle_interest_rate_condition(api_key, period_tuple):
    """ê¸ˆë¦¬ ì¸ìƒ ì¡°ê±´ì— ë§ëŠ” ë‚ ì§œ êµ¬ê°„ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜"""
    start_date, end_date = period_tuple
    hike_dates = get_interest_rate_hike_dates(api_key)
    
    event_periods = []
    for hike_date in hike_dates:
        event_start = hike_date
        event_end = event_start + timedelta(days=7) # Consider a 7-day window after hike
        
        # Ensure the event period is within the overall query period
        if event_start >= start_date and event_end <= end_date:
            event_periods.append((event_start, event_end))
            
    return event_periods

# Helper function to fetch data for a single stock for parallel processing
def _fetch_and_analyze_single_stock(stock_code, stock_name, overall_start, overall_end, event_periods):
    """
    ë‹¨ì¼ ì¢…ëª©ì˜ ì „ì²´ ê¸°ê°„ ë°ì´í„°ë¥¼ ì¡°íšŒí•˜ê³ , ê·¸ ì•ˆì—ì„œ ì´ë²¤íŠ¸ ê¸°ê°„ ìˆ˜ìµë¥ ì„ ë¶„ì„í•˜ëŠ” í—¬í¼ í•¨ìˆ˜ (ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•¨)
    """
    try:
        print(f" Â  Â  Â ë°ì´í„° ì¡°íšŒ ì‹œì‘: {stock_name}({stock_code}) - {overall_start.strftime('%Y-%m-%d')} ~ {overall_end.strftime('%Y-%m-%d')}")

        overall_prices = fdr.DataReader(stock_code, overall_start, overall_end)
        print(f" Â  Â  Â ë°ì´í„° ì¡°íšŒ ì™„ë£Œ: {stock_name}({stock_code})")

        if overall_prices.empty or len(overall_prices) < 2:
            return None 

        start_price = int(overall_prices['Open'].iloc[0])
        end_price = int(overall_prices['Close'].iloc[-1])

        period_returns = []
        for start, end in event_periods:
            prices_in_period = overall_prices.loc[start:end]
            
            if len(prices_in_period) > 1:
                event_start_price = prices_in_period['Open'].iloc[0]
                event_end_price = prices_in_period['Close'].iloc[-1]
                if event_start_price > 0:
                    period_returns.append((event_end_price / event_start_price) - 1)
        
        if period_returns:
            average_return = statistics.mean(period_returns)
            if pd.notna(average_return):
                return {
                    "code": stock_code, "name": stock_name,
                    "value": round(average_return * 100, 2), "label": "í‰ê·  ìˆ˜ìµë¥ (%)",
                    "start_price": start_price, # ì „ì²´ ê¸°ê°„ ì‹œì‘ ê°€ê²©
                    "end_price": end_price, # ì „ì²´ ê¸°ê°„ ì¢…ë£Œ ê°€ê²©
                }
    except Exception as e:
        print(f" - {stock_name}({stock_code}) ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    return None # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë˜ëŠ” ìœ íš¨í•œ ìˆ˜ìµë¥ ì´ ì—†ìœ¼ë©´ None ë°˜í™˜

def analyze_top_performers(target_stocks, event_periods, overall_period):
    """
    [ì„±ëŠ¥ ìµœì í™”] ì „ì²´ ê¸°ê°„ ë°ì´í„°ë¥¼ í•œ ë²ˆì— ì¡°íšŒ í›„, ë©”ëª¨ë¦¬ì—ì„œ ì¡°ê±´ ê¸°ê°„ì„ ìŠ¬ë¼ì´ì‹±í•˜ì—¬ ë¶„ì„ ì†ë„ë¥¼ ê°œì„ í•©ë‹ˆë‹¤.
    ë˜í•œ, ì—¬ëŸ¬ ì¢…ëª©ì˜ ë°ì´í„° ì¡°íšŒë¥¼ ë³‘ë ¬ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    """
    analysis_results = []
    top_stocks = target_stocks.nlargest(min(len(target_stocks), 50), 'Marcap').reset_index(drop=True)
    print(f"ì‹œê°€ì´ì•¡ ìƒìœ„ {len(top_stocks)}ê°œ ì¢…ëª©ì— ëŒ€í•œ ìˆ˜ìµë¥  ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    overall_start, overall_end = overall_period

    with concurrent.futures.ThreadPoolExecutor(max_workers=30) as executor: # 20ì—ì„œ 30ìœ¼ë¡œ ì¦ê°€
        future_to_stock = {
            executor.submit(_fetch_and_analyze_single_stock, stock['Code'], stock['Name'], overall_start, overall_end, event_periods): stock
            for index, stock in top_stocks.iterrows()
        }

        for i, future in enumerate(concurrent.futures.as_completed(future_to_stock)):
            stock_info = future_to_stock[future]
            stock_code, stock_name = stock_info['Code'], stock_info['Name']
            try:
                result = future.result()
                if result:
                    analysis_results.append(result)
                print(f" Â  ({i + 1}/{len(top_stocks)}) {stock_name}({stock_code}) ë¶„ì„ ì™„ë£Œ.")
            except Exception as exc:
                print(f" Â  - {stock_name}({stock_code}) ë¶„ì„ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {exc}")
    
    return analysis_results

def analyze_volatility(target_stocks, period_tuple):
    """ë³€ë™ì„± ë¶„ì„ í•¨ìˆ˜ (ë”œë ˆì´ ì œê±°)"""
    analysis_results = []
    start_date, end_date = period_tuple
    top_stocks = target_stocks.nlargest(min(len(target_stocks), 50), 'Marcap').reset_index(drop=True)
    print(f"ì‹œê°€ì´ì•¡ ìƒìœ„ {len(top_stocks)}ê°œ ì¢…ëª©ì— ëŒ€í•œ ë³€ë™ì„± ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...")

    # ThreadPoolExecutorë¥¼ ì‚¬ìš©í•˜ì—¬ ë³€ë™ì„± ë¶„ì„ì„ ìœ„í•œ ë°ì´í„° ì¡°íšŒë¥¼ ë³‘ë ¬ë¡œ ì²˜ë¦¬
    with concurrent.futures.ThreadPoolExecutor(max_workers=30) as executor: # 20ì—ì„œ 30ìœ¼ë¡œ ì¦ê°€
        future_to_stock = {
            executor.submit(
                lambda code, name, s_date, e_date: _fetch_and_calculate_volatility(code, name, s_date, e_date),
                stock_info['Code'], stock_info['Name'], start_date, end_date
            ): stock_info
            for index, stock_info in top_stocks.iterrows()
        }

        for i, future in enumerate(concurrent.futures.as_completed(future_to_stock)):
            stock_info = future_to_stock[future]
            code, name = stock_info['Code'], stock_info['Name']
            try:
                result = future.result()
                if result:
                    analysis_results.append(result)
                print(f" Â  ({i + 1}/{len(top_stocks)}) {name}({code}) ë³€ë™ì„± ë¶„ì„ ì™„ë£Œ.")
            except Exception as exc:
                print(f" Â  - {name}({code}) ë³€ë™ì„± ë¶„ì„ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {exc}")
    
    return analysis_results

def _fetch_and_calculate_volatility(code, name, start_date, end_date):
    """
    ë‹¨ì¼ ì¢…ëª©ì˜ ë°ì´í„°ë¥¼ ì¡°íšŒí•˜ê³  ë³€ë™ì„±ì„ ê³„ì‚°í•˜ëŠ” í—¬í¼ í•¨ìˆ˜ (ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•¨)
    """
    try:
        print(f" Â  Â  Â ë³€ë™ì„± ë°ì´í„° ì¡°íšŒ ì‹œì‘: {name}({code}) - {start_date.strftime('%Y-%m-%d')} ~ {end_date.strftime('%Y-%m-%d')}")
        overall_prices = fdr.DataReader(code, start_date, end_date)
        print(f" Â  Â  Â ë³€ë™ì„± ë°ì´í„° ì¡°íšŒ ì™„ë£Œ: {name}({code})")

        if overall_prices.empty:
            return None
        
        daily_returns = overall_prices['Close'].pct_change().dropna()
        volatility = daily_returns.std()
        if pd.notna(volatility):
            return {
                "code": code, "name": name,
                "value": round(volatility * 100, 2), "label": "ë³€ë™ì„±(%)",
                "start_price": int(overall_prices['Open'].iloc[0]),
                "end_price": int(overall_prices['Close'].iloc[-1])
            }
    except Exception as e:
        print(f" Â  - {name}({code}) ë³€ë™ì„± ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    return None

def handle_indicator_condition(condition_obj, period_tuple):
    """CPI, ê¸ˆë¦¬ ë“± ì§€í‘œ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ë‚ ì§œ êµ¬ê°„ì„ ë°˜í™˜"""
    bok_api_key = os.getenv("ECOS_API_KEY")
    if not bok_api_key: return []
    INDICATOR_MAP = {
        "CPI": {"stats_code": "901Y001", "item_code": "0"},
        "ê¸°ì¤€ê¸ˆë¦¬": {"stats_code": "722Y001", "item_code": "0001000"},
    }

    indicator_name = condition_obj.get("name")
    if indicator_name not in INDICATOR_MAP: return []

    indicator_info = INDICATOR_MAP[indicator_name]
    data_series = get_bok_data(bok_api_key, indicator_info['stats_code'], indicator_info['item_code'], period_tuple[0], period_tuple[1])

    if data_series is None: return []

    op_str = condition_obj.get("operator")
    value = condition_obj.get("value")

    if op_str == '>': matching_series = data_series[data_series > value]
    elif op_str == '>=': matching_series = data_series[data_series >= value]
    else: return []

    return [(d.replace(day=1), (d.replace(day=1) + timedelta(days=32)).replace(day=1) - timedelta(days=1)) for d in matching_series.index]

def get_bok_data(bok_api_key, stats_code, item_code, start_date, end_date):
    """
    í•œêµ­ì€í–‰ ECOS APIë¥¼ í†µí•´ íŠ¹ì • ì§€í‘œì˜ ì‹œê³„ì—´ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ Pandas Seriesë¡œ ë°˜í™˜.
    """
    start_str = start_date.strftime('%Y%m')
    end_str = end_date.strftime('%Y%m')
    
    url = f"https://ecos.bok.or.kr/api/StatisticSearch/{bok_api_key}/json/kr/1/1000/{stats_code}/MM/{start_str}/{end_str}/{item_code}"
    
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()  # HTTP ì˜¤ë¥˜ ë°œìƒ ì‹œ ì˜ˆì™¸ ë°œìƒ
        data = response.json()

        if "StatisticSearch" not in data or "row" not in data.get("StatisticSearch", {}):
            print("BOK API ì‘ë‹µì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None

        rows = data["StatisticSearch"]["row"]
        if not rows:
            return None

        df = pd.DataFrame(rows)
        df['TIME'] = pd.to_datetime(df['TIME'], format='%Y%m') 
        df['DATA_VALUE'] = pd.to_numeric(df['DATA_VALUE']) 
        df = df.set_index('TIME')                             
        
        return df['DATA_VALUE'].sort_index()

    except requests.exceptions.RequestException as e:
        print(f"í•œêµ­ì€í–‰ API ìš”ì²­ ì˜¤ë¥˜: {e}")
        return None
    except Exception as e:
        print(f"ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        return None
    
@askfin_bp.route('/')
def askfin_page():
    return render_template('askfin.html')

@askfin_bp.route('/analyze', methods=['POST'])
def analyze_query():
    """
    [ìˆ˜ì •ë¨] JSON ë¶„ì„ ì‹¤íŒ¨ ì‹œ, ì¼ë°˜ ëŒ€í™”í˜• AI ë‹µë³€ìœ¼ë¡œ í´ë°±í•˜ëŠ” ê¸°ëŠ¥ì´ ì¶”ê°€ëœ API.
    """
    if not model:
        return jsonify({"error": "ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. API í‚¤ë¥¼ í™•ì¸í•˜ì„¸ìš”."}), 500
    
    final_result = None # final_result ì´ˆê¸°í™”
    data = request.get_json()
    user_query = data.get('query')
    page = data.get('page', 1)
    cache_key = data.get('cache_key')

    if not user_query:
        return jsonify({"error": "ì˜ëª»ëœ ìš”ì²­ì…ë‹ˆë‹¤."}), 400

    try:
        intent_json = None
        if cache_key and cache_key in ANALYSIS_CACHE:
            print(f"CACHE HIT: ìºì‹œ í‚¤ '{cache_key}'ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            intent_json = ANALYSIS_CACHE[cache_key]['intent_json']
        else:
            print(f"1ì°¨ ì‹œë„: '{user_query}'ì— ëŒ€í•´ JSON ë¶„ì„ì„ ìš”ì²­í•©ë‹ˆë‹¤.")
            prompt = PROMPT_TEMPLATE.format(user_query=user_query)
            response = model.generate_content(prompt)
            raw_text = response.text

            try:
                start = raw_text.find('{')
                end = raw_text.rfind('}') + 1
                cleaned_response = raw_text[start:end]
                intent_json = json.loads(cleaned_response)
                
                new_cache_key = str(hash(json.dumps(intent_json, sort_keys=True)))
                ANALYSIS_CACHE[new_cache_key] = { 'intent_json': intent_json }
                cache_key = new_cache_key 
            
            except (json.JSONDecodeError, IndexError) as e:
                # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ, ì¼ë°˜ ëŒ€í™”í˜• ë‹µë³€ìœ¼ë¡œ í´ë°±
                print(f" JSON ë¶„ì„ ì‹¤íŒ¨({e}). ì¼ë°˜ ëŒ€í™”í˜• ëª¨ë¸ë¡œ í´ë°±í•©ë‹ˆë‹¤.")
                try:
                    general_prompt = f"ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•´ ì¹œì ˆí•˜ê³  ìƒì„¸í•˜ê²Œ ë‹µë³€í•´ì¤˜: {user_query}"
                    fallback_response = model.generate_content(general_prompt)
                    
                    final_result = {
                        "analysis_subject": "ì¼ë°˜ ë‹µë³€",
                        "result": [fallback_response.text.replace('\r\n', '<br>').replace('\n', '<br>').strip()] # ì¤„ë°”ê¿ˆ ì²˜ë¦¬ ê°•í™”
                    }
                    return jsonify(final_result)
                
                except Exception as fallback_e:
                    print(f" í´ë°± ëª¨ë¸ í˜¸ì¶œ ì‹¤íŒ¨: {fallback_e}")
                    traceback.print_exc()
                    return jsonify({"error": "ì§ˆë¬¸ì„ ë¶„ì„í•˜ëŠ”ë° ì‹¤íŒ¨í–ˆê³ , ì¼ë°˜ ë‹µë³€ë„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤."}), 500
        
        if not intent_json or not intent_json.get("query_type"): 
            print(f"ë””ë²„ê·¸: Geminiê°€ ë°˜í™˜í•œ JSONì´ ìœ íš¨í•˜ì§€ ì•Šê±°ë‚˜ query_typeì´ ì—†ìŠµë‹ˆë‹¤: {intent_json}")
            print(f" ì•Œ ìˆ˜ ì—†ëŠ” ì§ˆë¬¸ ìœ í˜• ë˜ëŠ” ìœ íš¨í•˜ì§€ ì•Šì€ JSON. ì¼ë°˜ ëŒ€í™”í˜• ëª¨ë¸ë¡œ í´ë°±í•©ë‹ˆë‹¤.")
            general_prompt = f"ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•´ ì¹œì ˆí•˜ê²Œ ë‹µë³€í•´ì¤˜: {user_query}"
            fallback_response = model.generate_content(general_prompt)
            final_result = {
                "analysis_subject": "ì¼ë°˜ ë‹µë³€",
                "result": [fallback_response.text.replace('\r\n', '<br>').replace('\n', '<br>').strip()]
            }
            return jsonify(final_result)

        query_type = intent_json.get("query_type")
        
        if query_type == "stock_analysis":
            final_result = execute_stock_analysis(intent_json, page, user_query, cache_key)
            if not final_result.get('result') or (isinstance(final_result.get('result'), list) and len(final_result['result']) == 0):
                print(f"ë””ë²„ê·¸: ì£¼ì‹ ë¶„ì„ ê²°ê³¼ê°€ 0ê°œì…ë‹ˆë‹¤. ì¼ë°˜ ëŒ€í™”í˜• ëª¨ë¸ë¡œ í´ë°±í•©ë‹ˆë‹¤.")
                general_prompt = f"ìš”ì²­í•˜ì‹  '{user_query}'ì— ëŒ€í•œ ì£¼ì‹ ë°ì´í„°ë¥¼ ì¶©ë¶„íˆ ì°¾ê±°ë‚˜ ë¶„ì„í•  ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ í•´ì£¼ì‹œê² ì–´ìš”?"
                fallback_response = model.generate_content(general_prompt)
                final_result = {
                    "analysis_subject": "ì¼ë°˜ ë‹µë³€",
                    "result": [fallback_response.text.replace('\r\n', '<br>').replace('\n', '<br>').strip()]
                }

        elif query_type == "indicator_lookup":
            lookup_result = execute_indicator_lookup(intent_json)
            
            if lookup_result.get("analysis_subject") == "ì•Œ ìˆ˜ ì—†ëŠ” ì§€í‘œ" or \
               not lookup_result.get('result') or \
               (isinstance(lookup_result.get('result'), list) and len(lookup_result['result']) == 0):
                
                print(f"ë””ë²„ê·¸: ì§€í‘œ ì¡°íšŒ ì‹¤íŒ¨ ë˜ëŠ” ê²°ê³¼ê°€ 0ê°œì…ë‹ˆë‹¤. ì¼ë°˜ ëŒ€í™”í˜• ëª¨ë¸ë¡œ í´ë°±í•©ë‹ˆë‹¤.")
                general_prompt = f"ìš”ì²­í•˜ì‹  '{user_query}'ì— ëŒ€í•œ ì§€í‘œ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜ ë¶„ì„ì— ë¬¸ì œê°€ ìˆì—ˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ í•´ì£¼ì‹œê² ì–´ìš”?"
                fallback_response = model.generate_content(general_prompt)
                final_result = {
                    "analysis_subject": "ì¼ë°˜ ë‹µë³€",
                    "result": [fallback_response.text.replace('\r\n', '<br>').replace('\n', '<br>').strip()]
                }
            else:
                final_result = lookup_result 

        else: 
            print(f"ë””ë²„ê·¸: ì•Œ ìˆ˜ ì—†ëŠ” query_type '{query_type}'. ì¼ë°˜ ëŒ€í™”í˜• ëª¨ë¸ë¡œ í´ë°±í•©ë‹ˆë‹¤.")
            general_prompt = f"ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•´ ì¹œì ˆí•˜ê²Œ ë‹µë³€í•´ì¤˜: {user_query}"
            fallback_response = model.generate_content(general_prompt)
            final_result = {
                "analysis_subject": "ì¼ë°˜ ë‹µë³€",
                "result": [fallback_response.text.replace('\r\n', '<br>').replace('\n', '<br>').strip()]
            }
            
        return jsonify(final_result)

    except Exception as e:
        traceback.print_exc()
        return jsonify({"error": f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"}), 500

@askfin_bp.route('/new_chat', methods=['POST'])
def new_chat():
    """ëŒ€í™” ê¸°ë¡(ì„¸ì…˜)ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    session.pop('chat_history', None)
    return jsonify({"status": "success", "message": "ìƒˆ ëŒ€í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤."})