from flask import Flask, render_template, jsonify, session
import FinanceDataReader as fdr
import pandas as pd
from pytz import timezone
from datetime import datetime, timedelta
import os
import json
import yfinance as yf
from pykrx import stock
import requests
from bs4 import BeautifulSoup
import traceback
from urllib.parse import urljoin
from readability import Document
import pickle
import re
import sys
from run import EnhancedStockPredictor

from transformers import AutoTokenizer, pipeline
from blueprints.analysis import analysis_bp
from blueprints.tables import tables_bp
from blueprints.data import data_bp
from blueprints.auth import auth_bp
from blueprints import askfin
from blueprints.askfin import askfin_bp, initialize_global_data, GLOBAL_TICKER_NAME_MAP
from blueprints.search import search_bp
from dotenv import load_dotenv

# from db.extensions import db # ì´ ì¤„ì€ ì œê±°í•˜ê±°ë‚˜ ì£¼ì„ ì²˜ë¦¬í•´ì•¼ í•©ë‹ˆë‹¤.
load_dotenv()

app = Flask(__name__)
app.secret_key = os.urandom(24)

# â”€â”€ ê¸ˆìœµ í‚¤ì›Œë“œ ì„¸íŠ¸ (data-files/finance.csv) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
finance_df = pd.read_csv(
    os.path.join(os.path.dirname(__file__), "data_files", "finance.csv"),
    encoding="utf-8-sig"
)
# CSVì— 'keyword' ì»¬ëŸ¼ì´ ìˆë‹¤ê³  ê°€ì •
FINANCE_KEYWORDS = set(
    finance_df['keyword']
    .dropna()
    .astype(str)
    .str.lower()
    .str.strip()
)

# â”€â”€ Sentiment model & pipelines ë¡œë”© â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SAVED_MODEL_DIR = os.path.join(os.path.dirname(__file__), "data_files", "saved_model")
tokenizer = AutoTokenizer.from_pretrained(
    SAVED_MODEL_DIR, use_fast=True, trust_remote_code=True
)
sentiment_pipeline = pipeline(
    'sentiment-analysis',
    model=SAVED_MODEL_DIR,
    tokenizer=SAVED_MODEL_DIR,
    return_all_scores=False,
    device=-1
)

# ë¶ˆìš© ë¬¸ì/í† í° ì œê±°ìš© (í•„ìš”ì‹œ ë” ì¶”ê°€)
STOP_CHARS_PATTERN = re.compile(r"[.,Â·()\[\]{}!?;:â€œâ€\"'`â€¦]")

def clean_for_sentiment(text: str) -> str:
    text = STOP_CHARS_PATTERN.sub(" ", text)
    text = text.replace("[UNK]", " ")
    text = re.sub(r"\s+", " ", text).strip()
    return text

# â”€â”€ ê¸°ì—…ëª… ì¶”ì¶œê¸° ë¡œë”© â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with open(os.path.join(os.path.dirname(__file__), 'data_files', 'keyword_processor.pkl'), 'rb') as f:
    keyword_processor = pickle.load(f)
corp_df = pd.read_csv(
    os.path.join(os.path.dirname(__file__), 'data_files', 'corp_names.csv'),
    encoding='utf-8-sig'
)
COMPANY_SET = set(corp_df['corp_name'].astype(str))
STOPWORDS = {"ETF", "ETN", "ì‹ íƒ", "SPAC", "í€ë“œ", "ë¦¬ì¸ "}

BOUNDARY = r"[ê°€-í£A-Za-z0-9]"   # ë‹¨ì–´ë¡œ ì·¨ê¸‰í•  ë¬¸ìë“¤

def is_standalone(word: str, text: str) -> bool:
    """
    text ì•ˆì—ì„œ wordê°€ ì–‘ì˜†ì´ ë¬¸ì/ìˆ«ìì— ë¶™ì–´ìˆì§€ ì•Šì€ 'ë…ë¦½ëœ' í˜•íƒœë¡œ ì¡´ì¬í•˜ëŠ”ì§€ ê²€ì‚¬
    """
    pattern = rf"(?<!{BOUNDARY}){re.escape(word)}(?!{BOUNDARY})"
    return re.search(pattern, text) is not None

def extract_companies(text: str) -> list[str]:
    cleaned = re.sub(r'â“’.*', '', text)
    candidates = keyword_processor.extract_keywords(cleaned)

    uniq = dict.fromkeys(candidates)  # ìˆœì„œ ìœ ì§€í•œ ì¤‘ë³µ ì œê±°
    filtered = []
    for w in uniq:
        # 1) ê¸°ë³¸ í•„í„°
        if w in STOPWORDS:
            continue
        if w not in COMPANY_SET:
            continue
        # 2) ë‹¨ë… ë‹¨ì–´ì¸ì§€ í™•ì¸ (ë¶€ë¶„ ë¬¸ìì—´ ë°©ì§€)
        if not is_standalone(w, cleaned):
            continue
        filtered.append(w)
    return filtered

# â”€â”€ ë³¸ë¬¸(fetch) í—¬í¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fetch_body(url: str) -> str:
    try:
        resp = requests.get(url, headers={"User-Agent": "Mozilla/5.0"}, timeout=5)
        resp.raise_for_status()
        html = resp.text
        doc = Document(html)
        content_html = doc.summary()
        soup = BeautifulSoup(content_html, "html.parser")
        for tag in soup(["script", "style"]):
            tag.decompose()
        paragraphs = [
            p.get_text(strip=True)
            for p in soup.find_all("p")
            if len(p.get_text(strip=True)) > 20
        ]
        return "\n".join(paragraphs)
    except Exception:
        return ""

# â”€â”€ Jinja2 í•„í„° ì •ì˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@app.template_filter('format_kr')
def format_kr(value):
    try:
        num = int(value)
        if num >= 100000000:
            return f"{num // 100000000}ì–µ"
        elif num >= 10000:
            return f"{num // 10000}ë§Œ"
        else:
            return str(num)
    except (ValueError, TypeError):
        return value

@app.template_filter('format_price')
def format_price(value):
    try:
        return f"{int(float(value)):,}"
    except (ValueError, TypeError):
        return value

@app.template_filter('format_value')
def format_value(value):
    try:
        return f"{int(value) // 100000000:,.0f}ì–µ"
    except (ValueError, TypeError):
        return value

app.jinja_env.filters['format_kr'] = format_kr
app.jinja_env.filters['format_price'] = format_price
app.jinja_env.filters['format_value'] = format_value

CACHE_PATH = 'cache/market_data.json'
CACHE_DIR = 'cache'

def get_latest_business_day():
    return stock.get_nearest_business_day_in_a_week()

def get_market_rank_data(date_str):
    kospi_df = stock.get_market_ohlcv(date_str, market="KOSPI").reset_index()
    kosdaq_df = stock.get_market_ohlcv(date_str, market="KOSDAQ").reset_index()
    for df in [kospi_df, kosdaq_df]:
        tickers = df['í‹°ì»¤']
        if askfin.GLOBAL_TICKER_NAME_MAP is None:
            askfin.initialize_global_data()
        names = [askfin.GLOBAL_TICKER_NAME_MAP.get(ticker, ticker) for ticker in tickers]
        df['Name'] = names
        df.rename(columns={'í‹°ì»¤': 'Code', 'ì¢…ê°€': 'Close', 'ê±°ë˜ëŸ‰': 'Volume', 'ê±°ë˜ëŒ€ê¸ˆ': 'TradingValue', 'ë“±ë½ë¥ ': 'ChangeRatio'}, inplace=True)
        for col in ['Close', 'Volume', 'TradingValue', 'ChangeRatio']:
            df[col] = df[col].fillna(0)
    return kospi_df.to_dict('records'), kosdaq_df.to_dict('records')

def get_wti_data(days=60):
    ticker = yf.Ticker("CL=F")
    df = ticker.history(period=f"{days}d")
    return df.reset_index()[['Date', 'Close']].dropna()

def calculate_change_info(df, name):
    if df is None or len(df.index) < 2:
        return {'name': name, 'value': 'N/A', 'change': 'N/A', 'change_pct': 'N/A', 'raw_change': 0}
    df_no_tz = df.copy()
    if isinstance(df_no_tz.index, pd.DatetimeIndex):
       df_no_tz.index = df_no_tz.index.tz_localize(None)
    df_no_tz = df_no_tz.sort_index().dropna()
    if len(df_no_tz) < 2:
        return {'name': name, 'value': 'N/A', 'change': 'N/A', 'change_pct': 'N/A', 'raw_change': 0}
    latest = df_no_tz.iloc[-1]
    previous = df_no_tz.iloc[-2]
    value = latest['Close']
    change = value - previous['Close']
    change_pct = (change / previous['Close']) * 100 if previous['Close'] != 0 else 0
    return {'name': name, 'value': f"{value:,.2f}", 'change': f"{change:,.2f}", 'change_pct': f"{change_pct:+.2f}%", 'raw_change': change}

def get_fdr_or_yf_data(ticker, start, end, interval='1d'):
    yf_ticker_map = {'USD/KRW': 'KRW=X', 'KS11': '^KS11', 'KQ11': '^KQ11', 'CL=F': 'CL=F'}
    actual_yf_ticker = yf_ticker_map.get(ticker, ticker)
    if ticker.endswith(('.KS', '.KQ')):
        actual_yf_ticker = ticker

    # Method 1: yfinance with specified interval (generally most reliable)
    try:
        print(f"Method 1: Attempting yfinance fetch for '{actual_yf_ticker}' with interval '{interval}'...")
        df = yf.download(actual_yf_ticker, start=start, end=end, interval=interval, auto_adjust=True, progress=False)
        if not df.empty:
            print("yfinance fetch successful.")
            df = df.reset_index()
            date_col = next((col for col in ['Date', 'Datetime', 'index'] if col in df.columns), None)
            if date_col:
                df.rename(columns={date_col: 'Date'}, inplace=True)
            return df[['Date', 'Close']].copy()
    except Exception as e:
        print(f"Method 1 failed: {e}")

    # Method 2: fdr (daily) fetch as a primary source for daily requests
    try:
        print(f"Method 2: Attempting fdr (daily) fetch for '{ticker}'...")
        df = fdr.DataReader(ticker, start, end)
        if not df.empty:
            # If a non-daily interval was requested, resample the daily data
            if interval != '1d':
                print(f"Resampling daily FDR data to interval '{interval}'...")
                df.index.name = 'Date'
                rule = {'1wk': 'W', '1mo': 'M'}.get(interval)
                if rule:
                    resampled_df = df['Close'].resample(rule).last().reset_index()
                    resampled_df.dropna(inplace=True)
                    if not resampled_df.empty:
                        print(f"FDR+resample to '{rule}' successful.")
                        return resampled_df[['Date', 'Close']].copy()
            else: # For daily requests
                print("FDR (daily) fetch successful.")
                df = df.reset_index()
                if 'index' in df.columns:
                    df.rename(columns={'index': 'Date'}, inplace=True)
                return df[['Date', 'Close']].copy()
    except Exception as e:
        print(f"Method 2 failed: {e}")
        
    print(f"All methods failed for ticker '{ticker}' with interval '{interval}'. Returning empty DataFrame.")
    return pd.DataFrame()

@app.route('/news/<string:code>')
def get_news(code):
    """
    íŠ¹ì • ì¢…ëª© ì½”ë“œì— ëŒ€í•œ ë‰´ìŠ¤ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    codeëŠ” '005930.KS'ì™€ ê°™ì€ yfinance í˜•ì‹ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    """
    formatted_news = []
    stock_code_6_digit = code.split('.')[0]
    print(f"DEBUG: '{code}' (6ìë¦¬: {stock_code_6_digit}) ì¢…ëª© ì½”ë“œì— ëŒ€í•´ ë„¤ì´ë²„ ëª¨ë°”ì¼ APIì—ì„œ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸° ì‹œë„.")
    try:
        url = f"https://m.stock.naver.com/api/news/stock/{stock_code_6_digit}?pageSize=10&page=1"
        headers = {"User-Agent": "Mozilla/5.0"}
        response = requests.get(url, headers=headers, timeout=5)
        response.raise_for_status()
        raw_data = response.json()

        if isinstance(raw_data, list):
            for data_group in raw_data:
                if isinstance(data_group, dict) and 'items' in data_group:
                    for item in data_group.get('items', []):
                        if isinstance(item, dict):
                            office_id, article_id = item.get('officeId'), item.get('articleId')
                            if office_id and article_id:
                                raw_dt = item.get('datetime', '')
                                f_date = f"{raw_dt[0:4]}-{raw_dt[4:6]}-{raw_dt[6:8]} {raw_dt[8:10]}:{raw_dt[10:12]}" if len(raw_dt) >= 12 else raw_dt
                                formatted_news.append({
                                    'title': item.get('title'),
                                    'press': item.get('officeName'),
                                    'date': f_date,
                                    'url': f"https://n.news.naver.com/mnews/article/{office_id}/{article_id}"
                                })

        if not formatted_news:
            print(f"DEBUG: '{code}'ì— ëŒ€í•œ ë‰´ìŠ¤ë¥¼ APIì—ì„œ ê°€ì ¸ì™”ìœ¼ë‚˜ í•­ëª©ì´ 0ê°œì…ë‹ˆë‹¤.")
            return jsonify({"error": "ê´€ë ¨ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. (API ê²°ê³¼ ì—†ìŒ)"}), 200
        
        processed_news = []
        for news_item in formatted_news[:5]:
            body = fetch_body(news_item["url"])
            sentiment = "ì—†ìŒ"
            companies = []

            if body and len(body.strip()) > 50:
                title_clean = clean_for_sentiment(news_item["title"])
                target_text = title_clean if title_clean.strip() else clean_for_sentiment(body)[:256]

                if target_text.strip():
                    try:
                        sentiment_result = sentiment_pipeline(target_text)[0]["label"]
                        sentiment = sentiment_result if sentiment_result else "ì—†ìŒ"
                    except Exception as sentiment_e:
                        print(f"DEBUG: ê°ì„± ë¶„ì„ ì˜¤ë¥˜: {sentiment_e}")
                        sentiment = "ì˜¤ë¥˜"

                try:
                    companies = extract_companies(body)
                except Exception as company_e:
                    print(f"DEBUG: ê¸°ì—…ëª… ì¶”ì¶œ ì˜¤ë¥˜: {company_e}")
                    companies = []
            
            processed_news.append({
                'title': news_item['title'],
                'press': news_item['press'],
                'date': news_item['date'],
                'url': news_item['url'],
                'sentiment': sentiment,
                'companies': companies
            })

        print(f"DEBUG: '{code}'ì— ëŒ€í•œ ë‰´ìŠ¤ {len(processed_news)}ê°œ ì„±ê³µì ìœ¼ë¡œ ê°€ì ¸ì˜´ (ê°ì„± ë¶„ì„ í¬í•¨).")
        return jsonify(processed_news)

    except requests.exceptions.Timeout:
        print(f"DEBUG: ë‰´ìŠ¤ API ìš”ì²­ íƒ€ì„ì•„ì›ƒ ë°œìƒ (ì¢…ëª©ì½”ë“œ: {code})")
        return jsonify({"error": "ë‰´ìŠ¤ ìš”ì²­ ì‹œê°„ ì´ˆê³¼ (API í˜¸ì¶œ ì‹¤íŒ¨)"}), 500
    except requests.exceptions.RequestException as e:
        print(f"DEBUG: ë‰´ìŠ¤ API ìš”ì²­ ì˜¤ë¥˜ (ì¢…ëª©ì½”ë“œ: {code}): {e}")
        return jsonify({"error": f"ë‰´ìŠ¤ API í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}"}), 500
    except json.JSONDecodeError:
        print(f"DEBUG: ë‰´ìŠ¤ API ì‘ë‹µì´ ìœ íš¨í•œ JSONì´ ì•„ë‹˜ (ì¢…ëª©ì½”ë“œ: {code})")
        return jsonify({"error": "ë‰´ìŠ¤ API ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨"}), 500
    except Exception as e:
        print(f"DEBUG: ë‰´ìŠ¤ API ì²˜ë¦¬ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ ë°œìƒ (ì¢…ëª©ì½”ë“œ: {code}): {e}")
        traceback.print_exc()
        return jsonify({"error": f"ë‰´ìŠ¤ API ì²˜ë¦¬ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜: {str(e)}"}), 500


def _get_news_from_naver_scraping():
    news_list = []
    print("DEBUG: Attempting to scrape general market news from Naver Finance.")
    try:
        url = "https://finance.naver.com/news/mainnews.naver"
        headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"}
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()

        soup = BeautifulSoup(response.text, 'html.parser')

        news_items = soup.select('.main_news .newsList .articleSubject a')
        press_items = soup.select('.main_news .newsList .articleSummary .press')
        date_items = soup.select('.main_news .newsList .articleSummary .wdate')

        for i in range(min(len(news_items), 20)):
            title = news_items[i].get_text(strip=True)
            link = news_items[i]['href']
            press = press_items[i].get_text(strip=True) if i < len(press_items) else 'N/A'
            date_time = date_items[i].get_text(strip=True) if i < len(date_items) else 'N/A'

            if link.startswith('/'):
                link = f"https://finance.naver.com{link}"

            news_list.append({
                'title': title,
                'press': press,
                'date': date_time,
                'url': link
            })
        print(f"DEBUG: Naver general news scraping successful. Found {len(news_list)} news items.")
    except Exception as e:
        print(f"DEBUG: Error fetching general market news via scraping: {e}")
        traceback.print_exc()
        news_list.append({'title': 'ì¼ë°˜ ì‹œì¥ ë‰´ìŠ¤ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤ (í¬ë¡¤ë§ ì˜¤ë¥˜).', 'press': 'N/A', 'date': 'N/A', 'url': '#'})
    return news_list

def get_key_statistic_current_data():
    ecos_api_key = os.getenv("ECOS_API_KEY")
    if not ecos_api_key:
        print("ECOS API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ ì£¼ìš” í†µê³„ í˜„í™© ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return []

    api_url = f"https://ecos.bok.or.kr/api/KeyStatisticList/{ecos_api_key}/xml/kr/1/100/"

    key_statistics_current_data = []
    print(f"DEBUG: ECOS ì£¼ìš” í†µê³„ í˜„í™© API í˜¸ì¶œ ì‹œë„: {api_url}")
    try:
        response = requests.get(api_url, timeout=10)
        response.raise_for_status()

        soup = BeautifulSoup(response.content, 'xml')
        items = soup.find_all('row')

        if not items:
            print("ECOS ì£¼ìš” í†µê³„ í˜„í™© APIì—ì„œ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return []

        for item in items:
            class_name = item.find('CLASS_NAME').get_text() if item.find('CLASS_NAME') else 'N/A'
            keystat_name = item.find('KEYSTAT_NAME').get_text() if item.find('KEYSTAT_NAME') else 'N/A'
            data_value = item.find('DATA_VALUE').get_text() if item.find('DATA_VALUE') else 'N/A'
            cycle = item.find('CYCLE').get_text() if item.find('CYCLE') else 'N/A'
            unit_name = item.find('UNIT_NAME').get_text() if item.find('UNIT_NAME') else 'N/A'

            key_statistics_current_data.append({
                "CLASS_NAME": class_name,
                "KEYSTAT_NAME": keystat_name,
                "DATA_VALUE": data_value,
                "CYCLE": cycle,
                "UNIT_NAME": unit_name
            })
        print(f"DEBUG: ECOS ì£¼ìš” í†µê³„ í˜„í™© ë°ì´í„° {len(key_statistics_current_data)}ê°œ í•­ëª© ì„±ê³µì ìœ¼ë¡œ ê°€ì ¸ì˜´.")
        return key_statistics_current_data

    except requests.exceptions.Timeout:
        print("ECOS ì£¼ìš” í†µê³„ í˜„í™© API ìš”ì²­ ì‹œê°„ ì´ˆê³¼.")
    except requests.exceptions.RequestException as e:
        print(f"ECOS ì£¼ìš” í†µê³„ í˜„í™© API ìš”ì²­ ì˜¤ë¥˜: {e}")
        traceback.print_exc()
    except Exception as e:
        print(f"ECOS ì£¼ìš” í†µê³„ í˜„í™© ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        traceback.print_exc()
    return []


def get_general_market_news():
    news_api_key = os.getenv("NEWS_API_KEY")
    raw_list = []

    if news_api_key:
        try:
            query = (
                "í•œêµ­ ê²½ì œ OR êµ­ë‚´ ì¦ì‹œ OR í•œêµ­ì€í–‰ OR ì½”ìŠ¤í”¼ OR ì½”ìŠ¤ë‹¥ "
                "OR êµ­ë‚´ ê¸°ì—… OR ê±°ì‹œê²½ì œ OR ê¸ˆë¦¬ OR í™˜ìœ¨ OR CPI "
                "OR ì¸í”Œë ˆì´ì…˜ OR GDP OR í†µí™”ì •ì±… OR ì¬ì •ì •ì±…"
            )
            api_url = (
                f"https://newsapi.org/v2/everything?"
                f"q={query}&language=ko&sortBy=publishedAt"
                f"&apiKey={news_api_key}&pageSize=50"
            )
            resp = requests.get(api_url, timeout=7)
            resp.raise_for_status()
            data = resp.json()

            if data.get("status") == "ok" and data.get("articles"):
                for art in data["articles"]:
                    raw_list.append({
                        "title": art.get("title", "ì œëª© ì—†ìŒ"),
                        "press": art.get("source", {}).get("name", "N/A"),
                        "date": art.get("publishedAt", "")[:10],
                        "url": art.get("url", "#"),
                    })
            else:
                raw_list = _get_news_from_naver_scraping()

        except Exception:
            traceback.print_exc()
            raw_list = _get_news_from_naver_scraping()
    else:
        raw_list = _get_news_from_naver_scraping()

    processed = []
    news_count_limit = 10
    for item in raw_list:
        if len(processed) >= news_count_limit:
            break
        body = fetch_body(item["url"])

        combined = (item["title"] + " " + item["press"] + " " + body).lower()
        if not any(kw in combined for kw in FINANCE_KEYWORDS):
            continue

        title_clean = clean_for_sentiment(item["title"])
        if not title_clean.strip():
            backup_text = clean_for_sentiment(body)[:256]
            target_text = backup_text if backup_text else "ë‚´ìš©ì—†ìŒ"
        else:
            target_text = title_clean

        sentiment = "ì—†ìŒ"
        if 'sentiment_pipeline' in globals() and sentiment_pipeline is not None:
            try:
                sentiment = sentiment_pipeline(target_text[:256])[0]["label"]
            except Exception as sentiment_e:
                print(f"DEBUG: ê°ì„± ë¶„ì„ íŒŒì´í”„ë¼ì¸ í˜¸ì¶œ ì˜¤ë¥˜: {sentiment_e}")
                sentiment = "ì˜¤ë¥˜"
        else:
            print("DEBUG: sentiment_pipelineì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")


        companies = extract_companies(body)

        processed.append({
            "title":     item["title"],
            "press":     item["press"],
            "date":      item["date"],
            "url":       item["url"],
            "sentiment": sentiment,
            "companies": companies
        })

    return processed

def get_international_market_news():
    news_api_key = os.getenv("NEWS_API_KEY")
    if not news_api_key:
        print("DEBUG: NEWS_API_KEY ì—†ìŒ. í•´ì™¸ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸° ê±´ë„ˆëœœë‹ˆë‹¤.")
        return []
    try:
        api_url = f"https://newsapi.org/v2/top-headlines?country=us&category=business&apiKey={news_api_key}&pageSize=10"
        print("DEBUG: NewsAPI.orgë¡œ í•´ì™¸ ì‹œì¥ ë‰´ìŠ¤ (ë¯¸êµ­ ë¹„ì¦ˆë‹ˆìŠ¤) ê²€ìƒ‰ ì‹œë„...")
        response = requests.get(api_url, timeout=7)
        response.raise_for_status()

        data = response.json()
        if data.get('status') == 'ok' and data.get('articles'):
            print(f"DEBUG: NewsAPI.orgì—ì„œ í•´ì™¸ ì‹œì¥ ë‰´ìŠ¤ {len(data['articles'])}ê°œ ì„±ê³µì ìœ¼ë¡œ ê°€ì ¸ì˜´.")
            return [{'title': a.get('title', 'ì œëª© ì—†ìŒ'), 'press': a.get('source', {}).get('name', 'N/A'), 'date': a.get('publishedAt', '')[:10], 'url': a.get('url', '#')} for a in data['articles']]
        else:
            print("DEBUG: NewsAPI.org ì‘ë‹µ ìƒíƒœ 'ok' ì•„ë‹˜ ë˜ëŠ” ê¸°ì‚¬ ì—†ìŒ. í•´ì™¸ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨.")
            return []
    except Exception as e:
        print(f"DEBUG: NewsAPI.org ì˜¤ë¥˜ (í•´ì™¸ ë‰´ìŠ¤): {e}. í•´ì™¸ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨.")
        traceback.print_exc()
        return []

def run_and_cache_quant_report():
    print("ğŸš€ ìµœì´ˆ í€€íŠ¸ ë¦¬í¬íŠ¸ ìƒì„± ë° ìºì‹± ì‹œì‘...")
    try:
        predictor = EnhancedStockPredictor(start_date='2015-01-01')
        
        predictor.collect_all_data()
        patterns = predictor.analyze_patterns()
        predictor.detect_anomalies()
        
        current_risks = predictor.calculate_economic_risks_detailed()
        predictions = predictor.predict_weekly_enhanced()
        
        if 'monthly' in patterns and isinstance(patterns.get('monthly'), pd.DataFrame):
            patterns['monthly'] = patterns['monthly'].reset_index().to_dict('records')
        if 'daily' in patterns and isinstance(patterns.get('daily'), pd.DataFrame):
            patterns['daily'] = patterns['daily'].reset_index().to_dict('records')
        
        risk_history_data = None
        if hasattr(predictor, 'risk_history') and not predictor.risk_history.empty:
            df = predictor.risk_history.reset_index()
            df['index'] = df['index'].dt.strftime('%Y-%m-%d')
            risk_history_data = df.to_dict('records')

        monitoring_indicators = []
        if current_risks['inflation']['risk'] > 40:
            monitoring_indicators.extend(["ì›ìì¬ ê°€ê²©", "ë‹¬ëŸ¬ ì¸ë±ìŠ¤", "ì¥ê¸° ê¸ˆë¦¬"])
        if current_risks['deflation']['risk'] > 40:
            monitoring_indicators.extend(["VIX ì§€ìˆ˜", "ê¸€ë¡œë²Œ ì£¼ê°€", "ê²½ê¸°ì„ í–‰ì§€ìˆ˜"])
        if current_risks['stagflation']['risk'] > 30:
            monitoring_indicators.extend(["í™˜ìœ¨", "ê³µê¸‰ë§ ì§€í‘œ", "ì„ê¸ˆ ìƒìŠ¹ë¥ "])
        if not monitoring_indicators:
            monitoring_indicators.extend(["ì „ë°˜ì  ì‹œì¥ ë™í–¥", "ê¸°ìˆ ì  ì§€í‘œ", "ê±°ë˜ëŸ‰"])
        monitoring_indicators = sorted(list(set(monitoring_indicators)))

        report_data = {
            "current_risks": current_risks,
            "future_risks": predictor.future_risks,
            "predictions": predictions,
            "patterns": patterns,
            "anomalies": predictor.anomalies,
            "overall_risk": current_risks.get('overall', 0),
            "risk_history": risk_history_data,
            "monitoring_indicators": monitoring_indicators
        }
        print(" ìµœì´ˆ í€€íŠ¸ ë¦¬í¬íŠ¸ ìºì‹± ì™„ë£Œ.")
        return report_data
    except Exception as e:
        print(f" ìµœì´ˆ í€€íŠ¸ ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
        traceback.print_exc()
        return None


@app.route('/api/latest-data')
def get_latest_data():
    try:
        end_date = datetime.now()
        start_date = end_date - timedelta(days=10)
        data = {}
        for ticker, name in [('KS11', 'kospi'), ('KQ11', 'kosdaq'), ('USD/KRW', 'usdkrw')]:
            df = get_fdr_or_yf_data(ticker, start_date, end_date)
            
            if isinstance(df.columns, pd.MultiIndex):
                print(f"DEBUG: Flattening MultiIndex columns for {name} in latest-data")
                df.columns = df.columns.get_level_values(0)
                df = df.loc[:,~df.columns.duplicated()]

            if ticker == 'USD/KRW' and 'Close' not in df.columns and 'USD/KRW' in df.columns:
                df.rename(columns={'USD/KRW': 'Close'}, inplace=True)
            
            if 'Close' in df.columns and 'Date' in df.columns:
                df['Close'] = pd.to_numeric(df['Close'], errors='coerce')
                df.set_index('Date', inplace=True)
                if not df.empty and len(df) >= 2:
                    data[name] = calculate_change_info(df.copy(), name.upper())
                else:
                    data[name] = {'name': name.upper(), 'value': 'N/A', 'change': 'N/A', 'change_pct': 'N/A', 'raw_change': 0}
            else:
                print(f"ERROR: 'Close' or 'Date' column not found for {name} in latest-data.")
                data[name] = {'name': name.upper(), 'value': 'N/A', 'change': 'N/A', 'change_pct': 'N/A', 'raw_change': 0}

        wti_df = get_wti_data(10)
        if not wti_df.empty: wti_df.set_index('Date', inplace=True)
        if not wti_df.empty and len(wti_df) >= 2:
            data['wti'] = calculate_change_info(wti_df, 'WTI')
        else:
            data['wti'] = {'name': 'WTI', 'value': 'N/A', 'change': 'N/A', 'change_pct': 'N/A', 'raw_change': 0}
            print(f"DEBUG: WTI ë°ì´í„° ë¶€ì¡± ë˜ëŠ” ìœ íš¨í•˜ì§€ ì•ŠìŒ. ë³€í™”ëŸ‰ ê³„ì‚° ê±´ë„ˆëœ€.")

        return jsonify(data)
    except Exception as e:
        print(f"Error in /api/latest-data: {e}")
        import traceback
        traceback.print_exc(file=sys.stderr)
        return jsonify({"error": str(e)}), 500
    
@app.route('/')
def index_main():
    return render_template('index_main.html')

@app.route('/index')
def index():
    os.makedirs(CACHE_DIR, exist_ok=True)
    try:
        with open(CACHE_PATH, 'r', encoding='utf-8') as f:
            cache = json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
        cache = {}

    latest_bday = get_latest_business_day()
    formatted_today_date = datetime.strptime(latest_bday, '%Y%m%d').strftime('%mì›” %dì¼')

    if cache.get('date') != latest_bday:
        print(f"DEBUG: ìºì‹œ ì—…ë°ì´íŠ¸ í•„ìš”. í˜„ì¬ ì˜ì—…ì¼: {latest_bday}, ìºì‹œëœ ë‚ ì§œ: {cache.get('date')}")
        new_cache = {'date': latest_bday}
        
        kospi_all_data = []
        kosdaq_all_data = []

        try:
            kospi_all_data, kosdaq_all_data = get_market_rank_data(latest_bday)
            
            current_key_stats_full = get_key_statistic_current_data()
            filtered_key_stats = [item for item in current_key_stats_full if item.get('DATA_VALUE') not in ['N/A', '', None]]
            important_key_stats = filtered_key_stats[:20]

            korean_news = get_general_market_news()
            international_news = get_international_market_news()

            new_cache.update({
                'kospi_all_data': kospi_all_data,
                'kosdaq_all_data': kosdaq_all_data,
                'korean_market_news': korean_news,
                'international_market_news': international_news,
                'key_statistic_current_data': important_key_stats
            })
        except Exception as e:
            print(f"Error creating cache: {e}")
            traceback.print_exc()
            new_cache.update({
                'kospi_all_data': [],
                'kosdaq_all_data': [],
                'korean_market_news': [],
                'international_market_news': [],
                'key_statistic_current_data': []
            })
        cache = new_cache
        with open(CACHE_PATH, 'w', encoding='utf-8') as f:
            json.dump(cache, f, ensure_ascii=False, indent=2)
    else:
        print(f"DEBUG: ìºì‹œ ìµœì‹  ìƒíƒœ ìœ ì§€. ë‚ ì§œ: {latest_bday}")


    context = {
        'today': datetime.strptime(latest_bday, '%Y%m%d').strftime('%Y-%m-%d'),
        'formatted_today_date': formatted_today_date,
        **cache,
        'key_statistic_current_data': cache.get('key_statistic_current_data', []),
        'korean_market_news': cache.get('korean_market_news', []),
        'international_market_news': cache.get('international_market_news', [])
    }
    days_to_fetch = 60
    start_date, end_date = datetime.now() - timedelta(days=days_to_fetch), datetime.now()

    for ticker, name in [('KS11', 'kospi'), ('KQ11', 'kosdaq'), ('USD/KRW', 'usdkrw')]:
        try:
            df = get_fdr_or_yf_data(ticker, start_date, end_date)
            if df is not None and not df.empty and 'Date' in df.columns:
                
                if isinstance(df.columns, pd.MultiIndex):
                    print(f"DEBUG: Flattening MultiIndex columns for {name}")
                    df.columns = df.columns.get_level_values(0)
                    df = df.loc[:,~df.columns.duplicated()]
                
                if 'Close' not in df.columns:
                    print(f"ERROR: 'Close' column not found for {name} after processing. Columns are: {df.columns.tolist()}")
                    context[f'{name}_data'], context[f'{name}_info'] = [], {'value': 'N/A'}
                    continue

                df['Close'] = pd.to_numeric(df['Close'], errors='coerce')
                df.set_index('Date', inplace=True)
                df_for_chart = df.reset_index().copy()
                df_for_chart['Date'] = pd.to_datetime(df_for_chart['Date']).dt.strftime('%Y-%m-%d')
                context[f'{name}_data'] = df_for_chart.tail(30).to_dict('records')
                if len(df) >= 2:
                    context[f'{name}_info'] = calculate_change_info(df.copy(), name.upper())
                else:
                    context[f'{name}_info'] = {'value': 'N/A', 'change': 'N/A', 'change_pct': 'N/A', 'raw_change': 0}
                    print(f"DEBUG: {name.upper()} ì°¨íŠ¸/ì •ë³´ ë°ì´í„° ë¶€ì¡±. ì •ë³´ ê³„ì‚° ê±´ë„ˆëœ€.")
            else:
                context[f'{name}_data'], context[f'{name}_info'] = [], {'value': 'N/A'}
                print(f"DEBUG: {name.upper()} ì°¨íŠ¸/ì •ë³´ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨ ë˜ëŠ” ë¹„ì–´ ìˆìŒ.")
        except Exception as e:
            print(f"Error processing {name} data: {e}")
            traceback.print_exc(file=sys.stderr)
            context[f'{name}_data'], context[f'{name}_info'] = [], {'value': 'N/A'}


    try:
        wti_df = get_wti_data(days_to_fetch)
        if not wti_df.empty and 'Date' in wti_df.columns:
            wti_df['Close'] = pd.to_numeric(wti_df['Close'], errors='coerce')
            wti_df.set_index('Date', inplace=True)
            wti_df_for_chart = wti_df.reset_index().copy()
            wti_df_for_chart['Date'] = pd.to_datetime(wti_df_for_chart['Date']).dt.strftime('%Y-%m-%d')
            context['wti_data'] = wti_df_for_chart.tail(30).to_dict('records')
            if len(wti_df) >= 2:
                context['wti_info'] = calculate_change_info(wti_df.copy(), 'WTI')
            else:
                context['wti_info'] = {'value': 'N/A', 'change': 'N/A', 'change_pct': 'N/A', 'raw_change': 0}
                print(f"DEBUG: WTI ì°¨íŠ¸/ì •ë³´ ë°ì´í„° ë¶€ì¡±. ì •ë³´ ê³„ì‚° ê±´ë„ˆëœ€.")
        else:
            context['wti_data'], context['wti_info'] = [], {'value': 'N/A'}
            print(f"DEBUG: WTI ì°¨íŠ¸/ì •ë³´ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨ ë˜ëŠ” ë¹„ì–´ ìˆìŒ.")
    except Exception as e:
        print(f"Error processing WTI data: {e}")
        traceback.print_exc(file=sys.stderr)
        context['wti_data'], context['wti_info'] = [], {'value': 'N/A'}

    kospi_all_data = cache.get('kospi_all_data', [])
    kosdaq_all_data = cache.get('kosdaq_all_data', [])
    context['kospi_top_volume'] = sorted(kospi_all_data, key=lambda x: x.get('Volume', 0), reverse=True)[:10]
    context['kospi_top_value'] = sorted(kospi_all_data, key=lambda x: x.get('TradingValue', 0), reverse=True)[:10]
    context['kospi_top_gainers'] = sorted(kospi_all_data, key=lambda x: x.get('ChangeRatio', -1000), reverse=True)[:10]
    context['kospi_top_losers'] = sorted(kospi_all_data, key=lambda x: x.get('ChangeRatio', 1000))[:10]
    context['kosdaq_top_volume'] = sorted(kosdaq_all_data, key=lambda x: x.get('Volume', 0), reverse=True)[:10]
    context['kosdaq_top_value'] = sorted(kosdaq_all_data, key=lambda x: x.get('TradingValue', 0), reverse=True)[:10]
    context['kosdaq_top_gainers'] = sorted(kosdaq_all_data, key=lambda x: x.get('ChangeRatio', -1000), reverse=True)[:10]
    context['kosdaq_top_losers'] = sorted(kosdaq_all_data, key=lambda x: x.get('ChangeRatio', 1000))[:10]


    return render_template('index.html', **context)

app.register_blueprint(auth_bp, url_prefix='/auth')
app.register_blueprint(analysis_bp)
app.register_blueprint(tables_bp)
app.register_blueprint(data_bp)
app.register_blueprint(askfin_bp)
app.register_blueprint(search_bp)


with app.app_context():
    try:
        initialize_global_data()
        app.config['QUANT_REPORT_CACHE'] = run_and_cache_quant_report()
        print("--- ëª¨ë“  ì´ˆê¸° ë°ì´í„° ë¡œë”© ì™„ë£Œ ---", flush=True)
    except Exception as e:
        print(f"CRITICAL ERROR during app initialization: {e}", file=sys.stderr, flush=True)
        traceback.print_exc(file=sys.stderr)
        sys.exit(1)

@app.context_processor
def inject_current_year():
    return {'current_year': datetime.utcnow().year}

@app.context_processor
def inject_firebase_config():
    """
    ëª¨ë“  í…œí”Œë¦¿ì— Firebase êµ¬ì„± ì •ë³´ë¥¼ ì£¼ì…í•©ë‹ˆë‹¤.
    """
    return {
        'firebase_config': {
            'apiKey': os.getenv('FIREBASE_API_KEY'),
            'authDomain': os.getenv('FIREBASE_AUTH_DOMAIN'),
            'projectId': os.getenv('FIREBASE_PROJECT_ID'),
            'storageBucket': os.getenv('FIREBASE_STORAGE_BUCKET'),
            'messagingSenderId': os.getenv('FIREBASE_MESSAGING_SENDER_ID'),
            'appId': os.getenv('FIREBASE_APP_ID'),
            'measurementId': os.getenv('FIREBASE_MEASUREMENT_ID')
        }
    }


@app.route('/api/chart_data/<string:ticker>/<string:interval>')
def get_chart_data(ticker, interval):
    try:
        end_date = datetime.now()
        # Map front-end interval names to yfinance interval codes
        interval_map = {'daily': '1d', 'weekly': '1wk', 'monthly': '1mo'}
        yf_interval = interval_map.get(interval)

        if not yf_interval:
            return jsonify({"error": "Invalid interval"}), 400

        # Determine start date based on interval
        if interval == 'daily':
            start_date = end_date - timedelta(days=90)
        elif interval == 'weekly':
            start_date = end_date - timedelta(days=365 * 3)
        else: # monthly
            start_date = end_date - timedelta(days=365 * 10)
        
        # Call the new robust data fetching function
        raw_df = get_fdr_or_yf_data(ticker, start=start_date, end=end_date, interval=yf_interval)

        if raw_df.empty:
            return jsonify({"error": "No data found for ticker"}), 404

        if isinstance(raw_df.columns, pd.MultiIndex):
            raw_df.columns = raw_df.columns.get_level_values(0)

        raw_df = raw_df.loc[:, ~raw_df.columns.duplicated()]

        date_col_name = next((col for col in ['Date', 'Datetime', 'index'] if col in raw_df.columns), None)

        possible_close_cols = ['Close', 'Adj Close']
        if ticker == 'USD/KRW':
            possible_close_cols.insert(0, 'USD/KRW')
            possible_close_cols.insert(0, 'KRW=X')

        close_col_name = next((col for col in possible_close_cols if col in raw_df.columns), None)

        if not date_col_name or not close_col_name:
            print(f"DEBUG: Could not find Date ({date_col_name}) or Close ({close_col_name}) column in {raw_df.columns} for ticker {ticker}")
            return jsonify({"error": "Could not identify Date or Close column"}), 500

        clean_df = pd.DataFrame({
            'Date': raw_df[date_col_name],
            'Close': pd.to_numeric(raw_df[close_col_name], errors='coerce')
        })

        clean_df.dropna(inplace=True)
        if clean_df.empty:
            return jsonify({"error": "Data became empty after cleaning"}), 404

        clean_df['Date'] = pd.to_datetime(clean_df['Date']).dt.strftime('%Y-%m-%d')

        return jsonify(clean_df[['Date', 'Close']].to_dict('records'))

    except Exception as e:
        import traceback
        print(f"Error in get_chart_data for {ticker}/{interval}: {e}")
        traceback.print_exc()
        return jsonify({"error": str(e)})


@app.route('/stock-model')
def stock_model():
    return render_template('stock_model.html')


if __name__ == '__main__':
    app.run(debug=True, port=5000)