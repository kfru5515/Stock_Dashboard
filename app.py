from flask import Flask, render_template, jsonify, session
import FinanceDataReader as fdr
import pandas as pd
from pytz import timezone
from datetime import datetime, timedelta
import os
import json
import yfinance as yf
from pykrx import stock
import requests
from bs4 import BeautifulSoup
import traceback
from urllib.parse import urljoin
from readability import Document
import pickle
import re

from run import EnhancedStockPredictor

from transformers import AutoTokenizer, pipeline
from blueprints.analysis import analysis_bp
from blueprints.tables import tables_bp
from blueprints.join import join_bp
from blueprints.data import data_bp
from blueprints.auth import auth_bp
from blueprints import askfin
from blueprints.askfin import askfin_bp, initialize_global_data, GLOBAL_TICKER_NAME_MAP #
from blueprints.search import search_bp
from dotenv import load_dotenv

from db.extensions import db
load_dotenv()

app = Flask(__name__)
app.secret_key = os.urandom(24)

# â”€â”€ ê¸ˆìœµ í‚¤ì›Œë“œ ì„¸íŠ¸ (dataâ€‘files/finance.csv) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
finance_df = pd.read_csv(
    os.path.join(os.path.dirname(__file__), "data_files", "finance.csv"),
    encoding="utf-8-sig"
)
# CSVì— 'keyword' ì»¬ëŸ¼ì´ ìˆë‹¤ê³  ê°€ì •
FINANCE_KEYWORDS = set(
    finance_df['keyword']
    .dropna()
    .astype(str)
    .str.lower()
    .str.strip()
)

# â”€â”€ Sentiment model & pipelines ë¡œë”© â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SAVED_MODEL_DIR = os.path.join(os.path.dirname(__file__), "data_files", "saved_model")
tokenizer = AutoTokenizer.from_pretrained(
    SAVED_MODEL_DIR, use_fast=True, trust_remote_code=True
)
sentiment_pipeline = pipeline(
    'sentiment-analysis',
    model=SAVED_MODEL_DIR,
    tokenizer=SAVED_MODEL_DIR,
    return_all_scores=False,
    device=-1
)

# ë¶ˆìš© ë¬¸ì/í† í° ì œê±°ìš© (í•„ìš”ì‹œ ë” ì¶”ê°€)
STOP_CHARS_PATTERN = re.compile(r"[.,Â·()\[\]{}!?;:â€œâ€\"'`â€¦]")

def clean_for_sentiment(text: str) -> str:
    text = STOP_CHARS_PATTERN.sub(" ", text)
    text = text.replace("[UNK]", " ")
    text = re.sub(r"\s+", " ", text).strip()
    return text

# â”€â”€ ê¸°ì—…ëª… ì¶”ì¶œê¸° ë¡œë”© â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with open(os.path.join(os.path.dirname(__file__), 'data_files', 'keyword_processor.pkl'), 'rb') as f:
    keyword_processor = pickle.load(f)
corp_df = pd.read_csv(
    os.path.join(os.path.dirname(__file__), 'data_files', 'corp_names.csv'),
    encoding='utf-8-sig'
)
COMPANY_SET = set(corp_df['corp_name'].astype(str))
STOPWORDS = {"ETF", "ETN", "ì‹ íƒ", "SPAC", "í€ë“œ", "ë¦¬ì¸ "}

BOUNDARY = r"[ê°€-í£A-Za-z0-9]"   # ë‹¨ì–´ë¡œ ì·¨ê¸‰í•  ë¬¸ìë“¤

def is_standalone(word: str, text: str) -> bool:
    """
    text ì•ˆì—ì„œ wordê°€ ì–‘ì˜†ì´ ë¬¸ì/ìˆ«ìì— ë¶™ì–´ìˆì§€ ì•Šì€ 'ë…ë¦½ëœ' í˜•íƒœë¡œ ì¡´ì¬í•˜ëŠ”ì§€ ê²€ì‚¬
    """
    pattern = rf"(?<!{BOUNDARY}){re.escape(word)}(?!{BOUNDARY})"
    return re.search(pattern, text) is not None

def extract_companies(text: str) -> list[str]:
    cleaned = re.sub(r'â“’.*', '', text)
    candidates = keyword_processor.extract_keywords(cleaned)

    uniq = dict.fromkeys(candidates)  # ìˆœì„œ ìœ ì§€í•œ ì¤‘ë³µ ì œê±°
    filtered = []
    for w in uniq:
        # 1) ê¸°ë³¸ í•„í„°
        if w in STOPWORDS: 
            continue
        if w not in COMPANY_SET:
            continue
        # 2) ë‹¨ë… ë‹¨ì–´ì¸ì§€ í™•ì¸ (ë¶€ë¶„ ë¬¸ìì—´ ë°©ì§€)
        if not is_standalone(w, cleaned):
            continue
        filtered.append(w)
    return filtered

# â”€â”€ ë³¸ë¬¸(fetch) í—¬í¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fetch_body(url: str) -> str:
    try:
        resp = requests.get(url, headers={"User-Agent": "Mozilla/5.0"}, timeout=5)
        resp.raise_for_status()
        html = resp.text
        doc = Document(html) #
        content_html = doc.summary() #
        soup = BeautifulSoup(content_html, "html.parser")
        for tag in soup(["script", "style"]):
            tag.decompose()
        paragraphs = [
            p.get_text(strip=True)
            for p in soup.find_all("p")
            if len(p.get_text(strip=True)) > 20
        ]
        return "\n".join(paragraphs)
    except Exception:
        return ""

# â”€â”€ Jinja2 í•„í„° ì •ì˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@app.template_filter('format_kr')
def format_kr(value):
    try:
        num = int(value)
        if num >= 100000000:
            return f"{num // 100000000}ì–µ"
        elif num >= 10000:
            return f"{num // 10000}ë§Œ"
        else:
            return str(num)
    except (ValueError, TypeError):
        return value

@app.template_filter('format_price')
def format_price(value):
    try:
        return f"{int(float(value)):,}"
    except (ValueError, TypeError):
        return value

@app.template_filter('format_value')
def format_value(value):
    try:
        return f"{int(value) // 100000000:,.0f}ì–µ"
    except (ValueError, TypeError):
        return value

app.jinja_env.filters['format_kr'] = format_kr
app.jinja_env.filters['format_price'] = format_price
app.jinja_env.filters['format_value'] = format_value

CACHE_PATH = 'cache/market_data.json'
CACHE_DIR = 'cache'

def get_latest_business_day():
    return stock.get_nearest_business_day_in_a_week()

def get_market_rank_data(date_str):
    kospi_df = stock.get_market_ohlcv(date_str, market="KOSPI").reset_index()
    kosdaq_df = stock.get_market_ohlcv(date_str, market="KOSDAQ").reset_index()
    for df in [kospi_df, kosdaq_df]:
        tickers = df['í‹°ì»¤']
        if askfin.GLOBAL_TICKER_NAME_MAP is None:
            askfin.initialize_global_data()
        names = [askfin.GLOBAL_TICKER_NAME_MAP.get(ticker, ticker) for ticker in tickers]
        df['Name'] = names
        df.rename(columns={'í‹°ì»¤': 'Code', 'ì¢…ê°€': 'Close', 'ê±°ë˜ëŸ‰': 'Volume', 'ê±°ë˜ëŒ€ê¸ˆ': 'TradingValue', 'ë“±ë½ë¥ ': 'ChangeRatio'}, inplace=True)
        for col in ['Close', 'Volume', 'TradingValue', 'ChangeRatio']:
            df[col] = df[col].fillna(0)
    return kospi_df.to_dict('records'), kosdaq_df.to_dict('records')

def get_wti_data(days=60):
    ticker = yf.Ticker("CL=F")
    df = ticker.history(period=f"{days}d")
    return df.reset_index()[['Date', 'Close']].dropna()

def calculate_change_info(df, name):
    if df is None or len(df.index) < 2:
        return {'name': name, 'value': 'N/A', 'change': 'N/A', 'change_pct': 'N/A', 'raw_change': 0}
    df_no_tz = df.copy()
    if isinstance(df_no_tz.index, pd.DatetimeIndex):
       df_no_tz.index = df_no_tz.index.tz_localize(None)
    df_no_tz = df_no_tz.sort_index().dropna()
    if len(df_no_tz) < 2:
        return {'name': name, 'value': 'N/A', 'change': 'N/A', 'change_pct': 'N/A', 'raw_change': 0}
    latest = df_no_tz.iloc[-1]
    previous = df_no_tz.iloc[-2]
    value = latest['Close']
    change = value - previous['Close']
    change_pct = (change / previous['Close']) * 100 if previous['Close'] != 0 else 0
    return {'name': name, 'value': f"{value:,.2f}", 'change': f"{change:,.2f}", 'change_pct': f"{change_pct:+.2f}%", 'raw_change': change}

def get_fdr_or_yf_data(ticker, start, end, interval='1d'):
    # tickerê°€ '.KS' ë˜ëŠ” '.KQ'ë¡œ ëë‚˜ëŠ” í•œêµ­ ì£¼ì‹ ì½”ë“œì¼ ê²½ìš°, yfinance í‹°ì»¤ ë§µì—ì„œ ì œê±°í•˜ê³  ì§ì ‘ ì‚¬ìš©
    # ì™œëƒí•˜ë©´ 'KS11'ì´ë‚˜ 'KQ11' ê°™ì€ ì§€ìˆ˜ í‹°ì»¤ì™€ '005930.KS' ê°™ì€ ê°œë³„ ì£¼ì‹ í‹°ì»¤ë¥¼ êµ¬ë¶„í•´ì•¼ í•˜ê¸° ë•Œë¬¸
    yf_ticker_map = {'USD/KRW': 'KRW=X', 'KS11': '^KS11', 'KQ11': '^KQ11', 'CL=F': 'CL=F'}
    
    # í•œêµ­ ê°œë³„ ì£¼ì‹ì€ yf_ticker_mapì— ì—†ìœ¼ë¯€ë¡œ ì§ì ‘ ticker ì‚¬ìš©
    if ticker.endswith(('.KS', '.KQ')):
        actual_yf_ticker = ticker
    else:
        actual_yf_ticker = yf_ticker_map.get(ticker, ticker) # ë§¤í•‘ëœ ì´ë¦„ ì‚¬ìš©

    if interval != '1d':
        try:
            print(f"Attempting to fetch '{actual_yf_ticker}' with yfinance (interval: {interval})...")
            yf_df = yf.download(actual_yf_ticker, start=start, end=end, interval=interval, auto_adjust=True, show_errors=True) # show_errors=True ì¶”ê°€
            if not yf_df.empty:
                yf_df = yf_df.reset_index()
                if 'index' in yf_df.columns:
                    yf_df.rename(columns={'index': 'Date'}, inplace=True)
                elif 'Datetime' in yf_df.columns:
                    yf_df.rename(columns={'Datetime': 'Date'}, inplace=True)
                print("yfinance fetch successful.")
                return yf_df[['Date', 'Close']].copy()
            raise ValueError("yfinance returned empty dataframe or ticker not in map")
        except Exception as yf_e:
            print(f"yfinance (interval) failed for '{actual_yf_ticker}': {yf_e}. Falling back to fdr.")

    try:
        print(f"Attempting to fetch '{ticker}' with fdr...")
        df = fdr.DataReader(ticker, start, end)
        if df.empty: raise ValueError("FDR returned empty dataframe")
        print("FDR fetch successful.")
        df = df.reset_index()
        if 'Date' not in df.columns:
            if 'index' in df.columns:
                df.rename(columns={'index': 'Date'}, inplace=True)
        return df[['Date', 'Close']].copy()
    except Exception as e:
        print(f"FDR failed for '{ticker}': {e}. Falling back to yfinance (daily).")
        try:
            # yfinance ì¼ë³„ ë°ì´í„° í´ë°±
            yf_df = yf.download(actual_yf_ticker, start=start, end=end, interval='1d', auto_adjust=True, show_errors=True) # show_errors=True ì¶”ê°€
            if yf_df.empty: return pd.DataFrame()
            print("yfinance (daily) fetch successful.")
            yf_df = yf_df.reset_index()
            if 'index' in yf_df.columns:
                yf_df.rename(columns={'index': 'Date'}, inplace=True)
            elif 'Datetime' in yf_df.columns:
                yf_df.rename(columns={'Datetime': 'Date'}, inplace=True)
            return yf_df[['Date', 'Close']].copy()
        except Exception as yf_e:
            print(f"yfinance (daily) also failed for '{actual_yf_ticker}': {yf_e}")
            return pd.DataFrame()

@app.route('/news/<string:code>')
def get_news(code):
    """
    íŠ¹ì • ì¢…ëª© ì½”ë“œì— ëŒ€í•œ ë‰´ìŠ¤ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    codeëŠ” '005930.KS'ì™€ ê°™ì€ yfinance í˜•ì‹ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    """
    formatted_news = []
    # yfinance ì½”ë“œì—ì„œ 6ìë¦¬ ìˆœìˆ˜ ì¢…ëª© ì½”ë“œ ì¶”ì¶œ (ë„¤ì´ë²„ APIìš©)
    stock_code_6_digit = code.split('.')[0]
    print(f"DEBUG: '{code}' (6ìë¦¬: {stock_code_6_digit}) ì¢…ëª© ì½”ë“œì— ëŒ€í•´ ë„¤ì´ë²„ ëª¨ë°”ì¼ APIì—ì„œ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸° ì‹œë„.")
    try:
        url = f"https://m.stock.naver.com/api/news/stock/{stock_code_6_digit}?pageSize=10&page=1" # 6ìë¦¬ ì½”ë“œ ì‚¬ìš©
        headers = {"User-Agent": "Mozilla/5.0"}
        response = requests.get(url, headers=headers, timeout=5)
        response.raise_for_status()
        raw_data = response.json()

        if isinstance(raw_data, list):
            for data_group in raw_data:
                if isinstance(data_group, dict) and 'items' in data_group:
                    for item in data_group.get('items', []):
                        if isinstance(item, dict):
                            office_id, article_id = item.get('officeId'), item.get('articleId')
                            if office_id and article_id:
                                raw_dt = item.get('datetime', '')
                                f_date = f"{raw_dt[0:4]}-{raw_dt[4:6]}-{raw_dt[6:8]} {raw_dt[8:10]}:{raw_dt[10:12]}" if len(raw_dt) >= 12 else raw_dt
                                formatted_news.append({
                                    'title': item.get('title'),
                                    'press': item.get('officeName'),
                                    'date': f_date,
                                    'url': f"https://n.news.naver.com/mnews/article/{office_id}/{article_id}"
                                })

        if not formatted_news:
            print(f"DEBUG: '{code}'ì— ëŒ€í•œ ë‰´ìŠ¤ë¥¼ APIì—ì„œ ê°€ì ¸ì™”ìœ¼ë‚˜ í•­ëª©ì´ 0ê°œì…ë‹ˆë‹¤.")
            return jsonify({"error": "ê´€ë ¨ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. (API ê²°ê³¼ ì—†ìŒ)"}), 200 # 500 ëŒ€ì‹  200ìœ¼ë¡œ ë³€ê²½
        
        # ë‰´ìŠ¤ ê°ì„± ë¶„ì„ ë° ê¸°ì—…ëª… ì¶”ì¶œ (ë‰´ìŠ¤ ì»¨í…ì¸  í¬ë¡¤ë§ì´ í•„ìš”í•˜ë¯€ë¡œ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŒ)
        processed_news = []
        for news_item in formatted_news[:5]: # ë„ˆë¬´ ë§ì€ ë‰´ìŠ¤ ë³¸ë¬¸ ë¶„ì„ì€ ë¹„íš¨ìœ¨ì ì´ë¯€ë¡œ ìƒìœ„ 5ê°œë§Œ
            body = fetch_body(news_item["url"])
            sentiment = "ì—†ìŒ"
            companies = []

            # ë³¸ë¬¸ì´ ë¹„ì–´ìˆì§€ ì•Šê³  ê¸¸ì´ê°€ ì¶©ë¶„í•  ë•Œë§Œ ê°ì„± ë¶„ì„ ë° ê¸°ì—…ëª… ì¶”ì¶œ ì‹œë„
            if body and len(body.strip()) > 50: # ìµœì†Œ ê¸¸ì´ ì„¤ì •
                title_clean = clean_for_sentiment(news_item["title"])
                target_text = title_clean if title_clean.strip() else clean_for_sentiment(body)[:256]

                if target_text.strip():
                    try:
                        sentiment_result = sentiment_pipeline(target_text)[0]["label"]
                        sentiment = sentiment_result if sentiment_result else "ì—†ìŒ"
                    except Exception as sentiment_e:
                        print(f"DEBUG: ê°ì„± ë¶„ì„ ì˜¤ë¥˜: {sentiment_e}")
                        sentiment = "ì˜¤ë¥˜" # ê°ì„± ë¶„ì„ ì‹¤íŒ¨ ì‹œ

                try:
                    companies = extract_companies(body)
                except Exception as company_e:
                    print(f"DEBUG: ê¸°ì—…ëª… ì¶”ì¶œ ì˜¤ë¥˜: {company_e}")
                    companies = [] # ê¸°ì—…ëª… ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ
            
            processed_news.append({
                'title': news_item['title'],
                'press': news_item['press'],
                'date': news_item['date'],
                'url': news_item['url'],
                'sentiment': sentiment,
                'companies': companies
            })

        print(f"DEBUG: '{code}'ì— ëŒ€í•œ ë‰´ìŠ¤ {len(processed_news)}ê°œ ì„±ê³µì ìœ¼ë¡œ ê°€ì ¸ì˜´ (ê°ì„± ë¶„ì„ í¬í•¨).")
        return jsonify(processed_news)

    except requests.exceptions.Timeout:
        print(f"DEBUG: ë‰´ìŠ¤ API ìš”ì²­ íƒ€ì„ì•„ì›ƒ ë°œìƒ (ì¢…ëª©ì½”ë“œ: {code})")
        return jsonify({"error": "ë‰´ìŠ¤ ìš”ì²­ ì‹œê°„ ì´ˆê³¼ (API í˜¸ì¶œ ì‹¤íŒ¨)"}), 500
    except requests.exceptions.RequestException as e:
        print(f"DEBUG: ë‰´ìŠ¤ API ìš”ì²­ ì˜¤ë¥˜ (ì¢…ëª©ì½”ë“œ: {code}): {e}")
        return jsonify({"error": f"ë‰´ìŠ¤ API í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}"}), 500
    except json.JSONDecodeError:
        print(f"DEBUG: ë‰´ìŠ¤ API ì‘ë‹µì´ ìœ íš¨í•œ JSONì´ ì•„ë‹˜ (ì¢…ëª©ì½”ë“œ: {code})")
        return jsonify({"error": "ë‰´ìŠ¤ API ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨"}), 500
    except Exception as e:
        print(f"DEBUG: ë‰´ìŠ¤ API ì²˜ë¦¬ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ ë°œìƒ (ì¢…ëª©ì½”ë“œ: {code}): {e}")
        traceback.print_exc()
        return jsonify({"error": f"ë‰´ìŠ¤ API ì²˜ë¦¬ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜: {str(e)}"}), 500


def _get_news_from_naver_scraping():
    news_list = []
    print("DEBUG: Attempting to scrape general market news from Naver Finance.")
    try:
        url = "https://finance.naver.com/news/mainnews.naver"
        headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"}
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()

        soup = BeautifulSoup(response.text, 'html.parser')

        news_items = soup.select('.main_news .newsList .articleSubject a')
        press_items = soup.select('.main_news .newsList .articleSummary .press')
        date_items = soup.select('.main_news .newsList .articleSummary .wdate')

        for i in range(min(len(news_items), 10)):
            title = news_items[i].get_text(strip=True)
            link = news_items[i]['href']
            press = press_items[i].get_text(strip=True) if i < len(press_items) else 'N/A'
            date_time = date_items[i].get_text(strip=True) if i < len(date_items) else 'N/A'

            if link.startswith('/'):
                link = f"https://finance.naver.com{link}"

            news_list.append({
                'title': title,
                'press': press,
                'date': date_time,
                'url': link
            })
        print(f"DEBUG: Naver general news scraping successful. Found {len(news_list)} news items.")
    except Exception as e:
        print(f"DEBUG: Error fetching general market news via scraping: {e}")
        traceback.print_exc()
        news_list.append({'title': 'ì¼ë°˜ ì‹œì¥ ë‰´ìŠ¤ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤ (í¬ë¡¤ë§ ì˜¤ë¥˜).', 'press': 'N/A', 'date': 'N/A', 'url': '#'})
    return news_list

# ECOS ì£¼ìš” í†µê³„ ì§€í‘œ í˜„í™© ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
def get_key_statistic_current_data():
    """
    í•œêµ­ì€í–‰ ECOS 'ì£¼ìš” í†µê³„ ì§€í‘œ í˜„í™©' ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    (KeyStatisticList APIë¥¼ ì‚¬ìš©í•˜ë©°, CLASS_NAME, KEYSTAT_NAME, DATA_VALUE ë“±ì„ í¬í•¨)
    """
    ecos_api_key = os.getenv("ECOS_API_KEY")
    if not ecos_api_key:
        print("ECOS API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ ì£¼ìš” í†µê³„ í˜„í™© ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return []

    api_url = f"https://ecos.bok.or.kr/api/KeyStatisticList/{ecos_api_key}/xml/kr/1/100/"

    key_statistics_current_data = []
    print(f"DEBUG: ECOS ì£¼ìš” í†µê³„ í˜„í™© API í˜¸ì¶œ ì‹œë„: {api_url}")
    try:
        response = requests.get(api_url, timeout=10)
        response.raise_for_status()

        soup = BeautifulSoup(response.content, 'xml')
        items = soup.find_all('row')

        if not items:
            print("ECOS ì£¼ìš” í†µê³„ í˜„í™© APIì—ì„œ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return []

        for item in items:
            class_name = item.find('CLASS_NAME').get_text() if item.find('CLASS_NAME') else 'N/A'
            keystat_name = item.find('KEYSTAT_NAME').get_text() if item.find('KEYSTAT_NAME') else 'N/A'
            data_value = item.find('DATA_VALUE').get_text() if item.find('DATA_VALUE') else 'N/A'
            cycle = item.find('CYCLE').get_text() if item.find('CYCLE') else 'N/A'
            unit_name = item.find('UNIT_NAME').get_text() if item.find('UNIT_NAME') else 'N/A'

            key_statistics_current_data.append({
                "CLASS_NAME": class_name,
                "KEYSTAT_NAME": keystat_name,
                "DATA_VALUE": data_value,
                "CYCLE": cycle,
                "UNIT_NAME": unit_name
            })
        print(f"DEBUG: ECOS ì£¼ìš” í†µê³„ í˜„í™© ë°ì´í„° {len(key_statistics_current_data)}ê°œ í•­ëª© ì„±ê³µì ìœ¼ë¡œ ê°€ì ¸ì˜´.")
        return key_statistics_current_data

    except requests.exceptions.Timeout:
        print("ECOS ì£¼ìš” í†µê³„ í˜„í™© API ìš”ì²­ ì‹œê°„ ì´ˆê³¼.")
    except requests.exceptions.RequestException as e:
        print(f"ECOS ì£¼ìš” í†µê³„ í˜„í™© API ìš”ì²­ ì˜¤ë¥˜: {e}")
        traceback.print_exc()
    except Exception as e:
        print(f"ECOS ì£¼ìš” í†µê³„ í˜„í™© ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        traceback.print_exc()
    return []


def get_general_market_news():
    """
    í•œêµ­ ì‹œì¥ ì£¼ìš” ë‰´ìŠ¤ë¥¼ ê°€ì ¸ì™€
    ë³¸ë¬¸(fetch) â†’ ê¸ˆìœµ í‚¤ì›Œë“œ í•„í„° â†’ ê°ì„±ë¶„ì„(ì œëª©ë§Œ) & ê¸°ì—…ëª…ì¶”ì¶œ í›„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    news_api_key = os.getenv("NEWS_API_KEY")
    raw_list = []

    # 1) NewsAPIë¡œ í•œêµ­ ì‹œì¥ ë‰´ìŠ¤ ì‹œë„
    if news_api_key:
        try:
            query = (
                "í•œêµ­ ê²½ì œ OR êµ­ë‚´ ì¦ì‹œ OR í•œêµ­ì€í–‰ OR ì½”ìŠ¤í”¼ OR ì½”ìŠ¤ë‹¥ "
                "OR êµ­ë‚´ ê¸°ì—… OR ê±°ì‹œê²½ì œ OR ê¸ˆë¦¬ OR í™˜ìœ¨ OR CPI "
                "OR ì¸í”Œë ˆì´ì…˜ OR GDP OR í†µí™”ì •ì±… OR ì¬ì •ì •ì±…"
            )
            api_url = (
                f"https://newsapi.org/v2/everything?"
                f"q={query}&language=ko&sortBy=publishedAt"
                f"&apiKey={news_api_key}&pageSize=10"
            )
            resp = requests.get(api_url, timeout=7)
            resp.raise_for_status()
            data = resp.json()

            if data.get("status") == "ok" and data.get("articles"):
                for art in data["articles"]:
                    raw_list.append({
                        "title": art.get("title", "ì œëª© ì—†ìŒ"),
                        "press": art.get("source", {}).get("name", "N/A"),
                        "date": art.get("publishedAt", "")[:10],
                        "url": art.get("url", "#"),
                    })
            else:
                raw_list = _get_news_from_naver_scraping()

        except Exception:
            traceback.print_exc()
            raw_list = _get_news_from_naver_scraping()
    else:
        # API í‚¤ ì—†ìœ¼ë©´ ë„¤ì´ë²„ ìŠ¤í¬ë˜í•‘ìœ¼ë¡œ
        raw_list = _get_news_from_naver_scraping()

    # 2) ë³¸ë¬¸(fetch) â†’ ê¸ˆìœµ í‚¤ì›Œë“œ í•„í„° â†’ ê°ì„±ë¶„ì„(ì œëª©ë§Œ) & ê¸°ì—…ëª…ì¶”ì¶œ
    processed = []
    for item in raw_list:
        body = fetch_body(item["url"])

        # â† ì´ ë¶€ë¶„ ë°”ë¡œ ì•„ë˜ì— ê¸ˆìœµ í‚¤ì›Œë“œ í•„í„° ì‚½ì…
        combined = (item["title"] + " " + item["press"] + " " + body).lower()
        if not any(kw in combined for kw in FINANCE_KEYWORDS):
            continue
        # â†’ ì—¬ê¸°ê¹Œì§€ í•„í„°ë§ êµ¬ê°„

        # â€”â€” ê°ì„±ë¶„ì„(ì œëª©ë§Œ) & ê¸°ì—…ëª…ì¶”ì¶œ â€”â€” 
        title_clean = clean_for_sentiment(item["title"])
        # ì œëª©ì´ ë„ˆë¬´ ì§§ê±°ë‚˜ ë¹„ì–´ìˆì„ ê²½ìš° ëŒ€ë¹„: ë³¸ë¬¸ ì¼ë¶€ë¡œ ë°±ì—…
        if not title_clean.strip():
            backup_text = clean_for_sentiment(body)[:256]
            target_text = backup_text if backup_text else "ë‚´ìš©ì—†ìŒ"
        else:
            target_text = title_clean

        # sentiment_pipeline ë¡œë”© í™•ì¸ ì¶”ê°€
        sentiment = "ì—†ìŒ"
        if 'sentiment_pipeline' in globals() and sentiment_pipeline is not None:
            try:
                sentiment = sentiment_pipeline(target_text[:256])[0]["label"]
            except Exception as sentiment_e:
                print(f"DEBUG: ê°ì„± ë¶„ì„ íŒŒì´í”„ë¼ì¸ í˜¸ì¶œ ì˜¤ë¥˜: {sentiment_e}")
                sentiment = "ì˜¤ë¥˜"
        else:
            print("DEBUG: sentiment_pipelineì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")


        companies = extract_companies(body)

        processed.append({
            "title":     item["title"],
            "press":     item["press"],
            "date":      item["date"],
            "url":       item["url"],
            "sentiment": sentiment,
            "companies": companies
        })

    return processed

def get_international_market_news():
    """í•´ì™¸ ì‹œì¥ ë‰´ìŠ¤ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤ (NewsAPI.org - ë¯¸êµ­ ë¹„ì¦ˆë‹ˆìŠ¤ í—¤ë“œë¼ì¸)."""
    news_api_key = os.getenv("NEWS_API_KEY")
    if not news_api_key:
        print("DEBUG: NEWS_API_KEY ì—†ìŒ. í•´ì™¸ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸° ê±´ë„ˆëœœë‹ˆë‹¤.")
        return [] # API í‚¤ ì—†ìœ¼ë©´ í•´ì™¸ ë‰´ìŠ¤ëŠ” ê°€ì ¸ì˜¤ì§€ ì•ŠìŒ
    try:
        api_url = f"https://newsapi.org/v2/top-headlines?country=us&category=business&apiKey={news_api_key}&pageSize=10"
        print("DEBUG: NewsAPI.orgë¡œ í•´ì™¸ ì‹œì¥ ë‰´ìŠ¤ (ë¯¸êµ­ ë¹„ì¦ˆë‹ˆìŠ¤) ê²€ìƒ‰ ì‹œë„...")
        response = requests.get(api_url, timeout=7)
        response.raise_for_status()

        data = response.json()
        if data.get('status') == 'ok' and data.get('articles'):
            print(f"DEBUG: NewsAPI.orgì—ì„œ í•´ì™¸ ì‹œì¥ ë‰´ìŠ¤ {len(data['articles'])}ê°œ ì„±ê³µì ìœ¼ë¡œ ê°€ì ¸ì˜´.")
            return [{'title': a.get('title', 'ì œëª© ì—†ìŒ'), 'press': a.get('source', {}).get('name', 'N/A'), 'date': a.get('publishedAt', '')[:10], 'url': a.get('url', '#')} for a in data['articles']]
        else:
            print("DEBUG: NewsAPI.org ì‘ë‹µ ìƒíƒœ 'ok' ì•„ë‹˜ ë˜ëŠ” ê¸°ì‚¬ ì—†ìŒ. í•´ì™¸ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨.")
            return []
    except Exception as e:
        print(f"DEBUG: NewsAPI.org ì˜¤ë¥˜ (í•´ì™¸ ë‰´ìŠ¤): {e}. í•´ì™¸ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨.")
        traceback.print_exc()
        return []

def run_and_cache_quant_report():
    """ì„œë²„ ì‹œì‘ ì‹œ í€€íŠ¸ ë¶„ì„ì„ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜"""
    print("ğŸš€ ìµœì´ˆ í€€íŠ¸ ë¦¬í¬íŠ¸ ìƒì„± ë° ìºì‹± ì‹œì‘...")
    try:
        predictor = EnhancedStockPredictor(start_date='2015-01-01')
        
        predictor.collect_all_data()
        patterns = predictor.analyze_patterns()
        predictor.detect_anomalies()
        
        # calculate_economic_risks_detailed()ê°€ ë‚´ë¶€ì ìœ¼ë¡œ risk_historyë¥¼ ê³„ì‚°í•¨
        current_risks = predictor.calculate_economic_risks_detailed() 
        predictions = predictor.predict_weekly_enhanced()
        
        # --- ìˆ˜ì •ëœ ë¶€ë¶„ ì‹œì‘ ---
        # DataFrameì„ JSON ì¹œí™”ì ì¸ í˜•íƒœë¡œ ë³€í™˜
        if 'monthly' in patterns and isinstance(patterns.get('monthly'), pd.DataFrame):
            patterns['monthly'] = patterns['monthly'].reset_index().to_dict('records')
        if 'daily' in patterns and isinstance(patterns.get('daily'), pd.DataFrame):
            patterns['daily'] = patterns['daily'].reset_index().to_dict('records')
        
        # risk_historyë„ JSONìœ¼ë¡œ ë³€í™˜
        risk_history_data = None
        if hasattr(predictor, 'risk_history') and not predictor.risk_history.empty:
            df = predictor.risk_history.reset_index()
            df['index'] = df['index'].dt.strftime('%Y-%m-%d')
            risk_history_data = df.to_dict('records')

        # ì£¼ìš” ëª¨ë‹ˆí„°ë§ ì§€í‘œ ìƒì„± ë¡œì§ ì¶”ê°€
        monitoring_indicators = []
        if current_risks['inflation']['risk'] > 40:
            monitoring_indicators.extend(["ì›ìì¬ ê°€ê²©", "ë‹¬ëŸ¬ ì¸ë±ìŠ¤", "ì¥ê¸° ê¸ˆë¦¬"])
        if current_risks['deflation']['risk'] > 40:
            monitoring_indicators.extend(["VIX ì§€ìˆ˜", "ê¸€ë¡œë²Œ ì£¼ê°€", "ê²½ê¸°ì„ í–‰ì§€ìˆ˜"])
        if current_risks['stagflation']['risk'] > 30:
            monitoring_indicators.extend(["í™˜ìœ¨", "ê³µê¸‰ë§ ì§€í‘œ", "ì„ê¸ˆ ìƒìŠ¹ë¥ "])
        if not monitoring_indicators:
            monitoring_indicators.extend(["ì „ë°˜ì  ì‹œì¥ ë™í–¥", "ê¸°ìˆ ì  ì§€í‘œ", "ê±°ë˜ëŸ‰"])
        # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
        monitoring_indicators = sorted(list(set(monitoring_indicators)))
        # --- ìˆ˜ì •ëœ ë¶€ë¶„ ë ---

        report_data = {
            "current_risks": current_risks,
            "future_risks": predictor.future_risks,
            "predictions": predictions,
            "patterns": patterns,
            "anomalies": predictor.anomalies,
            "overall_risk": current_risks.get('overall', 0),
            "risk_history": risk_history_data,
            "monitoring_indicators": monitoring_indicators # <-- ëª¨ë‹ˆí„°ë§ ì§€í‘œ ì¶”ê°€
        }
        print(" ìµœì´ˆ í€€íŠ¸ ë¦¬í¬íŠ¸ ìºì‹± ì™„ë£Œ.")
        return report_data
    except Exception as e:
        print(f" ìµœì´ˆ í€€íŠ¸ ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
        traceback.print_exc()
        return None


@app.route('/api/latest-data')
def get_latest_data():
    try:
        end_date = datetime.now()
        start_date = end_date - timedelta(days=10)
        data = {}
        for ticker, name in [('KS11', 'kospi'), ('KQ11', 'kosdaq'), ('USD/KRW', 'usdkrw')]:
            df = get_fdr_or_yf_data(ticker, start_date, end_date)
            if ticker == 'USD/KRW' and 'Close' not in df.columns and 'USD/KRW' in df.columns:
                df.rename(columns={'USD/KRW': 'Close'}, inplace=True)
            df['Close'] = pd.to_numeric(df['Close'], errors='coerce')
            df.set_index('Date', inplace=True)
            data[name] = calculate_change_info(df.copy(), name.upper())

        wti_df = get_wti_data(10)
        if not wti_df.empty: wti_df.set_index('Date', inplace=True)
        data['wti'] = calculate_change_info(wti_df, 'WTI')

        return jsonify(data)
    except Exception as e:
        print(f"Error in /api/latest-data: {e}")
        return jsonify({"error": str(e)}), 500

@app.route('/')
def index_main():
    return render_template('index_main.html')

@app.route('/index')
def index():
    os.makedirs(CACHE_DIR, exist_ok=True)
    try:
        with open(CACHE_PATH, 'r', encoding='utf-8') as f:
            cache = json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
        cache = {}

    latest_bday = get_latest_business_day()
    formatted_today_date = datetime.strptime(latest_bday, '%Y%m%d').strftime('%mì›” %dì¼')

    if cache.get('date') != latest_bday:
        print(f"DEBUG: ìºì‹œ ì—…ë°ì´íŠ¸ í•„ìš”. í˜„ì¬ ì˜ì—…ì¼: {latest_bday}, ìºì‹œëœ ë‚ ì§œ: {cache.get('date')}")
        new_cache = {'date': latest_bday}
        
        kospi_all_data = []
        kosdaq_all_data = []

        try:
            kospi_all_data, kosdaq_all_data = get_market_rank_data(latest_bday)
            
            current_key_stats_full = get_key_statistic_current_data()
            filtered_key_stats = [item for item in current_key_stats_full if item.get('DATA_VALUE') not in ['N/A', '', None]]
            important_key_stats = filtered_key_stats[:20]

            korean_news = get_general_market_news() # í•œêµ­ ë‰´ìŠ¤ ë‹´ë‹¹ í•¨ìˆ˜
            international_news = get_international_market_news()

            new_cache.update({
                'kospi_all_data': kospi_all_data,
                'kosdaq_all_data': kosdaq_all_data,
                'korean_market_news': korean_news,
                'international_market_news': international_news,
                'key_statistic_current_data': important_key_stats
            })
        except Exception as e:
            print(f"Error creating cache: {e}")
            traceback.print_exc()
            new_cache.update({
                'kospi_all_data': [],
                'kosdaq_all_data': [],
                'korean_market_news': [],
                'international_market_news': [],
                'key_statistic_current_data': []
            })
        cache = new_cache
        with open(CACHE_PATH, 'w', encoding='utf-8') as f:
            json.dump(cache, f, ensure_ascii=False, indent=2)
    else:
        print(f"DEBUG: ìºì‹œ ìµœì‹  ìƒíƒœ ìœ ì§€. ë‚ ì§œ: {latest_bday}")


    context = {
        'today': datetime.strptime(latest_bday, '%Y%m%d').strftime('%Y-%m-%d'),
        'formatted_today_date': formatted_today_date,
        **cache,
        'key_statistic_current_data': cache.get('key_statistic_current_data', []),
        'korean_market_news': cache.get('korean_market_news', []),
        'international_market_news': cache.get('international_market_news', [])
    }
    days_to_fetch = 60
    start_date, end_date = datetime.now() - timedelta(days=days_to_fetch), datetime.now()

    for ticker, name in [('KS11', 'kospi'), ('KQ11', 'kosdaq'), ('USD/KRW', 'usdkrw')]:
        try:
            df = get_fdr_or_yf_data(ticker, start_date, end_date)
            if not df.empty and 'Date' in df.columns:
                df['Close'] = pd.to_numeric(df['Close'], errors='coerce')
                df.set_index('Date', inplace=True)
                context[f'{name}_info'] = calculate_change_info(df.copy(), name.upper())
                df_for_chart = df.reset_index().copy()
                df_for_chart['Date'] = pd.to_datetime(df_for_chart['Date']).dt.strftime('%Y-%m-%d')
                context[f'{name}_data'] = df_for_chart.tail(30).to_dict('records')
            else:
                context[f'{name}_data'], context[f'{name}_info'] = [], {'value': 'N/A'}
        except Exception as e:
            print(f"Error processing {name} data: {e}")
            context[f'{name}_data'], context[f'{name}_info'] = [], {'value': 'N/A'}


    try:
        wti_df = get_wti_data(days_to_fetch)
        if not wti_df.empty and 'Date' in wti_df.columns:
            wti_df['Close'] = pd.to_numeric(wti_df['Close'], errors='coerce')
            wti_df.set_index('Date', inplace=True)
            context['wti_info'] = calculate_change_info(wti_df.copy(), 'WTI')
            wti_df_for_chart = wti_df.reset_index().copy()
            wti_df_for_chart['Date'] = pd.to_datetime(wti_df_for_chart['Date']).dt.strftime('%Y-%m-%d')
            context['wti_data'] = wti_df_for_chart.tail(30).to_dict('records')
        else:
            context['wti_data'], context['wti_info'] = [], {'value': 'N/A'}
    except Exception as e:
        print(f"Error processing WTI data: {e}")
        context['wti_data'], context['wti_info'] = [], {'value': 'N/A'}

    kospi_all_data = cache.get('kospi_all_data', [])
    kosdaq_all_data = cache.get('kosdaq_all_data', [])
    context['kospi_top_volume'] = sorted(kospi_all_data, key=lambda x: x.get('Volume', 0), reverse=True)[:10]
    context['kospi_top_value'] = sorted(kospi_all_data, key=lambda x: x.get('TradingValue', 0), reverse=True)[:10]
    context['kospi_top_gainers'] = sorted(kospi_all_data, key=lambda x: x.get('ChangeRatio', -1000), reverse=True)[:10]
    context['kospi_top_losers'] = sorted(kospi_all_data, key=lambda x: x.get('ChangeRatio', 1000))[:10]
    context['kosdaq_top_volume'] = sorted(kosdaq_all_data, key=lambda x: x.get('Volume', 0), reverse=True)[:10]
    context['kosdaq_top_value'] = sorted(kosdaq_all_data, key=lambda x: x.get('TradingValue', 0), reverse=True)[:10]
    context['kosdaq_top_gainers'] = sorted(kosdaq_all_data, key=lambda x: x.get('ChangeRatio', -1000), reverse=True)[:10]
    context['kosdaq_top_losers'] = sorted(kosdaq_all_data, key=lambda x: x.get('ChangeRatio', 1000))[:10]


    return render_template('index.html', **context)

app.register_blueprint(auth_bp, url_prefix='/auth')
app.register_blueprint(analysis_bp)
app.register_blueprint(tables_bp)
app.register_blueprint(join_bp)
app.register_blueprint(data_bp)
app.register_blueprint(askfin_bp)
app.register_blueprint(search_bp)

app.config['SQLALCHEMY_DATABASE_URI'] = 'mysql+pymysql://humanda5:humanda5@localhost/final_join'
app.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = False


db.init_app(app)

with app.app_context():
    initialize_global_data()
    app.config['QUANT_REPORT_CACHE'] = run_and_cache_quant_report()
    print("--- ëª¨ë“  ì´ˆê¸° ë°ì´í„° ë¡œë”© ì™„ë£Œ ---")

@app.context_processor
def inject_current_year():
    return {'current_year': datetime.utcnow().year}

@app.route('/api/chart_data/<string:ticker>/<string:interval>')
def get_chart_data(ticker, interval):
    try:
        end_date = datetime.now()
        yf_interval = '1d'
        if interval == 'daily':
            start_date = end_date - timedelta(days=90)
        elif interval == 'weekly':
            yf_interval = '1wk'
            start_date = end_date - timedelta(days=365 * 3)
        elif interval == 'monthly':
            yf_interval = '1mo'
            start_date = end_date - timedelta(days=365 * 10)
        else:
            return jsonify({"error": "Invalid interval"}), 400

        raw_df = get_fdr_or_yf_data(ticker, start=start_date, end=end_date, interval=yf_interval) # start, end ì¸ì ì´ë¦„ ëª…ì‹œ
        if raw_df.empty:
            return jsonify({"error": "No data found for ticker"}), 404

        if isinstance(raw_df.columns, pd.MultiIndex):
            raw_df.columns = raw_df.columns.get_level_values(0)

        raw_df = raw_df.loc[:, ~raw_df.columns.duplicated()]

        date_col_name = next((col for col in ['Date', 'Datetime', 'index'] if col in raw_df.columns), None)
        close_col_name = next((col for col in ['Close', 'Adj Close', ticker] if col in raw_df.columns), None)

        if not date_col_name or not close_col_name:
            print(f"DEBUG: Could not find Date or Close column in {raw_df.columns}")
            return jsonify({"error": "Could not identify Date or Close column"}), 500

        clean_df = pd.DataFrame({
            'Date': raw_df[date_col_name],
            'Close': pd.to_numeric(raw_df[close_col_name], errors='coerce')
        })

        clean_df.dropna(inplace=True)
        if clean_df.empty:
            return jsonify({"error": "Data became empty after cleaning"}), 404

        clean_df['Date'] = pd.to_datetime(clean_df['Date']).dt.strftime('%Y-%m-%d')

        return jsonify(clean_df[['Date', 'Close']].to_dict('records'))

    except Exception as e:
        import traceback
        print(f"Error in get_chart_data for {ticker}/{interval}: {e}")
        traceback.print_exc()
        return jsonify({"error": str(e)}), 500

@app.route('/stock-model')
def stock_model():
    return render_template('stock_model.html')


if __name__ == '__main__':
    app.run(debug=True, port=5000)

